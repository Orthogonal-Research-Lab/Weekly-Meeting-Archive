ranscript     
0:00     
hi Hi how are you I added something called an edge um measurement Edge uh     
0:08     
point is a a probe it's an edge probe to try to find the height of my structure     
0:16     
and it's better but it's not like     
0:22     
I it's somewhere there's the math that goes with this you know how it adds and     
0:29     
it says integration 4 I have no idea what that is so just what is integration     
0:37     
4 and then you put it on addition and it does the same thing     
0:44     
so I I don't know yeah I don't     
0:51     
know and I just got turned down for funding to go on a course about console     
0:58     
because it was 700 us for four three or four days so they     
1:06     
just uh yeah they said oh no not a required course yeah so it's not funded     
1:15     
so yeah so there's no help to a help desk there so yeah unless you pay a ,000     
1:24     
Canadian just for the privilege of talking to someone about it I don't know     
1:33     
right yeah they deserve to have themselves priced out of the market completely oh yeah yeah think they've     
1:40     
been taking full advantage of their uh Monopoly power as being like a the     
1:46     
standard uh multiphysics platform so yeah so they may should just that lab is     
1:54     
kind of like that too but um yeah not quite as Shameless as commo     
2:00     
yeah well I've just anyway I did find the um     
2:08     
steady the one that's mat lab oriented yeah uh programming so I would like to     
2:14     
do that next okay yeah anyway so can you see me uh well your camera's     
2:24     
off yeah is that what it is yeah I mean just went off it was on before I don't     
2:29     
know yeah no I may sign off and uh come back     
2:34     
in okay all right because yeah     
2:42     
yeah okay if I can see my screen I'll sign off okay anyway all right we'll see     
2:48     
you in a bit all right see you hello     
2:56     
Pocky hi hello all right     
3:01     
uh so do you have updates uh yeah so over the last week I     
3:07     
was working on uh pre-processing the data that I would be passing on to the     
3:13     
graft and the MLP like I had mentioned in the last meeting so     
3:20     
uhbe I can share my screen yeah     
3:37     
yeah is it visible yes uh yeah so uh if you remember I like     
3:45     
in the last meeting I told I was trying to run some uh code that they had put in     
3:50     
their reposit the nvp people they had made a new comment but like I was facing     
3:57     
some error so I just dropped that I just wanted to see what implementation they had done they had used some     
4:02     
reinforcement learning algorithm in the new commit so like I dropped that for     
4:08     
time being so like I was just generating data so uh I was I'm planning on like     
4:14     
using time series data from like the xlsx file that you had shared with me     
4:20     
about the birth and time death timings so using that I'm like trying to generate the data as of now and like     
4:27     
these uh nodes basically like I'm uh removing the     
4:32     
parent because like it gets divided into two and then adding both the children instead so like uh at each time stamp     
4:40     
what the graph sort of would look like so I'm currently working on like the data preparation it should be done in a     
4:47     
day or two and then like from tomorrow I am expecting to start like building the     
4:52     
model as well okay that's good yeah     
5:02     
okay uh so this is just data processing     
5:07     
and then you would plug this into the neural developmental program uh yeah yeah okay like uh those     
5:16     
functions also I like uh worked on like editing some modifying some functions to     
5:22     
like uh make our needs like what we need to do with like the growing even Network     
5:28     
they have they are using like they're trying to mirror like the exor gates and     
5:33     
a few models they're using so I like edited those functions as well over the last week so okay yeah yeah once I have     
5:41     
like the data ready I'll have like more sense of what I need to add to those functions thep function so yeah okay um     
5:49     
so y last time you said you were having trouble implementing the the program     
5:55     
that they have for NDP so they have a software there     
6:01     
uh yeah yeah uh the in the NDP like in their     
6:07     
repository they had like recently made a new commit which in which they were using like reinforcement learning so I     
6:13     
found that interesting so I just wanted to explore what it is but like when I tried running the code I was like     
6:20     
running into some errors so like I dropped that okay so you you did get them     
6:26     
resolved or do you have a work around uh no like I mentioned right I like then     
6:33     
got to like starting my work as well I'm looking into it at the side it requires     
6:40     
me to install like some libraries and all so that was taking up GPU in collab and like I don't have GPU in my laptop     
6:47     
so I like preserve the GPU for making our model for now     
6:52     
yeah yeah I'll get back to it and like let you know if it runs okay yeah yeah     
6:58     
well that's yeah it's going to be the next step so you've got some time on that to get that all sorted out yeah     
7:04     
we've often run into problems with you know GPU time and GPU access so this is     
7:11     
a common problem and we don't really have a good answer to it but um yeah I     
7:17     
don't know I don't know what to tell you there but I guess it's probably something you could do just kind of with     
7:22     
the collab resources um but yeah let's let's kind     
7:28     
of think in terms of like GPU scarcity and and uh how to overcome     
7:34     
that I     
7:39     
guess okay yeah yeah okay well thanks for the update     
7:45     
Pocky um looks good uh hello dick how are     
7:52     
you hi and we talked to Susan about her     
7:58     
console Miss Adventures or Adventures or whatever     
8:04     
right so the first thing is is that sometimes a lot of the stuff we do in D.A worm and openworm get cited so we     
8:11     
write a paper and then people cite that paper and if you use something like uh     
8:17     
research gate it alerts you to when your paper has been cited so dick sent me     
8:22     
this email already already got about this openworm paper this this is a paper     
8:28     
that was published in 2018 it kind of goes over openworm in terms of the different     
8:34     
components and it's been cited probably about 150 times by now so it's you know     
8:40     
a decent paper in terms of citations uh one of the recent     
8:46     
citations and you know the citations are really diverse like you see all sorts of     
8:51     
different citations for it not a lot in the traditional seans of arure but     
8:57     
actually a lot in the sort of the AI space and and modeling space so that's     
9:03     
interesting but this is the uh latest citation of this paper this is     
9:09     
discovering neural policies to drive Behavior by integrating deep re enforcement learning agents with     
9:17     
biological Network so this is in nature machine intelligence and with these co-authors     
9:22     
so I figured you know i' I'd go get the paper and look it over and see kind of     
9:27     
what they're citing and you know what they're doing so this is the paper here nature     
9:35     
machine intelligence uh discovering neural policies to drive Behavior by     
9:40     
integrating deep reinforcement learning agents with biological neural networks     
9:46     
so they're using this deep reinforcement learning Paradigm uh which is you know     
9:51     
of interest uh you know for a number of problems uh I think it has a good     
9:57     
relevance to biological problems because you know organisms to learn things will     
10:04     
often engage in some form of reinforcement so reinforcement learning to recap is something that kind of came     
10:11     
out of Behavioral psychology uh you think of BF Skinner's work and some of the other behaviorists     
10:20     
of the early to mid 20th century uh that's kind of where it comes from and     
10:25     
then Rich Sutton who actually has a degree in behavioral Psych ology brought it into computer science and brought it     
10:33     
into AI applications so we're doing reinforcement learning you're basically     
10:39     
training uh an agent on some environmental stimuli and they have     
10:45     
these policies that get um basically chose you have to choose a policy to     
10:52     
address what's in the environment so when you're learning you're learning a     
10:57     
policy and you're learning from a set of policies which one is the best and so     
11:02     
you reinforce the learning with different types of things you can reinforce it     
11:08     
with uh sort of a conditional stimulus you can reinforce it with some sort of reward so there are a lot of ways to     
11:15     
reinforce the information that's being learned and then that that's included in the policy so the policy gives     
11:22     
information about like you know what you're do you know what how you respond to a certain stimulus where what the     
11:28     
reward is what the conditional stimulus is and so on so it's a really you know     
11:33     
this is something that's been very hot especially in the last couple years um     
11:38     
with respect to U you know kind of getting solving a whole new class of     
11:43     
problem so and then there are these different variants so uh in my other     
11:49     
group we're working on a a project where we're looking at multi-agent     
11:54     
reinforcement learning so this is like reinforcement learning across the group of Agents or a population of     
12:01     
Agents deep reinforcement learning of course combines sort of deep learning with reinforcement learning and you know     
12:08     
this this whole idea so that's what they're doing here so there's different variants of reinforcement learning and     
12:14     
the way they approach is a little bit different but you know it's useful I think to get um to getting some really     
12:22     
interesting results and so uh so this paper they talk about deep reinforcement     
12:28     
learning and how it's been successful in a variety of domains uh but it's not yet     
12:34     
been directly used to learn biological tasks by interacting with a living nervous system so this is something     
12:41     
where now people are taking these Advanced reinforcement learning techniques and trying to map them back     
12:47     
to biology and to behavior so that we can learn something about those domains     
12:53     
so it's really interesting the Circus route that uh reinforcement learning has been taking where you know kind of     
12:59     
taking it as a computer science tool and then now reapplying it to organismal uh     
13:07     
learning so as proof of principle we show how to create such a hybrid system     
13:13     
trained on a Target finding task so this is again a hybrid system um which     
13:19     
they'll lay out in the paper and they have this you know Target finding task     
13:24     
where the organism has to find a Target and there's some you know Criterion for     
13:30     
success and then there's a policy that describes how to make that decision and     
13:36     
then that policy is reinforced uh through some sort of you     
13:41     
know which one is sort of the best at doing it and it's rewarded and then it's Chosen and so     
13:48     
forth using optogenetics we interface the nervous system of the nematodes the elegans with a deep reinforcement     
13:54     
learning agent so they've taken the nervous system which is basically I guess simulating the connecto and then     
14:01     
adding a deep reinforcement learning agent to that um agents adapted to     
14:06     
strikingly different sites of neural integration and learn site specific activations to guide animals towards a     
14:12     
Target including in cases where the agents interfaced with sets of neurons     
14:18     
with previously uncharacterized responses to optogenetic modulation so they're using this technique it's this     
14:24     
optogenetic technique so if you're not familiar with that it's where you have have some sort of um you have cells     
14:31     
where you have some sort of protein and you use uh light spectrum to activate     
14:37     
the neurons so you activate this this fluorescent marker you activate the     
14:43     
protein it's turned basically turns on and off the neuron so you can actually     
14:49     
stimulate specific neurons or populations of neurons with an optical     
14:54     
signal and you know this is something that's very well controll controlled     
14:59     
when you have a system where you can specify the Protein that's being uh you     
15:04     
know turned on or off and you know you just pick the right sort of protein for that and you can get some uh you know     
15:12     
you can actually modulate the nervous system in that way so this is uh they've done a lot of     
15:19     
work with optogenetics in terms of characterizing responses of different     
15:24     
neurons to the stimulation so in celegans of course we know whatever neuron is and what it does and people     
15:31     
have worked on this problem for a while so they kind of know what the Char the responses are to these kind of stimuli     
15:39     
and you know the stimuli aren't necessarily a binary so it's not that you just turn it on or off you actually     
15:47     
uh stimulate the neuron and you can get this modulated response so it's it's     
15:53     
it's an interesting approach as well sort of incorporating optogenetics with     
15:58     
re enforcement learning agent uh you know computation and then of course     
16:04     
doing this in a connecto where you know all the cells um agents were analyzed by     
16:11     
plotting their learned policies to understand how different sets of neurons were used the guide movement so basically each agent learned     
16:19     
a policy and that policy basically told them how to guide their movement you     
16:25     
know what kinds of neurons to activate and so forth um and then we want to understand how     
16:31     
different sets of neurons were used to guide movement so some organisms when they made a successful movement towards     
16:38     
the target you know certain um neurons were activated and you know that that     
16:44     
corresponds to a policy in the reinforcement learning space further the animal and the agent     
16:50     
generalized to new environments using the same learned policies and food search tasks showing that the system     
16:56     
achieved Cooperative computation rather than the agent acting as a controller for a soft robot which     
17:05     
um I guess this is where you know they just talk about how the uh this system     
17:12     
generalized to new environments so basically you can use the same learn policies and food search     
17:18     
tasks the different types of food search tasks and there's this generalization that's available to it um so you know     
17:28     
you can use sort of a this kind of approach to control a soft robot you can use this kind of approach to show the     
17:36     
sort of generalizability our system demonstrates     
17:41     
that deep reinforcement learning is a viable tooth uh tool both for learning how neural circuits can produce gold     
17:47     
directed behavior and for improving biologically relevant behavior in a flexible way the first thing I want to     
17:54     
do is find the actual citation of the open paper okay it's number 50 actually     
18:01     
so that's um so yeah the first thing I wanted to show was that we have this     
18:08     
citation of openworm so this is a section here actually that's really interesting um so this section kind of     
18:16     
focuses on what deeper enforcement learning agents are able to do and then     
18:22     
what cgans nervous systems can do so deep reinforcement learning tends to     
18:27     
require large amounts of data for instance agents learning to play Atari can require thousands of hours of     
18:34     
gameplay to achieve good performance so this is from citations 33 and 34 and     
18:40     
there's a whole literature on sort of using Atari games as a benchmark for     
18:45     
reinforcement learning uh algorithms so that's probably what they're citing so they're basically saying that you know     
18:52     
to do this sort of deep reinforcement learning effectively you need large amounts of data and just kind of mention in what     
18:59     
kinds of you know if you're using a benchmark how long it takes to train the     
19:04     
model it was infeasible to collect thousands of hours of recordings in our environment and unlike video games or     
19:11     
physical systems with reliable Dynamics adequate computer simulations     
19:16     
of the Sean's nervous system and its behaviors are not available to generate training data so that's     
19:22     
50 so yeah and you know with openworm you don't really have uh you know it's it's it's not     
19:30     
really when we have computer simulations it's not really the same thing is allowing us to do this so I don't know     
19:38     
if that's like a positive thing or a negative thing but in any case I think it's the point is is that you know even     
19:44     
when you're simulating a nervous system like C elegans um it doesn't really isn't necessarily enough to sort of uh     
19:53     
you know successfully train a deep re enforcement learning agent and have it perform noral and a benchmark so you     
20:01     
know they needed to facilitate algorithm development and reduce the amount of data needed to learn the target finding     
20:07     
task so they needed to do some actual training from in in in wor actual worms     
20:15     
in a wet lab and then following approaches and supervised learning the data were then augmented during training     
20:21     
by randomly translating and rotating the animal in a virtual Arena approximately     
20:27     
the size of a 4 cm diameter evaluation or so that's what they're doing they're     
20:32     
training the worms to behave and then they're getting this training data the     
20:38     
Deep reinforcement learning agents run stable and PR to sudden performance drops uh similar to previous work so     
20:46     
they have previous work on this in simulated environments such performance crashes can be monitored using     
20:52     
evaluation episodes in the exact environment used for testing our environment evaluation     
20:58     
episodes were imp practical because they would have required many more times the amount of data than we're used to train     
21:04     
agents so this is interesting they're training these deeper enforcement learning agents they're doing this kind     
21:10     
of from uh you know these observations these direct observations are making     
21:16     
which you know often happens when you're doing modeling of a experiment that if you're very if you     
21:22     
want very specific types of data you often have to get those data through a very specific type of experiment     
21:29     
um so a lot of the data we've talked about is very general and it allows you to look at like very general questions     
21:36     
but like some for something like this you probably need a very specific dataa     
21:41     
and then what they're pointing out here is that in training these deeper enforcement learning agents run stable     
21:48     
and prone to sudden performance drops so this is in the supplementary figure one     
21:54     
um you know it's not really clear why that would be it's interesting because and so they they used the number     
22:00     
of regularization methods to help with stability and found that ensembles of Agents were effective for environment so     
22:07     
they use ensembles of Agents so there are a lot of methods that you can use for     
22:12     
this um so I'm going to go through some of the figures and see what they have     
22:22     
here so this is the basic setup of the experiment you have this uh     
22:29     
this interface between the biological neural network and the artificial neural network you have this target finding     
22:35     
task so here's uh nemato doing a Target finding task and you're mapping between     
22:41     
these you're using the data to train this network you have the seans and its     
22:47     
surroundings and this reinforcement learning agent so there's this observation reward that happens when the     
22:53     
seans does this target finding task that information is fed to the RL agent which     
22:59     
then issues an action and then we have this agent which is the it's an actor critic model where     
23:06     
you have this observation of an actor and a Critic and there's feedback from the critic to the actor that helps the     
23:13     
actor output an Adaptive action that's you know that that's where you get your     
23:19     
your uh polic your different policies from and the policies that are     
23:24     
selected uh then these this opto genetics aspect where they actually stimulate the neurons based on this     
23:31     
information from the RL agent so they do the simulation of the RL agent they then     
23:37     
use optogenetics to influence the celan's nervous system and they use this led camera so this is where their arena     
23:45     
is in this plate so we're dealing with very small scales and you know we have the stimulation here delivered by the     
23:52     
LED and it's uh delivering a signal to     
23:58     
plate and so that's basically what they're doing     
24:03     
um and then there's a lot of stuff so they're using this Ensemble approach where they're kind of you know     
24:09     
simulating multiple agents and sending a single signal to the celegans so that's basically what     
24:16     
they're doing in the experiment um this     
24:21     
actually gives a little bit more information about it you have these different targets this is their sort of     
24:27     
their task they start in the middle they go towards Targets in different directions if they     
24:33     
if basically you stimulate uh the celegans with a random light P pulse so like what you can do is     
24:41     
you can do this untrained Paradigm where you stimulate the Sean's nervous system     
24:47     
with a random pulse a random optogenetic pulse and the worm just kind of goes     
24:52     
around in from their starting position using I don't know if it's brownie in motion but it's itely this unformed     
25:00     
trajectory that doesn't really do anything doesn't move towards the target is really the point so randomite     
25:07     
stimulation know you can shut down or excite certain neurons and it kind of disrupts the nervous system the motor     
25:14     
system um with no agent meaning that you just have like this uh pulse on the     
25:21     
neurons I guess you think are going to be involved or there's no stimula whatsoever worm kind of explores it     
25:28     
environment maybe moves a little bit closer to the Target but doesn't really do you know anything significant and     
25:36     
then with the agent they show this trajectory that kind of wanders around it may Enders around but it makes it to     
25:42     
the Target uh this is with the agent and then the proper stimulation and so this is f is the     
25:49     
known policy so this is with the known policy in the reinforcement learning Paradigm where they go from their     
25:56     
starting position up to the Target they don't quite get to the Target and so this is uh this kind of describes the     
26:03     
different ways that they kind of thought this problem out so they're using this reinforcement learning agent they have     
26:09     
these known policies to get from the starting position to the Target and you notice there's a lot of noise in the     
26:15     
movement but it basically makes it towards the Target and so we have these uh you know performance metrix here this     
26:23     
is closest distance to Target in centimeters this is uh I guess     
26:28     
with agent is here known policy is here so these are two different     
26:34     
conditions uh that result in some sort of success and you see that uh the     
26:39     
distance to Target is is minimized and then you have these others no agent and random light and those are both sort of     
26:46     
higher in terms of their how close they make it to the Target so definitely uh     
26:53     
this this you know having the reinforcement learning policy in place has an effect reinfor enforcement     
26:58     
learning has an effect um so that's in figure     
27:04     
two then in figure three the system could successfully navigate different optogenetic lines to a Target so these     
27:12     
are of course with agent random light it's a random light they kind of go around the actually move away from the     
27:19     
target with agent they kind of move away from the target but come back towards the Target no agent they kind of move     
27:26     
around maybe towards the Target but don't make it to the Target it's just kind of like a random biased random     
27:34     
exploration uh so this is line four as line five is with the agent and random     
27:41     
light so there's some role here I think uh so this is for different lines of     
27:46     
celegans I believe where you know they're just testing that line to see if     
27:51     
they're you know how they perform and so I guess you know you could say with agent they make it to the Target they     
27:58     
explore around the target random light they maybe you know they don't really do anything significant and then no agent     
28:06     
they kind of make it towards the target but they don't make it to the Target and then line six you see similar     
28:14     
results um and then the system learned to navigate different optogenetic lines     
28:19     
to a target with neurons specific strategies so this is where you have specific neurons as we know in the     
28:26     
seigan nervous system we know that there's specific neurons that play a role in the in the motor     
28:32     
circuit and so we can stimulate those we can use policies to stimulate them in     
28:38     
combination but basically we have uh these different ensembles     
28:44     
here um and they you know they give you specific movements so this is the body     
28:51     
angle and the relative head angle this just shows you kind of their     
28:56     
change for each uh I guess this is like all possible movements here and then     
29:03     
they show this count of uh well they show the red the relative head angle for     
29:09     
different lines and the line so they show the body angle the relative head     
29:15     
angle um at plus or minus 180 degrees so as the body changes relative to the Head     
29:21     
angle you know what does that look like and they have these kind of visualizations in Phase space so that     
29:26     
show the wild type here where it's kind of blurry that means it's kind of at     
29:32     
random line one behaves a certain way line two and three behave a different way lines four five and six have this     
29:39     
somewhat similar to line one but it's a little bit less structured so you get     
29:45     
these different movement behaviors basically that you can extract from the     
29:50     
data and plot like this so they look at the mean L2 difference uh between the uh individual     
29:59     
agents in The Ensemble and the final trained Ensemble and then uh D is where all     
30:05     
agent policies for line 1 through six which is up here and an agent trained on wild type data where there is no     
30:11     
possible successful policy so they show this uh wild type data had no successful     
30:19     
policy and the rest of these were uh these uh basically just all agent     
30:26     
policies combined     
30:32     
and then finally this agent policies can predict agent performance on other lines     
30:37     
so this kind of shows um this uh agent training line where you have these three     
30:44     
agents uh they show the different uh neurons where you know they're they're     
30:49     
turning on specific neurons so this one you have aiy line two you have AWC on     
30:57     
and line three is here and then you have you know it goes into this uh this this     
31:05     
uh for sort of phase space and you can see that based on different neurons and different strategies that you have these     
31:12     
different patterns so for aiy for example you have this kind of striped     
31:18     
these I don't they're almost like mock bands where you know diagonally where they have this pattern and then agent     
31:24     
two and three have a similar phase space and that's based on the this AWC and you know with reference to this     
31:31     
we have this movement circuit it's it's rather relatively small it involves neurons like aiy and AWC and they're     
31:38     
connected in certain ways so when you activate or suppress one neuron it has     
31:43     
an effect on the rest of the network but those effects are different and that's kind of what you see here this this     
31:50     
portrait is you know that's why it's variable like that and it affects performance um so this is just basically     
31:57     
showing again some of the data where they have these different lines so you have a line     
32:03     
one animal with a line two agent so what you're doing is you're mapping the results from the animal to the agent the     
32:10     
agent has this phase space that represents the output and then you're com you're uh cross combining different     
32:18     
strategies so basically you're you you're taking a line one animal with the wine one intervention with agent two and     
32:25     
what that phase space looks like so you're combining these two together you can see you can make combinations of all     
32:32     
three of these and that's what they do here so this is where you have a line one animal with a line two agent this is     
32:38     
the target this is the starting point here and then it explores actually away     
32:44     
from the starting point uh the same thing with line one animal with line three agent it explores away from the     
32:50     
target line two animal with line one agent the you know it explores away from     
32:56     
the target but not as much line two animal with line three agent it actually goes towards the target line     
33:03     
three animal with line one agent it goes away from the target again and then the line three animal     
33:09     
with line two agent goes around sometimes away from the target But     
33:15     
ultimately reaching the target with significant exploration so this means     
33:21     
that you know some of these policies will match or in in the agents will     
33:26     
match the intervention with the uh you know with the uh the different neuronal     
33:32     
interventions they're making here and sometimes they don't so it's an interesting study and I can't say that I     
33:39     
really understand exactly how these map to the different policies and because     
33:44     
this is uh at this point it's kind of like they're combining so many things it's kind of hard to see what the     
33:51     
mechanism is here but in any case they're getting some interesting results um so yeah that's good and and     
33:58     
so I think that's I'm going to talk about for that paper hello Morgan let's     
34:03     
see he made it so this is an article that I don't remember where I came about     
34:10     
this but this is an interesting article about optimal transport and uh someone saying everyone     
34:17     
should learn optimal transport part one this is uh muon lee or Bill Lee and he     
34:24     
wrote this blog post on optimal transport which is something that you know it's an optimization problem so     
34:31     
it's interesting from a machine learning perspective from a computational perspective but it's also something we     
34:38     
have in biophysics it's actually a very broad topic so there's all sorts of     
34:43     
Transport in cells and in neurons and everything else where things are being     
34:48     
transported within the cell in between cells Etc and so this is something that     
34:55     
you know he kind of sells the idea of optimal transport and so it's kind of walking through what this     
35:03     
is and why it's important and I I just thought it was interesting so I wanted to bring it up in the     
35:09     
meeting so um he says in my opinion optimal transport is a seriously     
35:15     
underrated topic I think part of the reason is the way optimal transport is often     
35:21     
introduced and it's introduced usually as an optimization problem or a metric     
35:26     
on probability distributions so it's like you know we think about optimal transport we think about the     
35:32     
optimization aspect that it's this optimal process that's you know     
35:37     
optimized over time there's this uh you know we think about this in terms of uh     
35:43     
being a metric on probability distributions and there are a lot of things we can do with that in     
35:49     
biophysics so you know this is actually of interest to probability distributions     
35:55     
uh and and some of the things that were're interested with there um in a series of two posts I'm going to present     
36:02     
a message that is well understood by experts but often missed by the uninitiated and that is optimal     
36:08     
transport gives us calculus on the space of probability distributions so this is     
36:14     
important for so uh a lot of theoretical machine learning as well as a lot of     
36:19     
biophysics that we want to do you know we want to look at the calculus in the space of probability distributions as     
36:25     
well as this this sort of optimal process uh while I will Begin by     
36:31     
discussing on a high level what I meant by this message I will later present two concrete examples using optimal     
36:37     
transport as a tool of calculus that will make this very clear these two examples will form the basis of the two     
36:43     
blog posts so he gets into some of these things uh the first one is from General but weak is specific but powerful and     
36:52     
so he actually asks the question we should review while we want to have a     
36:58     
calculus in the first place the space of probability measures naturally forms a set which in the language of set theory     
37:05     
is very general so if you have a probability measure you have this sort of set of things at any given time that     
37:13     
are present however when studying probability measures we often desire more structure for example we often want     
37:20     
to equip the space of a topology of weak convergence and then this is the math     
37:25     
for that in simpler terms this def defines what it means for a subset of of P which is this probability measure to     
37:32     
be open and the structure of open sets Define a topology so yeah that and so     
37:39     
most of you reading this post have probably seen optimal transport Divine by a variant of the following problem uh     
37:47     
and so you know we we basically have two probability distributions and a set of     
37:52     
all couplings which are joint distributions and marginals list here     
37:58     
then we can Define the two wer SE distance so we use a distance metric here we can indeed show that this     
38:06     
distance metric is a metric on the probability space the space of distributions with second moments this     
38:13     
provides a specific notion of distance so we're building from this set of uh     
38:19     
this sort of uh probability metric or probability space we have these sets     
38:25     
that Define what's there at any given time and then we're moving towards a     
38:31     
topology and then we're moving towards distance metrics so we have these sets we have     
38:38     
the sets are around a topology there's a distance between the sets on that     
38:44     
topology and so uh so we have distance     
38:49     
consequently this forms a metric space as we discussed now as many of you know     
38:56     
or suspect the metric structure is much more powerful than a topology and there are many more theorems you can we can     
39:03     
prove about this metric spaces and topological spaces so topological spaces     
39:08     
are generally just kind of like spaces with some sort of form to them you know     
39:14     
it might be a curvature it might be a flat surface it might be something like a a graph     
39:21     
topology but we need to have a metric space to make sense of what that is otherwise we're just kind of pointing in     
39:27     
a certain direction saying you know we have there's another set over there or to the left you know it's very     
39:34     
disorienting so we needed a metric space to describe you know sort of the     
39:40     
location of things relative to one another at the same time ukian spaces     
39:46     
like Rd are also metric spaces but they're far easier to work with compared     
39:51     
to metric spaces key gap between ukian and Metric spaces is differentiability     
39:57     
so this is an important thing for like machine learning and deep learning is     
40:02     
differentiability so you have this distance metric you have this metric space on the metric space then you have     
40:10     
sort of a distance between two things and then you want to be able to differentiate over that distance so you     
40:17     
can know sort of the different moments of say like um we saw those trajectories     
40:23     
in the first paper so we might want to say take that trajectory knowing that we have a starting point and a Target     
40:30     
knowing that there's a a distance between them knowing that that's on a metric space and then saying things     
40:37     
about say not only the position but the acceleration and the velocity and other     
40:43     
properties of that trajectory so this is why we want to have differentiability but we also want to be     
40:49     
able to say maybe about things in time so we want to know like in a different in you know over a certain amount of     
40:56     
time you know how does this trajectory unfold because if we just show you a     
41:01     
trajectory without any temporal information you know it doesn't really make a lot of sense it's just a squiggly     
41:07     
line but if we can differentiate over time we can know that over this time     
41:12     
this period of time the agent traveled this amount of distance and with more     
41:18     
time the agent traveled more distance those distances are often not uniform so     
41:24     
sometimes the agent speeds up and slows down over that trajectory and so this is why we want     
41:31     
differentiability in fact spaces with a metric and a differentiable structure have a name they're called renan     
41:37     
manifolds and particularly we need a special inner product called the renum metric or g and an aying connection     
41:44     
which can be thought of as identifying a unique second derivative the above discussion can be     
41:50     
summarized in the table below so this is our sort of our notation for space and     
41:55     
then for a structure so we have have for example we have sets which are this     
42:01     
notation P there's no structure to a set inherently so the set is just a     
42:06     
collection of things then there's this topological space which is p and to and     
42:13     
that's an open set so the open set allows for things to move around for it to divide and move around in that way     
42:21     
it's on this topological space so the set isn't just like a single thing     
42:27     
collection of things but you know there's this sort of notion of spatial     
42:33     
distance or of temporal distance but it's a space that doesn't have a metric so then we have metric spaces the     
42:40     
metric spaces are p and d and the structure there is distance so now we     
42:45     
have a distance between those open sets so when we have one set that maybe becomes two sets and drifts apart we can     
42:54     
then detect what the distance is between those two sets and we can say if it's     
42:59     
positive or negative or if it's to the left or to the right and then finally the r many and manifold which is beyond     
43:07     
a metric space these three notations here is triplet and that's gives us     
43:12     
calculus so once we have distance on a metric space then we can do calculus we     
43:17     
have we can do differentiation we can do other types of integration we can do     
43:22     
other types of operations that are I guess you could say calculus like if not     
43:28     
true calculus so basically you know we can do this leads us to this mathematical     
43:34     
representation now to fully develop the theory of the renan manifold structure of optimal transport sometimes called     
43:41     
autoc calculus requires a whole book which I to you here uh but however I     
43:46     
will sketch out a concrete example next to illustrate an application of the Calculus Tools that optimal transport     
43:53     
provides and so kind of talks about an optimization for example we care about     
43:59     
the convergence of gradient descent and of course we care about gradient descent in machine learning and in deep learning     
44:05     
as well because this is the way we kind of figure out what our you know how to minimize our loss in the algorithm so as     
44:13     
the algorithm explores the problem space it's going towards the minimal point in     
44:19     
that gradient and so there's this gradient descent process uh but also in optimization we also have a gradient     
44:26     
descent process we have these gradient Flows In continuous time so we need these     
44:31     
trajectories we need this metric space we need these renum manifolds uh it     
44:36     
turns out the exact characterization of the exponential convergence uh is due to     
44:41     
this Factor here cited in this citation we will Define the inequality as follows     
44:48     
um so there's this inequality the PO inequality poak and LTZ and so this is this is where the CED     
44:57     
here once again the importance of this PL inequality is that exactly characterizes the exponential     
45:03     
convergence rate of gradient flow and so then you know suppose XT is the solution     
45:09     
of the gradient flow with respect to the potential of this and the initial condition of this so here we have the     
45:16     
equation here then F satisfies a PL inequality with constant Alpha larger     
45:21     
than zero and if and only if gradient flow converges exponentially fast for     
45:27     
all initial conditions while this may seem a bit mysterious at the moment this     
45:32     
character characterization is that a derivation is straightforward is a straightforward exercise in calculus in     
45:39     
fact we can quickly sketch the forward Direction so we can sketch out the time     
45:45     
derivative we can look at the that with respect to the pl inequality and then we     
45:51     
can look at this in terms of grown walls unol which is the desired exponential convergence result so you can do I mean     
45:59     
this is pretty much mathematics and kind of finding equivalence and equivalencies     
46:05     
but this is this is why this is interesting so uh that's one point the     
46:11     
second point is on the convergence of uh langen diffusion so a widely studied     
46:17     
process in physics statistics and machine learning is the langen diffusion defined by the following stochastic     
46:24     
differential equation which is here so these are differential equations that have a stochastic component to     
46:30     
them uh where BT is a standard brownie in motion so you have so for the the     
46:36     
stochastic component here you have a brownie in motion that we saw in the last paper where there's this movement     
46:42     
around a a central point and it's it's stochastic in nature so it kind of     
46:47     
squiggles around that point and so you know we plug in brown and motion to this     
46:53     
stochastic differential equation and this describes uh langen diffusion and     
47:00     
then effectively it's a gradient flow plus browni and noise now there are many reasons why uh     
47:08     
Lang given is nice but one particularly helpful property is the stationary distribution uh is the Gibbs     
47:14     
distribution which is here and this leads us of course to thermodynamics and Gibbs free energy um     
47:21     
it's rare to be able to characterize a stationary distribution of a stochastic process in such an explicit nice closed     
47:28     
form we can use this to show many desirable properties however when when the time T is finite how good is Langan     
47:36     
at approximating the stationary distribution so they move from here they     
47:41     
move from sort of this diffusion process to stochastic differential equations to     
47:47     
a Gibs distribution that describes these probability     
47:53     
distributions and so this gives us this exponential conversion Ence and then we can look at exponential     
48:01     
decay in entropy so we can start to move towards entropy and move towards     
48:06     
callback legal Divergence so K Divergence of course is used widely in machine learning but also describe sort     
48:13     
of the Divergence of two systems under entropy and so you know there are these     
48:18     
connections between physics and thermodynamics and machine learning and     
48:24     
generalized learning so there are a lot of interesting connections here uh and I     
48:30     
you know I'm not going to go on too much more about this but I think you know     
48:35     
kind of gives some final words here I think the most Insight I gained through learning optimal transport is the fact     
48:42     
that calculus is powerful the DraStic simplification of several well-known pdes or partial differential equations     
48:50     
using wer Ste gradient flow which we didn't talk about up here but it follows from the discussion we had about wer     
48:58     
um uh distance and so Maps set to this gradient flow is an incredible     
49:05     
breakthrough we have just observed one important application of it for the fuer plank equation I hope this post is     
49:12     
already sufficient motivation for everyone to learn optimal transport so that being said he has     
49:18     
another blog post uh that is on the topic and I won't go through that blog post I just wanted to go through the     
49:24     
first one in the next next post I intend to explore the geometric aspect of the bazer Ste manifold     
49:31     
interpretation then discuss discuss an application to meanfield neural networks this is where you get into neural     
49:37     
networks finally and sort of the idea that there are these physical uh you     
49:43     
know they have these physical properties to them and they can be used as physical uh mechanisms for     
49:50     
learning furthermore I intentionally left out many important steps and details in particular the wer Ste     
49:56     
gradient can be computed explicitly there are many nice references to Optimal transport today     
50:02     
that cover these details but I strong strongly recommend V3 as an accessible introduction so this     
50:09     
is a sort of a computer science type of ref way of putting references in a paper     
50:15     
it's basically the first three letters of the authors so uh bg14 is bakri gento     
50:23     
and Leo 2014 so it's bg14 this is V3 which is if we go down here it's     
50:31     
volani uh topics and optimal Transportation the American mathematical Society so um we have other you know we     
50:40     
have polic radiant methods for minimizing functionals we have Auto the geometry of     
50:46     
dissipative evolution equations forus medium equation a top topological     
50:52     
property of real analytical subsets the variational formulation of the fuer plank equation that's in the cam journal     
51:00     
Jordan Kinder Lair and auto and then this paper by baky gentil and Leo     
51:06     
analysis and geometry Markov diffusion operators so there's a lot of heavy math     
51:12     
in that but I think it's like kind of describes this sort of the the Practical     
51:17     
usefulness of uh optimal transport and how it leads us from single like sets of     
51:25     
objects or collections of objects to calculus and to looking at things like     
51:31     
gradients and optimization and Beyond     
51:38     
so I know that was pretty heavy stuff I didn't expect to you know get very     
51:45     
deeply into it so there's probably a lot there that I didn't really hit     
51:54     
on we have any comments or questions about any of     
52:03     
that yeah it was yeah but that's you know like if you go into theoretical machine learning they     
52:10     
really get heavy with math especially also with biophysics um but I think you     
52:17     
know there are some takeaway messages there where you go through this sort of process of you know looking at a problem     
52:23     
and you know even like thinking about like the biophysics of development you know we have like these collections of     
52:30     
things that we're observing like cells or proteins or whatever and then they     
52:35     
you know you can build upon that you can say okay we're modeling this collection of things and then there's this     
52:42     
biological process it could be like cell differentiation it could be protein     
52:48     
diffusion there a lot of things that you can model using these methods and then     
52:53     
you know you build this sort of metric space you build this these other tools     
52:59     
and then you can build models of like diffusion models of radiant descent and you can get you know you can     
53:06     
optimize uh your model and then you can end up with some pretty sophisticated     
53:11     
analysis of that so you know it's really an exercise and model building as well     
53:18     
um the the stuff I showed several weeks back on building an optimal cell that's     
53:23     
the kind of thing they're doing there except a lot of it's under the hood of software so a lot of the mathematics are     
53:29     
in software and you plug in data and you plug in parameters and you run it and     
53:35     
then you know you get these answers and same truth same is true with multiphysics as well where you get these     
53:42     
um you know you don't have to deal with solving equations you just have to kind of know what they do and then run them     
53:49     
and then get an answer and so there's all there all sorts of techniques then you would use besides that and modeling like core     
53:57     
screening or optimizing the parameters or you know     
54:03     
whatever right well thanks for attending and uh Susan okay no I didn't have anything     
54:11     
else okay thank you for that overview that looks interesting no problem okay thank you bye have a good week     
54:22     
bye thank you     
54:44     
I think all right so I'm gonna draw out on     
54:51     
the board a little bit about what we've learned from this optimal transport     
54:58     
so basically what we're trying to do is we're trying to build probability     
55:04     
distributions which give us some sort of structure or structural     
55:12     
information all right so that's what they're getting and so structure just simply     
55:18     
means if I have like set of data that exists in     
55:24     
clusters right I can divide this up in a number of     
55:30     
ways I can look at different clusters I can at the probability that some point     
55:36     
is in a cluster so I can define a cluster in some way using a centroid and     
55:41     
I can define whether you know I'm classifying some point in a     
55:48     
structure but I can also say you know have these sets and say you know what kinds of you     
55:57     
know how what like How likely is it that any one of our dis our particles are     
56:03     
within that set and we can have you know statistics about the distribution of the     
56:08     
set so you can say a lot about structure spatial structure and even temporal structure because these sets evolve and     
56:16     
we'll see this in a minute but basically what we want to do is we want to find structure say in transport or in some     
56:23     
other system so if we think about like a p where we have transport going through     
56:30     
the PIP you know we might have particles along for the ride and they might be     
56:37     
going down this pipe at a certain rate but you know the distribution of     
56:44     
particles is not even it's sort of clustered like this in some     
56:49     
ways you know you have some point some some particles that are closer together     
56:55     
than others there isn't this sort of uniform distribution of the particles     
57:01     
and so that can lead to things like uh turbulent flows because you can have     
57:07     
barriers within the pipe you can have edes form and you can go from this sort of laminer flow that I show here more of     
57:14     
a turbulent FL and so this is where the structure becomes Amplified so that's     
57:19     
where we're trying to look at with with respect to this so you know he kind of gives a a     
57:28     
sort of a process of thinking about different mathematical tools and how     
57:33     
that relates the     
57:40     
structure thinking about different mathematical tools and how that relates to you know sort of getting at that     
57:48     
structure so we starts with the notion of a set and of course a set is a collection of things     
58:00     
that collection of things are you know pretty generic but it doesn't have any     
58:05     
structure at least inherently a set can have structure if it's as I showed you like if it's part of the cluster or if     
58:13     
it's maybe something that has you know some relevance relative to the rest of     
58:19     
the system but in general when we Define a set it's a collection of things and it     
58:25     
doesn't have any structure then he talks about topological     
58:32     
spaces which of course are have open they're they're sort of a     
58:38     
collection of open sets and open sets don't have structure     
58:44     
inherently but they do allow you to have you know you can think of sets as being like these static     
58:50     
things all right and open sets are may be more dynamic     
58:56     
so if we think of our favorite example of cell division we can think of like the mother     
59:04     
cells being maybe like the initial set and then it's it's an open set in the sense that whatever is in that original     
59:11     
set can bifurcate into daughter sets or daughter cells and along with those     
59:16     
cells of course you have all sorts of things like proteins and cytoplasm and other things that go along and sort of     
59:25     
uh you know change their identity in this in this fashion so once you have this     
59:31     
topological space which is results from having these sets that are sort of     
59:38     
relational and you have the space you have to have some sort of topology that they operate     
59:43     
on then we can move to metric spaces so metric spaces of course     
59:50     
are going to have a little bit more relevance to analyzing     
59:57     
structure so we have our distance metric which takes these open sets and allows     
1:00:04     
us to make statements about how far apart they are or how far they've kind     
1:00:10     
of diverged so you can see this where you have these different sort of calculations of     
1:00:18     
distance and so the distance metric is you know allows us to say things     
1:00:24     
relative to one another so you know you can have top topological spaces that have these relational aspects but the     
1:00:32     
metric space actually allows us to put numbers on a distance instead of just     
1:00:37     
saying that it's diverged or that's to the left or that's to the     
1:00:43     
right then you have these renan     
1:00:51     
manifolds and these are parts of a metric space that allow us to sort of     
1:00:57     
derive a calculus and so once we have these distances between these open sets on a     
1:01:05     
metric space we can then take say this trajectory and analyze it by     
1:01:15     
differentiating or by integrating and so forth so we can do     
1:01:20     
all sorts of things uh that you can do with Calculus on these trajectories     
1:01:29     
and so this is this is where this becomes sort of important for reconstructing     
1:01:34     
our probability distribution because then we can say thing that you know different things are     
1:01:41     
different properties um it could be a renan manifold that you know allows us     
1:01:46     
to get information about like Divergence could be about sort of the temporal     
1:01:53     
structure so for example like said previously you know it the Divergence of     
1:02:00     
these two uh these two open sets are not uniform     
1:02:05     
one might diverge at a faster rate than the other there might be leaps and and and starts and different things and I'll     
1:02:12     
talk about a paper in a little bit that kind of goes over the reality of what we     
1:02:18     
have in cell biology with respect to how cells and cell populations behave a soft     
1:02:25     
active materials and then finally we have our Gibbs     
1:02:31     
distribution which allows us to do two things allows us to     
1:02:39     
reconstruct probability distribution so we have a probability     
1:02:47     
distribution how are things distributed in time or in space it's usually a spatial array that we're interested in     
1:02:53     
reconstructing and then we have     
1:02:59     
our thermodynamic aspect and so this allows us to sort of     
1:03:07     
figure out how things are distributed in terms of energy so you know we usually     
1:03:13     
have in stat Mech we'll have this energy landscape with     
1:03:18     
Minima and we have particles that kind of start at an initial point and then they kind of minimize     
1:03:26     
their energy accordingly they move to the lower the lowest parts of this     
1:03:31     
energy landscape and then we calculate the distribution of all those points     
1:03:36     
there's an underlying distribution here and that tells us something about sort of the     
1:03:46     
energetics but there's also a probability distribution which is is connected to     
1:03:52     
this but in something like machine learning or deep learning learning the probability distribution isn't     
1:03:58     
explicitly connected to the energetics so we can use the Gibbs distribution or we can use this kind of model to     
1:04:04     
calculate a probability distribution calculate a gradient descent without worrying too much about the energetic     
1:04:10     
aspect we just assume that things are uh optimized things that you know they're     
1:04:16     
minimizing their loss or minimizing their energy appropriately so we don't worry about the thermodynamics     
1:04:22     
explicitly here but what we do have is we have a model of probability distribution and a model of     
1:04:28     
thermodynamics all in one place and of course then we can do the calculus we can do you know talk about properties of     
1:04:35     
structure and then do the analysis for that now I'm going to highlight a very     
1:04:41     
long biophysical review article just kind of going over the uh table of     
1:04:46     
contents and talking about its relevance cell biology and then the relevance to what we're talking about with optimal     
1:04:53     
transport so optimal transport of course has relevance to cell biology both in     
1:04:58     
terms of because there is there are transport processes that we're interested in it's also you know useful     
1:05:04     
in modeling because then we can take biological processes and Abstract them to mathematical structures and     
1:05:11     
mathematical tractability but then we also have this aspect     
1:05:16     
of uh biology as a soft active material or biological properties of soft active     
1:05:23     
materials and that's what we'll talk about in this article so this article is called life off the     
1:05:29     
Beaten Track in biomechanics imperfect elasticity cytoskel glassiness and     
1:05:35     
epithelial unjamming so this is where they talk about elasticity the elastic     
1:05:40     
properties of tissues in the biomechanics um that you have a lot of     
1:05:46     
hysteresis so you don't have like perfect elasticity but you do have these properties that are interesting uh in     
1:05:53     
terms of you know if you deform form tissues uh you know how do they respond     
1:06:00     
then you have the cytoskeletal glassiness so this is where we're talking about spin glass models and the     
1:06:06     
properties of of glasses and linking that to how cytoskeletal components     
1:06:12     
behave menum of epithelial and jamming of course we've talked about jamming phase transitions in the past and how     
1:06:19     
those jamming phase transitions uh mediate migration of cells in she     
1:06:26     
and in populations of cells that are migrating and developing so as I said this is the biophysics review     
1:06:33     
article and the abstract reads textbook descriptions of elasticity viscosity and     
1:06:39     
viso elasticity aild to account for certain mechanical behaviors that typify     
1:06:44     
soft living matter so we have our typical like types of     
1:06:50     
behaviors in sort of inorganic systems um but the mechanical behaviors aren't     
1:06:58     
really you know don't really hold up when we're talking about tissues and I think Susan's experiencing this of     
1:07:03     
trying to model biotensegrity as opposed to mway tensegrity where you have a     
1:07:08     
mechanical system like a structure like a a building or some sort of mechanical     
1:07:14     
structure that exhibits tensegrity when you try to apply to the biological context that doesn't really work as     
1:07:21     
well that doesn't mean that that model is wrong it just means that we don't have the right parameters we have the     
1:07:26     
right concept of how these things work in soft living matter here we consider     
1:07:33     
three examples first strong empirical evidence suggests that within lung paranal tissues the frictional stress is     
1:07:40     
expressed at the microscale or fundamentally not a viscous origin second the     
1:07:48     
cytoskeleton or CSK of the airway smooth muscle cell as well as that of all     
1:07:53     
eukaryotic cells is more solid like than fluid like yet the elastic modulus is     
1:07:59     
softer than the softest of soft rubbers by a factor of 10 4th 10 the     
1:08:05     
5th so this is where you know we kind of talk about these systems uh we have these sort of     
1:08:12     
properties that are not exactly consistent with what we think of when we     
1:08:17     
kind of model them so lung perinal tissues there are frictional stresses     
1:08:22     
that are expressed at the micro scale these are not viscous origin so we have     
1:08:28     
uh you know we have to model this appropriately we also have the cytoskeleton of the airway smooth muscle     
1:08:34     
cell uh and and as well as all eukaryotic cells that are more solid like than fluid like so they have     
1:08:41     
they've undergone this sort of jamming phase transition and they they have the solid like aspect yet the elastic     
1:08:48     
modulus is very soft and so you would think it would be you would have to model it as a fluid and in fact you     
1:08:54     
would model it as a Sol ID that has a very soft elastic modulus so this is     
1:09:00     
something that you know again uh if you use something like a multiphysics platform may or may not be able to uh     
1:09:08     
calibrate your models appropriate moreover the eukaryotic CSK     
1:09:13     
expresses power law rology innate malleability and fluidization when shared so it has all these properties uh     
1:09:21     
these sort of power law responses uh that have a a lot of sort of rare events     
1:09:26     
or rare modes in them this sort of malleability and fluidization when sheared that don't occur in uh comp     
1:09:36     
comparable sort of inorganic physical or mechanical contexts for these reasons     
1:09:44     
taken together the CSK of the living eukariotic cell is reminiscent of the class of materials called Soft     
1:09:51     
glasses thus likening into inert materials such as clays pastories emulsions and Foams so this is     
1:09:58     
a type of glass called Soft glasses we often think of like Clays or emulsions     
1:10:05     
or Foams as being comiment so again we have these models we know how they     
1:10:11     
behave but you know this is closer to what we're looking for than you know our     
1:10:17     
typical sort of mechanical models that we use thir the cellular Collective     
1:10:23     
comprising a conf confluent upath the oiil lay can become solid like and jammed fluid like and unjammed or     
1:10:30     
something in between so like I said we have these jamming phase transitions that when you get enough     
1:10:37     
particles in a certain like at a certain density it jams the material so if you     
1:10:42     
have a loose collection of particles it be behaves like a fluid it flows when     
1:10:48     
you get enough particles sort of at a critical mass they become jammed and     
1:10:53     
they behave like a solid and a lot of systems like with cell migration or with     
1:10:58     
material properties they can go in between these two modes so they can     
1:11:04     
become jammed and unjammed at different phases of their their function and their development so we have these aspects of     
1:11:11     
the epithelial layer we have some and sometimes they can be solid like and jammed or they can be fluid like and     
1:11:18     
unjammed or they can have this intermediate uh phase of behavior which     
1:11:23     
is I guess the UN stable phase of sort of the phase transition esoteric though each may seem     
1:11:31     
these discoveries are consequential in so far as they impact our understanding of bronchospasm and wound healing so you     
1:11:39     
know wound healing is another component where you or another example of where you have a lot of these kind of     
1:11:45     
behaviors uh other than development so wound healing you have a lot of migration of cells to the injury site     
1:11:52     
you have a lot of different types of cells operating there and you have a tissue physics you have to integrate     
1:11:58     
that into an existing tissue often with some sort of topological aspect to it so if you have     
1:12:04     
a wound on your skin you know in an area where your finger is curved or you know     
1:12:10     
you have a fold in the skin then you know that's something that those cells have to respond     
1:12:16     
to there's some interesting connections there between something like wound healing the soft material aspects of it     
1:12:24     
and then what we're talking about in the optimal transport article where they     
1:12:29     
talked about you know you can go from say like topological space to these metric spaces to these models that allow     
1:12:37     
for calculus and uh probability distributions based on uh thermodynamics     
1:12:43     
so it's an interesting kind of extension of this uh cancer cell Invasion and     
1:12:49     
embrionic development are also sort of characterized by the soft active material     
1:12:56     
um and so yeah moreover there are reasons to suspect that certain of these phenomena first arosed in the early     
1:13:03     
protest as a result of evolutionary pressures exerted by the primordial micro     
1:13:08     
enironment so these uh phenomena that we see in um soft living matter or tissues     
1:13:16     
of soft living matter or cells of soft living matter they go back very far in     
1:13:22     
evolutionary history so they probably occurred early protests and so what that     
1:13:27     
means is that very early in the evolution of life we get cells the cells     
1:13:33     
can be you know these rigid structures they could be they could be mechanical I     
1:13:39     
suppose but they're not they have these uh soft active material properties and     
1:13:45     
so it's really interesting to think about how some of these things evolved so how some of these uh types of     
1:13:52     
materials evolved and why they behave in the mostes they behaven and so the primordial micro environment sort of     
1:13:59     
forced a lot of these things to be selected for and so this is an     
1:14:04     
interesting aspect of this we have hypothesized further that each then     
1:14:09     
became passed down virtually unchange to the present day as a conserved court process so these are properties that are     
1:14:17     
conserved widely in the tree of life and you know for good reason because it allows for these biological systems to     
1:14:24     
be flexible and to do things like self-repair and other other types of aspects as well um these topics are     
1:14:32     
addressed here not only because they are interesting but also because they track the Journey of one laboratory along a     
1:14:38     
path Less Traveled by so I guess that means that they've engaged in a lot of cutting edge     
1:14:43     
research when this wasn't as mainstream as it is now um so they have a lot of     
1:14:50     
really interesting things in the table of contents they talk about the imp perfect elasticity of lung perinal     
1:14:57     
tissues and this has relevance to tissue friction which is where you know the     
1:15:03     
tissue moves against itself or you know in different uh parts of the tissue move     
1:15:09     
against different layers of tissue um there are classical descriptions of course which are more or     
1:15:16     
less inadequate and so you need to be able to make you know change those     
1:15:23     
models and then there's this concept of tissue hysteresivity which is of course     
1:15:29     
hysteresis in tissues and its Dynamic response uh there's hyst resistivity     
1:15:38     
smooth muscle contraction and actomyosin cycling so Hyer resistivity means that     
1:15:44     
it's resisting hysteresis um we have this is in the context of smooth muscle     
1:15:51     
contraction and actomyosin cycling so we have this model the hhm computational     
1:15:56     
model that talks about this and they introduced that in chapter 3 chapter     
1:16:01     
four is perturbed equilibrium of meios and minding five is a hard day in the life of a soft cell this is where they     
1:16:08     
get into the idea of soft cells where you know we talk about how what the     
1:16:14     
hardness or softness of the cytoskeleton is because a cytoskeleton is you know     
1:16:19     
composed of a number of composite uh macromolecules that kind of     
1:16:25     
get configured in a certain way so you know when we're talking about biotensegrity we're not really talking     
1:16:31     
about the softness or hardness of the cytoskeleton we're talking about the structural stability of the cytoskeleton     
1:16:38     
so this is something where we can explore the cytoskeleton in terms of you     
1:16:43     
know maybe it doesn't behave like a     
1:16:51     
a an inorganic phys uh mechanical system maybe it has its own sort of properties     
1:16:59     
um and so you know there's also this aspect of mapping the cytoskeleton to soft glass models that we have in     
1:17:07     
physics um and of course we have this plastic response of course so it's not     
1:17:12     
just this static cytoskeleton we have a cytoskeleton that's plastic and it     
1:17:18     
changes with respect to forces so unlike our inorganic mechanical systems it's     
1:17:25     
capable of self-repair it's capable of the sort of deformation that can it can recover from and so forth um and then of     
1:17:34     
course we have the role of water which we don't think of too much but of course it's extremely important in biological     
1:17:41     
systems because biological systems especially tissues are immersed in water     
1:17:47     
and it's not something we worry about that much at least not in terms of uh     
1:17:52     
you not something we worry about too much in inorganic mechanics usually     
1:17:57     
water is maybe compartmentalized in those kind of models or it has a specific interface and so this is     
1:18:04     
something that we have to think about um so stretch fluidizes the     
1:18:12     
cytoskeleton that's uh chapter six chapter seven is mapping physical forces     
1:18:17     
that drive Collective cell migration so then they talk about cellular migration     
1:18:23     
epithelial jamming unjamming and so there's this phase transition that     
1:18:29     
exists and this kind of talks about some how this sort of was discovered and sort     
1:18:35     
of the Dynamics of it uh there's a theory uh unjamming transitions cell     
1:18:41     
shape in the vertex model so they talk about this model in     
1:18:47     
the fruit fly embryo the genomics of the unjamming transition model and the     
1:18:52     
unjamming transition and cancer ression so they give a nice set of model organisms and model systems     
1:18:59     
here uh then they talk about eukaryotic Evolution which I think is probably the most interesting chapter at least to me     
1:19:07     
soft matter physics and eukariotic evolution so they talk about the cytoskeleton as a soft glass and this is     
1:19:15     
of course the basis for sort of thinking about maybe biotensegrity is being     
1:19:20     
fundamentally different from inorganic mechanical tensity     
1:19:26     
you have rigid or soft which means just kind of like what's you know if you if you look at the origins of life should     
1:19:34     
organisms be rigid or should they be soft should they be you know mechanical     
1:19:40     
or should they be sort of Their Own Thing biologically soft as we think about it today so you know we think     
1:19:48     
about like molecular machines we think about cism is perhaps enclosing those     
1:19:53     
molecular machines and what's to say that that shouldn't have been maybe like a very rigid hard     
1:19:59     
shell or should it have been an Adaptive soft shell those are all things that happen very early in evolution at least     
1:20:06     
in new caros there advantages of being soft of course they are soft ecosystems and     
1:20:13     
strong selective pressures for the softness then then these conserved core     
1:20:19     
processes that lead to this sort of cytoskeletal softness throughout the     
1:20:25     
tree of life or at least through UK chariots so it's interesting to think about what those conserve core processes     
1:20:32     
are they're you know a lot of molecular Pathways that are involved in making these soft materials that you know we     
1:20:39     
have yet to learn soft materials are very important for example in in organic     
1:20:45     
chemistry and in biophysics or in other parts of physics there's a soft matter physics field which is distinct from     
1:20:51     
biophysics and so you know you could learn perhaps how to design design materials if you understood those core     
1:20:58     
processes and sort of how proteins are built and so forth um why is biology innately messy     
1:21:07     
and so they talk about some of these aspects of the messiness of biology so some of that is that it's poised on the     
1:21:13     
edge of chaos that there's always sort of always in the state of like kind of     
1:21:19     
the transition between jamming and unjamming there are a lot of other sort of things at the edge of chaos that are     
1:21:26     
interesting in biology we'll get into that here and there is a lot of literature on this actually but that's     
1:21:32     
one of their reasons and then of course this need to be plastic to be malleable and to be able to remodel on demand so     
1:21:40     
this is where you know we get into this aspect of plasticity but also being at the edge     
1:21:46     
of chaos kind of an interesting fora into this sort of considering why things     
1:21:52     
are the way there so they they kind of to finish up we talk about this table here and this is     
1:22:00     
sort of their own Reflections on their own work but they talk about these eight con     
1:22:05     
eight consequential discoveries and basic biophysical Sciences which I think are really interesting and so you know let's go     
1:22:13     
through them so the first one is fuctional stresses expressed within lung paranal tissues are tied directly to the     
1:22:21     
tissue elastic stress and not to the overwhelmingly fashionable and then the     
1:22:26     
was mistaken notion of a tissue viscous stress so they're thinking about this in     
1:22:32     
a different way they're thinking about tissue stresses on lung pral tissues friction within the lung pinkal tissues     
1:22:39     
so how things move against one another and so people typically use this measure of tissue viscous stress which is fine     
1:22:47     
if it's a viscous material it it describes certain aspects but you can actually look at the tissue elastic     
1:22:54     
stress and what they're arguing is that's a better measure for frictional     
1:23:01     
stress number two is that fictional stress expressed within activated Airway smooth muscle which is     
1:23:08     
ASM uh precisely tracks and thus compromises a window     
1:23:13     
into and thus comprises a window into acto myos and cycling rate hydrolysis of     
1:23:20     
adenosine triphosphate or ATP the actomyosin cycling an unloaded     
1:23:26     
shortening velocity basically frictional stress     
1:23:31     
within Airway smooth muscle actually allows you to look at some of these aspects of     
1:23:39     
metabolism number three activated ASM and the healthy lung attains a length     
1:23:44     
that is equilibri not by a balance of static forces or rather becomes lengthened dynamically in response to     
1:23:51     
load fluctuations associated with the act of ordinary spontaneous breathing so     
1:23:56     
you know in your typical static model you would balance out forces and you would observe them and you say okay     
1:24:02     
there's an equilibrium what they're saying is that there's actually more of this dynamic     
1:24:08     
equilibrium that is dynamic over time so if you have load fluctuations in time     
1:24:15     
you have this Dynamic response to those load fluctuations and so rather than using a static model you would use a     
1:24:22     
dynamic model and so the where this differs so you can actually get a better     
1:24:27     
account of what's going on using a dynamic model so you know sometimes     
1:24:33     
things operate in a sort of a static equilibrium so for example when you're breathing normally that the balance of     
1:24:41     
static forces model describes that pretty well but during a spontaneous asmatic attack ASM can become     
1:24:48     
excessively shortened stiffened and frozen into a solid like static muscle State called Latch so this looks at     
1:24:55     
things where you know outside of the normal mode of operation so if you have     
1:25:00     
a spontaneous asmatic attack the tissue actually inter goes this jamming phase transition where it     
1:25:08     
changes its nature from a fluid to a solid or a solid like State and so this     
1:25:14     
is an interesting point when you're looking at these out of equilibrium uh     
1:25:21     
instances number four the cytoskeleton or CSK of the living cell expresses     
1:25:26     
certain Universal features including small stiffness coral orology and innate     
1:25:32     
malleability which we talked about reminiscent of soft glasses so this     
1:25:38     
these are the properties of the cytoskeleton that they've observed um very distinct from you know other types     
1:25:46     
of uh materials and certainly from inorganic mechanical     
1:25:52     
systems number five in response to the transient stretch and hold the cytoskeleton of the living cell     
1:25:59     
expresses prompt nonlinear strain stiffening followed by slow stress relaxation so you have this nonlinear     
1:26:06     
response to stiffening and relaxation so this is the cytok skeleton's response um     
1:26:13     
and this is very different than something like transient stretch and hold um but in response to transient     
1:26:20     
stretch and unstretch with zero residual strain the say skeleton reveals a simple     
1:26:25     
Universal response namely fluidization so if we think of the cytoskeleton as sort of a solid uh solid     
1:26:34     
Network you know we can use transient stretch and hold to you know sort of     
1:26:41     
probe what it does and what we get are these nonl not only these nonlinear modes of     
1:26:47     
response but we get this fluidization so it behaves sort of     
1:26:53     
differently under this assay so this is an interesting sort of aspect of     
1:26:59     
cytoskeleton cytoskeleton fluidization is attributable in part to the severing of acting filaments by the action of uh     
1:27:06     
cofin which occurs predominantly during the unstretched phase so as you go through the phases of a response to some     
1:27:13     
sort of peration um the cytoskeleton behaves differently changes its material phase     
1:27:20     
and other things happen as well number six cells comprising a confluent     
1:27:27     
epithelial air tend to migrate collectively along orientations that minimize local implan in or cellular     
1:27:33     
sheer stress or what they call pthot taxis and tend to exert tractions     
1:27:38     
pulling towards any nearby self-re void or kenot taxis so basically is cells uh     
1:27:46     
epithelial cells migrate collectively along different orientations they try to minimize you     
1:27:53     
know pth taxes and tend to exert tractions that     
1:27:58     
lead to the kenot taxes so they're basically uh minimizing sheer     
1:28:07     
stress and pulling themselves towards any sort of self-re void so they're     
1:28:12     
pulling themselves towards open space they're minimizing sheare stresses     
1:28:18     
and so they're sort of amplifying their migration number seven the confluent     
1:28:25     
epithel lay and mature and unperturbed tends towards a solidly collective cellular phase that is jammed in such a     
1:28:32     
jammed phase cells are crowded and trapped by their neighbors and thereby become immobilized and stuck in place so     
1:28:40     
jamming the jamming phase is where you get this critical level of density of     
1:28:46     
particles or in this case cells epithelial cells they become jammed even     
1:28:51     
when trying to migrate and so become immobilized and stuck in place in the     
1:28:57     
sheet but by a variety of means such a layer can undergo an unjamming transition or     
1:29:02     
ujt where in the cellular Collective fluidizes and migrates much as a flowing     
1:29:08     
fluid so you get these transitions between this jamming where they become stuck and then this unjamming where they     
1:29:14     
become unstuck and so this is managed by a single     
1:29:20     
uh parameter that we can measure and uh you know but it but any fluid any sort     
1:29:27     
of migratory process of cells under goes multiple jamming and unjamming     
1:29:32     
transitions as the sort of the environmental uh conditions required in     
1:29:39     
doing so cell shapes become systematically more elongated and more variable moreover cells with the     
1:29:45     
unjammed layer retain a purely aelo phenotype including intact cell cell     
1:29:50     
Junctions intact barrier function and no other idence of mesenchimal markers so     
1:29:56     
the ug there is distinct from this epith of mesenchimal transition so these phase     
1:30:03     
transitions these jamming phase transitions are independent of transitions between epithelia and mezen     
1:30:10     
cells so this is interesting because you know we can decouple this process and     
1:30:16     
understand the mechanisms um and so yeah they retain a pyop theal     
1:30:22     
phenotype so we can look at the phenotype and say that there's no difference in the phenotype between     
1:30:29     
jamming and unjammed and so this is interesting especially with respect to some of the stuff we've been talking     
1:30:34     
about again with uh biotensegrity that you know we have to     
1:30:39     
consider maybe this is cell type independent or maybe there are some     
1:30:45     
really interesting properties of cells that we haven't really discovered     
1:30:52     
so um and then finally number eight the confluent epithelial aay ordinarily     
1:30:58     
stands poised in a disordered state that is just at the brink of unjamming ready     
1:31:03     
to unjam in migrate and response the cues associated with wound healing umic     
1:31:08     
development tissue remodeling or cancer progression proximity to jamming can be     
1:31:14     
Quantified by cell shape and shape variability uh so this is where we talk about cell shape being poised at the     
1:31:21     
edge of a phase transition uh where you know you you like there's this by     
1:31:26     
stability where the cell has a certain shape and it's ready to go or has another type of shape and it's not ready     
1:31:32     
to go in this case what they're saying is a confluent epithelial layer is     
1:31:38     
always poised in a disordered state so it's sort of at the edge of chaos operating at the edge of chaos and it's     
1:31:45     
just at the brink of unjamming so if it's sitting in this confluent layer stuck in place it's ready to unjam when     
1:31:51     
it's needed and so this is key to this sort of plastic response it maintains     
1:31:57     
Integrity when it needs to be but then if there's a plastic response that's needed the cells can be recruited and     
1:32:03     
they can change their shape and uh you know go undergo this phase transition so     
1:32:09     
this is really interesting uh work and I think these points get sort of they talk     
1:32:15     
about them more in the paper and I don't want to get very deeply into this but I think they're very interesting for     
1:32:20     
followup especially with respect to biotens integrity and related     
1:32:26     
research so thank you for paying attention and I hope you learn something
