     
Transcript     
0:00     
hello     
0:07     
Jesse uh yeah we did have a diva worm last week it's recorded it was a pretty     
0:13     
nice set of updates on gon too so let me go back to that     
0:20     
hello hi Hi how are you oh I'm still getting data     
0:28     
Okay so it sort of works this is for your um     
0:34     
incr stuff yes I've got my three models and     
0:42     
um actually Four models because they made the post a different size and they     
0:48     
seem to behave differently with that so yeah but I have to get each data     
0:55     
point separately so it's being um sort of longer process and     
1:04     
yeah well it's good sounds good well it's all I can do and     
1:12     
hopefully I'll get it all done in couple of weeks before the compell 3D course oh     
1:20     
yeah yeah that's coming up yeah so I I'm supposed to check that out     
1:28     
to so I know how to work with it yeah     
1:34     
yeah anyway I'm     
1:39     
just model crunching or something yeah model crunching that's a good word right yeah thanks for the update s     
1:48     
mahul how are you uh hi Bradley hi everyone I'm good     
1:54     
uh thanks for asking so there is a small blackout here that's why I can't turn on my camera     
2:00     
it's completely black okay uh otherwise otherwise I wanted to share something     
2:06     
the small update okay so B last discussion     
2:39     
So based on the distance threshold of five uh we have created the hyper graphs     
2:44     
for every time points and that is how it looks like so because it was taking too     
2:50     
much time to create hyper or all     
3:07     
it is getting and okay so based on the last uh week's discussion we like     
3:13     
thought we'll diverge towards more on hyper Dynamic hyper graph because the graph was way too uh what do you say     
3:20     
mixed up because we had all the time points so to segregate based on time points we decided on making Dynamic     
3:27     
hyper now coming on that so last time we saw two types of hyper graph spatial and     
3:33     
then lineage hyper graph so in spatial also we like created it in two ways so one was by distance base metric we did     
3:40     
thresholding and the other was uh uh clustering using DB scan so that is what     
3:45     
we did so this is the first method by threshold and we have created a bunch of     
3:51     
hyper graphs based on time points so keeping a distance of 10 10 10 uh we     
3:57     
have created like these many hyper graphs I think it's 4 into 5 20 19 hyper     
4:04     
graphs so creating like 190 hyper graphs for every point which is 190 was taking     
4:10     
too much time so for now I created only like individual hyper hyper graphs after every time step so it's also easier to     
4:17     
visualize and we can see that uh the number of cells are also increasing as we go time forward but in the end like     
4:24     
as we reach towards uh time Point 190 it again gets quite crowded so so just a     
4:30     
note here that uh in time Point 81 we are not including those cells which were     
4:36     
present at let's say time point one or time Point 11 it only has cells which has which has time point which are which     
4:43     
which exist and which live at time Point 181 so still it is quite     
4:50     
overcrowded then uh yeah then I created hypergraphs based on clustering and as     
4:55     
we saw uh yesterday uh last week in the previous meet the hypergraphs created by     
5:00     
clustering method were little uh were little less cluttered uh unlike the     
5:07     
previous method so uh based on this method uh I created again the hyper     
5:13     
graph so this hyper graph is created only for a single time point which is tal 1 and we see that here we have four     
5:22     
unique cells so one is EMS then AB ABP and ABA and we observe that uh     
5:30     
for every cell we have like multiple hyper edges which looks     
5:42     
uh understand that a little bit B so what we observed     
5:49     
that at time point one uh we have total number of 24 cells and those cells are     
5:56     
like this so cells are AB ad a a Ab so we see that same four cells have like     
6:01     
multiple instances of it so ab ab is here and so on similarly for ABA ABA     
6:07     
also like exist at multiple instances and when we observe uh these cell     
6:13     
positions so for example at time equal one only we are focusing on so this a     
6:19     
this AB cell has this set of XYZ coordinates and its size and another     
6:25     
cell AB the same cell AB uh but it has a different coordinates XYZ and also a     
6:31     
different size even though their mother will of course will remain same because it's originating from the same parent     
6:38     
but still uh looks like every uh this same type of cell also exists at     
6:43     
different but when we visualize these cells uh in our hyper graph we saw the     
6:49     
overlapping of these so this is because we are not actually drawing the coordinates XYZ if like that uh if if we     
6:57     
try to do that then it will be little harder to observe it because then we will have to a 3D plot but the goal is     
7:03     
next that only uh anyways let's focus here only     
7:08     
so that is why we saw that there was uh these many hyper edges     
7:16     
overlap so at this point I made a decision like uh just to make it less     
7:21     
fluttered just to have a single hyper Edge for every unique type of cell uh     
7:27     
what I thought was uh we can do uh we can we can just count the unique cells     
7:33     
here so at 9.1 we have these many cells 24 so instead of this we'll just count the unique count of it which is like     
7:41     
four here and but but we would also count the frequency of them so if if a exists like 10 out of 24 times I think     
7:48     
that is not the case but still assuming uh then we can have a bigger set of B     
7:53     
bigger size of node for a compared to like if EMS only exist twice then we can have a smaller EMS let you know if that     
8:00     
can be useful uh just to just to see that AB is existing more number of times     
8:06     
in that in that organism the Elance organism at that particular time     
8:12     
Point uh yeah so I think that is what I did so     
8:20     
you can see at time oh and one more uh point I wanted to mention so now since     
8:25     
we have uh this a existing multiple times but we only want to take like single instance of ab uh so uh these AB     
8:35     
values which have like XYZ coordinates and size so I I I I I averaged these     
8:41     
coordinates across all different ABS at that particular time point which is t equal 1 in our case for now so now when     
8:48     
will I when I when we'll plot at time equal 1 K so this is how it will look     
8:54     
like so this um yeah so all these graphs so at time uh Point equal one     
8:59     
this AB uh and all other cells also will have XYZ coordinates which are averaged     
9:07     
for different instances of it at that time point and we see now that we have a single hyper Edge for every uh unique     
9:15     
cell right yeah and that is what it and that is what yeah and that is what we     
9:21     
did for time Point 11 21 and 31 and so on and it's less     
9:26     
cluttered uh and possibly will make more sense when we are analyzing but another     
9:31     
thing as I mentioned we can try is we can increase this node size based on how many instances of ab present at that     
9:38     
particular thing maybe it's useful I thought so just for visualizing purpose     
9:43     
yeah um so yeah so I had a bunch of plans after     
9:49     
after this in specifically Dynamic hypergraphs so what we can do is uh we can create like incidence Matrix so     
9:56     
incidence Matrix is how we mathematically represent the hyper is we have other ways also like incidence     
10:02     
dictionary we can do that also uh so it will help us uh view the number of cells     
10:08     
density aspect Etc at at different time points so I think that will be quite useful and uh then second thing is we     
10:15     
can view the degree neighbors and other features other special features for     
10:20     
every nodes or we can say hyper nodes in our case so as we can see this AB right     
10:26     
so this AB is a collection of a lot of ABS at that particular instance so I assume loely it can be called as hyper     
10:33     
nodes also at this point uh please let me know about this     
10:39     
also whether I'm using it in a right way     
10:46     
okay yeah third thing is uh we can build a sub hyper edges so uh what right now     
10:53     
we have is we still have like at at higher time points we have still cluttered too many uh uh cells at     
11:00     
at every time points so instead of like let's say if you want to focus on a particular cell or particular     
11:05     
connections uh which are origin either from spatial or lineage uh we can we can     
11:11     
build sub hyper edges or sub graphs I have saided wrongly here Sub sub hyper     
11:20     
graphs hyper graphs we can do this by restricting the nodes and the edges and     
11:26     
I was seeing the hyperne library and we can do that so that is how I got the idea from and few other things then we     
11:33     
can do is we can measure cality measures photographs and every time point and begin see the evolution of centrality     
11:39     
meur that will be interesting to see I I hope I think and then so this was something which I got idea from     
11:45     
somewhere else Advanced tutorial of Library so what it was saying is uh we     
11:51     
can build the weighted hyper edges so here we have hyper edges right across     
11:58     
all for all the cells and maybe for across multiple cells at a time so but we don't have any Associated weights for     
12:05     
it not telling like how big or high strong hyperedges but we can in our case     
12:11     
also we can create weighted hyperedges and we can create that uh by the count     
12:16     
of how many times this hyper Edge has appeared uh at different instances so for example let's say a particular hyper     
12:23     
Ed has been existing for time Point 11 12 13 and let's say till 21 so its     
12:29     
weight can be of value of let's say 10 because it has appeared a lot many times compared to a single hyper Edge which     
12:35     
only existing existed for one or two time steps and then it got like divided or segregated into further sub points or     
12:43     
maybe the cells got like separated uh based on their movements so we can have like a weighted     
12:51     
hyperedges if it is required we can we can model it that way and then I was     
12:57     
seeing the concept of dual hypergraphs I'm I should have posted a picture but     
13:03     
yeah so I wanted to ask whether dual hypergraphs can be useful in our case and I wrote it useful for lineage     
13:10     
hypergraphs specifically so to show dual what dual hypergraphs is I will yeah I     
13:17     
just open du I     
13:32     
yeah so this should be a good in visualization to understand what a dual     
13:38     
hyper graph is is it a dual hyper graph yes this this is a dual hyper graph so     
13:44     
what we are doing this so initially we have this uh normal graph where we have     
13:49     
V2 V3 V1 V2 V3 edges and we have vertices and we have then E1 E2 and I     
13:56     
think E6 E5 edges so we just invert edges and vertices so now uh we have uh     
14:04     
edges more uh edges as vertices and vertices as hyper edges specifically so     
14:12     
for instance uh this V1 is connecting to three Hypes right I think it's E1 E3 and     
14:19     
E5 so now this vertex V1 which is now a hyper Edge will encapsulate three uh     
14:26     
vertices the three new vertices which we call as E1 E3 and E5 so these     
14:32     
three um I don't remember why I thought it's useful for Lage hypergraph but I'll     
14:37     
get back back to it uh but but if you get any idea like how we can model how     
14:43     
dual hypergraphs can be useful to model lineage hyph in our situation then we can definitely try that right     
14:51     
and I have uh one doubt before we uh go next     
14:58     
so we increase size of Bas yeah I asked this before only so should we increase     
15:03     
the size of the node based on the number of that kind of sell yeah that point like if we have like multiple instance     
15:09     
of this cell should we increase the size of this yeah I have ask this um so for now that yeah for for now     
15:18     
that was it I have a bunch of plans and I was exploring different libraries so want you     
15:25     
do well that's a pretty good uh update thank you it looks like a lot of good work I like the uh the hyper graphs by     
15:34     
time point so you generated all these graphs and they're sort of not really     
15:40     
dense at first but then they become denser and I assume we can explore each     
15:45     
one if we find one that's because it looks like you know you start with like this these cells on the edge and then     
15:53     
you start to get this thicker Edge and I'm just describing the sort of the topology with some cell in the middle of     
16:00     
that sphere and then you're so this is really informative then it stting to     
16:06     
densify in the middle and on the edges and then finally it just becomes really dense and I'm you know I don't know what     
16:13     
the structure is if we zoom in on each no hyper node and look at how those you     
16:19     
know if we look at like kind of local structure but I think this is really interesting because it does bring up     
16:24     
that kind of you know and we know from like an embryo that you know we start     
16:29     
out with maybe two or three cells and then we start to get these sort of you know Meta Meta patterns in the uh     
16:39     
distribution of cells so this is really interesting uh yeah so you know we might     
16:46     
mine these like look at these uh look at the sort of the positions of the hyper     
16:51     
nodes and there are all sorts of analyses you can do and I'm not sure I     
16:56     
know that you know the standard Network measures we could probably use those I'm not sure what uh kinds of measures are     
17:03     
state-of-the-art for hyper graphs but there might be some graph measures we can derive and that that would have that     
17:10     
would involve going to the literature and finding those but that's that once you have the graphs you know uh the     
17:17     
description in terms of statistics then we can actually or in terms of the the     
17:23     
position and the uh connectivity then we can do the statistical analysis on it so     
17:29     
that's that's good though that we have those then if you go down to your     
17:35     
questions can you scroll     
17:46     
down is it there no it's frozen I think okay okay I'm already at the all     
17:54     
right there we go proba yeah there we go all right so then the plan and yeah we     
18:00     
could use an incidence Matrix uh view degree neighbors that's that's all like statistic graph     
18:08     
statistics then build sub hyper graphs yeah I think sub hyper graphs is a good strategy you know we can restrict the     
18:15     
nodes and edges in different ways and you know get like maybe clean up the you     
18:20     
know we might use like a more restrictive Criterion and see if it cleans up the topology a little bit I     
18:27     
mean what I mean by that is just so that you can see things uh the strongest relationships     
18:33     
say and you know they're you just working that method out to see what that     
18:38     
looks like um and then you know uh let's     
18:44     
see so we have then we have the Dual hypergraphs which I I like this idea I think it would be useful in     
18:52     
lineage graphs um you know like okay so in the winy Edge graphs are nodes are uh     
19:01     
cells our hyper nodes are uh sort of cell categories or cell classes whatever     
19:08     
you want to call them the edges are sort of the Next     
19:13     
Generation or you know if we have a a round of cell division those cells May differentiate     
19:22     
so they leave that category or they um you know the category doubles so the the     
19:29     
number of cells doubles so that cell becomes two cells so there's a conect connection from one to two and all that     
19:37     
so yeah that's so those are our edges and our hypernodes then if we invert     
19:42     
those if I'm understanding this correctly you know that might be interesting we might be able to actually     
19:49     
have I guess nodes that would describe those division events or those uh     
19:55     
differentiation events and then you know the edges would be the I guess the uh cell types or     
20:04     
categories I'm probably getting it wrong but the point is is that I think it'd be useful to describe some of those     
20:11     
relationships in different ways so you know we know kind of the structure from     
20:16     
the wi AG tree it's pretty straightforward what that is but if we invert those relationships we can     
20:22     
actually find out you know some some especially since you're sampling the     
20:27     
graph at time points that aren't naturally uh division events which you     
20:34     
know is is probably good because we're not really interested in maybe the     
20:40     
division events per se we're interested in the relationship of or the state of the embryo at that time window so     
20:47     
whatever is there at that time window that's what we want to know and you know     
20:52     
the the doubling events will be captured in that so that's that's good um yeah so I     
20:59     
think there's there's some application I have to think about it a little     
21:04     
bit uh the last one the question should we increase the size of the node based on the number of that kind of sell in     
21:10     
the same time so yeah that might be useful to have like a a map where you     
21:16     
show like the relative contribution so like you know this the data that's collected on these are you know they'll     
21:24     
sample the the Cell at different times and theoretically the longer the cell's     
21:29     
around the more samples you'll have so like if a cell lives for 20 seconds or     
21:36     
20 minutes let's say let 20 minutes uh versus 10 minutes there     
21:43     
should be more instances of that cell in the data set not really sure how that works out um their sampling scheme but     
21:51     
basically I think it it should be over represented so you know it be nice to     
21:57     
know of that information about what cells are over because we tend to average out over all the cell uh     
22:05     
observations and that's good for kind of a you know mean description but there's     
22:11     
a lot of information in there maybe we're not picking up so like if I take the mean of something that's been     
22:17     
everywhere in the embryo I mean the mean Position will be in the middle of the embryo that's not very informative so we     
22:24     
need to have something that's a little bit less um smooth over you know we need     
22:30     
to have some information about the variation that you might see as     
22:35     
well but not noise     
22:41     
yeah okay yeah that so that's very good that's very good and I think that you know I think you're going in the right     
22:47     
direction here um I think that you know there there are things you can play with     
22:53     
but I think once you have the basic scheme down then we can do these things like the graph statistics s and and     
22:59     
playing around with the the layout and the with a lot of network science you     
23:04     
know the layout is kind of a thing onto itself so you know there are a lot of     
23:10     
different types of layouts that people play with you know there there's like the hair ball which is what kind of what     
23:15     
you're showing and then there's like you know Force directed graphs there ordered     
23:20     
graphs all these other things that we could do but we have to get the basic relationships right first which you're     
23:27     
seemingly getting done so okay okay uh yes thank you very much     
23:34     
thank you yeah uh okay uh so for like uh this week's uh     
23:40     
plan I'm thinking of like as we discuss uh this plan like I'll try to do one two three four uh and probably five as I'll     
23:48     
explore dual graphs a little later uh but like most important thing which I'm thinking of right now is exploring     
23:54     
lineage hyper and which will then lead to dual hypergraphs also if you think it's use um yeah so I hope this sounds good     
24:02     
for the week for the for the upcoming week and like yeah right now I feel like     
24:07     
we are diverging too much I mean that's not bad I mean like we are exploring the of hypergraphs and which hypergraph     
24:14     
modeling would be pretty will be useful for our data case but like after like two couple of weeks I would like to like     
24:20     
go into specific two three kinds of hyper graph so we can model both the spatial and Lage hyper graph uh as good     
24:27     
as we can so yeah we can conclude well in the     
24:33     
end so so from from your Explorations do you think the clustering or DB scan     
24:40     
method is better yeah yeah so okay right I will I     
24:46     
have written it down now in the previous notebook which we saw this was a DB scan     
24:52     
method so a few methods oh yeah a few features I have written about DB scanners     
24:58     
so one feature is that we don't have to specify the number of clusters so what happens in K means is uh we have to     
25:06     
specify beforehand without knowing even the topology of how cells are oriented spatially we have to tell that we want     
25:12     
this many number of clusters and this is this actually adds up to one of one of hyper parameters which we have to decide     
25:19     
which we have to come up with so DB scan avoids the need of this so the first point is that and the second is it is     
25:27     
quite robust to outline so what we can observe is uh like at time point one we     
25:33     
saw all the Four Points were quite D quite Divergent at a different different places so uh so possibly all could look     
25:40     
like outliers in the beginning at at at beginning time points a lot of cells so DB scan is quite robust to outliers so     
25:48     
what it does is it basically does not assign those points to any cluster so we saw that going to the same notebook uh     
25:57     
before     
26:07     
um yeah I'll I'll wait for like few seconds so this yeah so the screen is     
26:13     
static okay yeah in the so in yeah I should I hope it should     
26:20     
be there now yeah so at time Point yeah so at time point one uh we had these for     
26:26     
our self right ab ab ab okay uh so we saw the now we see the cluster indices     
26:31     
of it so I did not explain this part so we see that it is minus one minus one minus one for all the cells so what it     
26:38     
means is uh that these cells are not being assigned any particular uh uh uh     
26:45     
hyper Edge or cluster because we are doing clustering here because all the cells are quite far away right now from     
26:52     
each other uh specially so DB scan allows us to do that it allows us to     
26:58     
deal with outliers in the way that it won't assign them to any clusters on the other hand K means will will force or     
27:05     
even other method will force uh the cells every cell even though it's very     
27:10     
farther from like from the closes Hy it will force them to be a part of it     
27:18     
so here DB scan is useful and then coming to the third point it is     
27:24     
capability to find arbitrary shaped ofes yeah this is so this is very important uh so this is specifically the K means     
27:30     
drawback so K means only can assign sperical uh structures uh uh when it's     
27:37     
creating the cluster it doesn't deal with like for example if we say ring like cluster and in in between there is     
27:43     
some different cluster so DB scan but but DB scan does allow that I can show a     
27:49     
picture but is it clear like the third Point yeah yeah     
28:00     
yeah so that were the three reasons uh DB scan could be more useful specifically on our case because of how     
28:07     
our data set is and we have a few hyper parameters associated with it which is     
28:12     
Epsilon and the Min samples so Epsilon says uh so how many Epsilon is basically     
28:19     
the maximum distance between two samples for them to be considered as part of the same neighborhood or same CL so this I     
28:24     
have assigned it as five only like we have done in the previous threshold five value if we do 5050 then it is too much     
28:31     
and Min samples so like to to consider a hyper Edge to consider a cluster as a     
28:36     
hyper Edge or to so this is the minimum number of samples so I have put it as simply one so a single uh a single node     
28:44     
or single hyper node can also be treated as a cluster on     
28:49     
it yeah and then you have yeah s yeah it's     
28:57     
basically like almost seems like K means is forcing us to uh explore the spherical cow I don't     
29:04     
know if you know that expression but um and then DB scan allows us to sort of     
29:11     
have more sort of shapely clusters categories but it     
29:18     
doesn't assign a cell to any one category if it doesn't like that there's a category to assign it to so it's both     
29:25     
good and bad I mean I guess we could just treat those cells as individual points and then you know start to get     
29:33     
hyper hyper nodes but um so yeah this this looks good um this     
29:41     
is yeah so this looks like DB span scan might be good I I would just say     
29:46     
probably it's a trade-off though I mean we could kind of go with DB scan but then also you know kind of have c means     
29:53     
in the background if there are things we can't get to with DB scan we might try K means as an alternate way of finding clusters     
30:01     
and you know because you you don't always want to try the same method uh     
30:06     
for everything um because this is a kind of a tricky data set you have like the     
30:12     
spatial heterogeneity this temporal heterogeneity it's temporal growth and     
30:18     
then you know you're trying to sort of describe all these different I guess     
30:24     
sublineages of cell with the same model so it's really hard to do to with one     
30:31     
method you know um yeah but this looks very good thank you for this update I     
30:38     
think this is very informative so yeah yeah so I mean the     
30:43     
next week you'll probably kind of work on the maybe some of the graph statistics or kind of polishing up some     
30:50     
of your exploration or stuff like that yeah yeah uh but before that uh uh     
30:58     
you mentioned a point like I have doubt on so what you mentioned was like we     
31:04     
have this uh multiple instances of ab when we were constructing right at time     
31:10     
point when we have number of sucess 24 and we have multiple instan of ab and what I did was I concatenated not     
31:16     
concatenated I averaged the coordinates XYZ so you said that even if we have like cells AB uh at four different     
31:23     
corners but if you like do averaging then it won't make sense because it will fall in the center     
31:28     
so uh right now I was following that and uh what we got is a graph like this but     
31:35     
let's say if I don't follow that I mean it's wrong to follow that as you correctly mentioned so how should we     
31:41     
create spatial graph in that way because if we don't average them then we would have to concatenate them like in a     
31:47     
fashion that we have four set of different nodes of the same type but I but we also don't want to clear uh make     
31:55     
like this set of graph like this where we have like uh multiple hyperedges     
32:01     
across all ac across a single cell I mean it is okay but it's just for     
32:08     
visualization I try to get a different me yeah so should we     
32:14     
proceed yeah yeah oh so yeah there different ways you can do an average too     
32:19     
so you could do a weighted average which means that each one has some contribution to the overall mean uh or     
32:26     
you know you could have like I don't know it's an interesting question how to do that um not really     
32:35     
sure I mean you could have like a centroid which would be like this meme which is basically a centroid of all     
32:41     
those cells um and then you know you could weight them by sort of their variance so     
32:48     
like if your average turns out that you have a lot of you know variation like if     
32:55     
you have cells at like four points on the edge of a sphere and then you end up     
33:01     
getting like a value in the center of that sphere and that sphere is very large you know you might waight that     
33:07     
wait what because they're not going to be evenly distributed you can weight them accordingly or you can you know maybe do     
33:14     
some sort of thing where you create a hyper node of all the cells of that type     
33:20     
like all the ab cells are in a hyper node the hyper node has like a a     
33:26     
centroid and has variation and you know you might actually just treat the cells themselves     
33:33     
as hypernodes with other instances inside and then they have uh you know edges     
33:40     
with other hyper nodes which are other cell types and you could do that too I mean there are a lot of ways you can     
33:46     
treat the individual cells uh I would just you know the base model would be     
33:52     
the averaging the simple averaging but if you can think of a way to do like a weighted average     
33:58     
or some sort of centroid model or even like the H where you just have a hyper graph of all the     
34:05     
instances that might be useful too because I'm not you know I'm not     
34:10     
really sure how the sampling uh you know when we collect the     
34:15     
data you know we sample the the embryo based on just these kind of observations     
34:20     
so there's no evidence that one observation is better than any other but     
34:25     
um it's also the fact that you know they're all individual they're all independent     
34:31     
observations so they all may have some value in telling us something about the     
34:38     
about the structure of the embryo so I know that's kind of a hard answer     
34:44     
to your question but it's uh you know that it's just that's the way we that's     
34:50     
what we have for data set yeah yeah but that is an interesting     
34:56     
feature of data set I think so I think hyper yeah hyper node I think it's a     
35:02     
good idea so somehow we can put all the information by by by telling the variance that we have a lot of variance     
35:09     
and different in their positions but but still we have this hyper with this averaged sort of like weighted average     
35:17     
hyper uh hyper node I'm sorry yep yeah that's good well thank     
35:24     
you for that update yeah thanks all right so well I actually mentioned     
35:30     
the spherical cow before and I don't know you know people are familiar with     
35:36     
that that metaphor but uh so there's an old joke about a physicist who um is     
35:44     
commissioned to study like milk production and cows and then you know     
35:49     
they say okay how do you start solving this problem and then they say assum aspherical cow and of course the the     
35:57     
joke is is that in physics you do things as spherical models you you take things and you abstract them away to the point     
36:04     
where they're not really like uh they don't have any practical value but you can actually find theoretical value in     
36:11     
those models and I think there's a variation of that joke with an economist     
36:16     
because economics does the same thing where they have like a they abstract the     
36:21     
real system so much that they have this model that's you know mathematically tractable but not um     
36:28     
doesn't have a lot of connection with the real world and so that's always a joke that people use to sort of make the     
36:35     
point that our theoretical models are a lot of times divorced from The Real     
36:40     
World models but the point the other point the Counterpoint is that there's     
36:46     
this theoretical value in these kind of models that you don't get with just a     
36:51     
like a simple data analysis of the real world data and this is why because our     
36:56     
real world data is often times so messy there's so many variables variation uh present that we     
37:04     
can't really do anything useful with just this straightforward analysis so     
37:10     
now I'd like to go over uh something I mentioned in the meeting and that is to talk about the average position or the     
37:18     
centroid of a cell it's been observed multiple times so let's take AB as an example so     
37:25     
we have AB and we can observe this over a number of time points we might observe this from     
37:33     
T1 to T2 all the way to say 20 time points T20 we can get independent observations     
37:40     
that each time Point AB is either present or absent in the time point that we're looking at so usually when we get     
37:47     
data we get microscopy data with sample that specific time points and then that     
37:53     
image had either has that cell or does not have that cell so our image of course is going to     
38:01     
capture that cell in any one time and of course the cell can move around it can     
38:06     
be in different locations in the embryo you know it's it's maybe an imperfect     
38:12     
method for getting this information in any case this cell persists until we get     
38:19     
a division event in after which ABA and ABP exist and     
38:27     
then exist for a time we have the same problem with ABA and ABP so we have this temporal     
38:33     
variation we also but what's more important are you know when we have like     
38:39     
we're looking at crossed organisms AC crossed embryos and in those cases we can have     
38:44     
instances of observation so we see AB from in one embryo and ab in another     
38:50     
embryo and an AB and another embryo they all have their own threedimensional position and those are independent     
38:56     
observations and those observations will persist until again there's a division event and     
39:02     
then we can look at ABA and ABP in general we think of these as     
39:08     
independent observations so this is the symbol for an independent observation and ab nominally is independent of ab so     
39:16     
any one instance of ab is nominally independent of any other instance of ab     
39:21     
we could actually you know track the cell over time but because it's you know     
39:28     
it's hard to really capture that time Evolution we consider it to be uh     
39:33     
independent now you could have a situation where in the time series where     
39:39     
they're dependent so ab1 is autocorrelated with ab2 so as you     
39:47     
know the position changes in time that represents true migration on the other hand it's hard to do because we have     
39:54     
different frames of of observation so this is something that we have to keep in mind and then we can also have a     
40:02     
summary a statistical summary of an average or a mean so the mean of ab is     
40:07     
the mean of all these instances and so we can have these independent observations we can have     
40:14     
these temporally dependent observations or we can have these mean     
40:20     
observations and so when we have a mean observation we have something like this where we have a point Cloud for ab and     
40:27     
that point Cloud consists of a bunch of cells and those cells can sit in     
40:33     
different positions sometimes there's quite a lot of variation around them and     
40:38     
they sit in this point Cloud you could Define this point Cloud roughly like this and then we end up with a centroid     
40:44     
position so this is a centroid for ab and the mean description is the     
40:50     
centroid position but as you notice we're filtering out a lot of variation     
40:57     
now when we move to the division case AB or that centroid will divide into ABA     
41:02     
and ABP ABA and ABP again can be represented as these Point clouds and again with a     
41:09     
lot of variation around the centroid so our model of sort of cell division is we     
41:16     
take the centroid for ab we take the centroid for ABA and the Cent forp and     
41:22     
we you know it's a nice neat sort of uh set of lines that diverge but in reality     
41:28     
you have a lot of overlapping volumes with overlapping cell positions over     
41:33     
across all these independent instances or sometimes temporal instances and so this this makes it a     
41:41     
little harder for us to understand what's going on the the centroid model makes it easier to sort of represent it     
41:48     
but there's a lot of variation underlying that that isn't described so this brings us to the thing     
41:55     
I was talking about the meeting having a weighted average so your weighted average is basically where you have this     
42:01     
AB Point Cloud you have these independent observations of cells and you have the centroid the centroid value     
42:09     
is the mean of these cells but we don't really want to just take the mean and leave it at that we might want to take a     
42:15     
weighted average and the weighted average would be where we take each cell     
42:21     
and it has a certain weight relative to say a centroid so like in the C's example of uh what Mah was talking about     
42:29     
uh you might have like you might Define a centroid position and then you might say okay how far away is each cell from     
42:37     
this centroid position so you drive a centroid position a priori you then fit     
42:42     
the cells to that model with the centroid and then you derive a new centroid centroid Prime which is um you     
42:51     
know the value of this so it would be like AB me Prime which would then be the     
42:58     
new Cent the weighted average and you know that gives you a little bit more     
43:03     
refined information about this point Cloud so this is a point cloud of points     
43:09     
for an individual cell and in the embryo we have this point cloud of points for all     
43:14     
cells and so thinking about the hypergraph case we have these hyper     
43:20     
nodes contain nodes these nodes are individual c IDs     
43:26     
they can be averaged now we can also do a hyper node or individ so this is for the     
43:33     
embryo or for cell     
43:40     
types and we can also do this for a single cell type so this is the hypernode for     
43:46     
ab and you have a bunch of ABS within this hyper node and then they connect     
43:54     
other hyper nodes so you might have say for example     
44:02     
ABA and     
44:07     
ABP and that represents all the hypernotes here so we can     
44:13     
actually look at how these are connected like     
44:20     
this we get um you know information about how all     
44:27     
these individual cells kind of divide so we'll have     
44:33     
this mapping from one hyper node to     
44:38     
another it's basically this trivariant graph where you have the hyper node for ab mapping to     
44:48     
its daughter hyper nodes and you have these correspondences that tell you something     
44:54     
about the division events and they're all going to be different so we would take the information for each um     
45:00     
instance of observation and build a division event build a branch of a tree     
45:06     
and then we can do what we want with that we could average those we could describe those in terms of modes so     
45:14     
sometimes you know you might have uh clusters within a hyper node or clusters     
45:19     
within a hyper node so you know these cells over on this part of the hyper node ABA M to these parts of the     
45:27     
uh cells in the hypernode ab and so we can say that there's like some spatial     
45:33     
uh you know some of the observations or some spatial bias so it gives us a     
45:38     
better account of this variation uh I I just wanted to mention that um thank you     
45:44     
mahul and Pocky for your iub     
45:50     
contributions yeah thank you I saw that it's merged now yeah so uh this is uh     
45:56     
the dor graph repository and so we've been active with poll requests here so I     
46:02     
have accepted several poll requests this past week uh mahul uh pushed his data     
46:08     
exploration notebooks and this is uh number 21 number 20 was kmap the kmpp or     
46:16     
implementation by Pocky that was all the stuff that she presented last week um     
46:22     
and so those are two that are have been pushed thank you for your contributions     
46:27     
and hopefully you know we get more as we get on with gck um and you know I I     
46:32     
would encourage people to push you know if you have maybe every week or every other week just to have the habit of     
46:39     
pushing things to the repositories so those contributions are live in Devo     
46:45     
graph um they're part of stage two so if we go to stage two we have the TD and K     
46:52     
mapper and then we also have the hyper Devo graph uh directory which has a lot     
46:58     
of those things in it so that's where those things     
47:04     
live okay that's good so I also wanted to mention um that the gecko conference     
47:12     
is this week where it's starting up uh imminently and um gecko is a a     
47:19     
conference on genetic algorithms so if you're interested in genetic algorithms there are a couple of     
47:26     
conferences there's uh evostar there's aife and then there is Gecko and gecko     
47:34     
is more sort of the applied genetic algorithms conference where people do things just kind of like tweaking the     
47:40     
algorithms or you know applying them to different problems whereas aif is more     
47:45     
maybe the theoretical side of that and then Evo stars is kind of like uh um a     
47:51     
bunch of stuff that people are doing it's you know so gecko is     
47:57     
this is gecko um it's being held in Melbourne but it's a hybrid conference this year the full title is genetic and     
48:05     
evolutionary computation conference and their uh logo not surprisingly is a gecko with double helix over laying the     
48:13     
gecko and that's not to scale of course so uh but that's the uh that's why they     
48:18     
call it gecko um and so yeah they're they're holding in Melbourne there's you     
48:26     
know it's a pretty big conference you have a lot of talks it's it's you know it's one of these conferences where it's     
48:33     
pretty overwhelming because it's of reasonable size so you have to plan your route through the     
48:39     
conference uh this thing here is this tutorials page so one of the interesting things about a lot of conferences is     
48:45     
they have these tutorials and people will host tutorials on a number of topics usually these are very useful     
48:52     
because they give you some depth in a certain area that you wouldn't be able to say take a class on or it might be     
48:59     
something that you see in papers again and again and you don't really understand the method as it's described     
49:04     
in the paper so they'll have these tutorials where they walk you through a method they give you the software they     
49:11     
run a set of problems and then you know you get some insight into your um into     
49:18     
some method in the field so they have a lot of things here they have uh robust     
49:24     
optimization so genetic algorithm GMS of course one of the fosy of genetic     
49:29     
algorithms optimization and creating optimization schemes for data and you     
49:35     
know this is uh of course an longstanding interest in the field uh so there's robust     
49:42     
optimization uh there's this gentle introduction to theory for non- theoreticians uh this is of course just     
49:49     
talking let's see this is about uh sort of     
49:54     
theory uh this doesn't this discuss mathematical methods of particular results but it talks about what the     
50:01     
theory of evolutionary algorithms aims at so there's a theory of evolutionary algorithms uh not just the     
50:08     
implementation there's this there's theoretical research in evolutionary computation and that's separate from     
50:14     
sort of benchmarking and those sorts of things um then they just kind of tell you how to deal with Theory and how to     
50:22     
formulate Theory and what are the trends in the community so this is an interesting looks like an interesting     
50:28     
Workshop or tutorial they have basy and optimization and this is of course in the context of     
50:35     
genetic algorithms so one of the strengths of evolutionary algorithms is that they can be applied to blackbox     
50:41     
optimization problems and so this is where you know we can use these methods     
50:47     
uh for a lot of things other than just kind of modeling Evolution or you know     
50:53     
looking at that problem in of itself so uh basian optimization is actually     
51:00     
something that we can model with genetic algorithms it's very amendable to it and that's what they're talking about here     
51:06     
basy and optimization has a lot of applications to deep learning and design optimization or even stochastic     
51:13     
optimization something we use in operational research and so this is where you have to build features into     
51:20     
your genetic algorithm some sort of gausian process and have a memory for     
51:27     
previous evaluations and you put those together into a basian model uh so this is an     
51:33     
interesting thing because you know we often talk about basian methods but sometimes they kind of are separated     
51:40     
away from they're abstracted away from the problems that they're trying to solve uh you know I don't have time to     
51:47     
like give concrete examples but um just I said to say that people throw it     
51:53     
around bisy and around a lot and the insights are not as obvious as the sort     
52:01     
of the the promise of the method so this is nice that people are kind of working out the details of how to implement this     
52:08     
in code um there is coevolutionary computation     
52:14     
for adversarial deep learning so there are some applications of deep learning there's the evolution of neural     
52:21     
networks which is interesting from the standpoint of what we're talking about with um neural developmental programs so     
52:28     
this is uh another kind of approach to evolving artificial neural networks so     
52:35     
you know you can take a uh there's this Paradigm called neuroevolution where people build neural     
52:41     
networks and they evolve them to sort of be specialized for certain     
52:47     
tasks uh it's different from like training a model in a number of     
52:52     
interesting ways and so as they point out here uh deep     
52:58     
learning uh deep learning performance depends crucially on the network design I.E its architecture hyperparameters and     
53:05     
other elements so while many such designs are too complex to be optimized     
53:10     
by hand neuro Evolution can be used to do so automatically and so this is what they call evolutionary Auto Machine     
53:18     
learning and this is where you can get good deep learning performance even with     
53:23     
limited resources or state-of-the-art performance so this is one of the goals of course of trying to build deep     
53:30     
learning networks is to build them in a way that doesn't require huge computational     
53:37     
resources um and so yeah there's a lot there's a lot of focus on uh stochastic     
53:43     
problems and stochastic Solutions and things like that evolutionary computation is amenable to that because     
53:50     
of course Evolution works on the principle of stochastic uh generation of variation     
53:55     
and selection so we can actually select from a stochastic generator and we can get you     
54:02     
know we know that like theoretically they should work so it's very amenable to those types of uh problem     
54:10     
spaces uh and again optimization is a big deal um there's also evolutionary     
54:16     
machine learning for interpretable and explainable AI which is a hot topic now     
54:21     
but you know linking evolutionary algorithms into that deep learning space machine learning     
54:28     
space uh evolutionary evolutionary reinforcement learning which is a an     
54:34     
interesting area because people have been you know kind of getting into reinforcement learning after kind of a     
54:40     
law in the field and so there are a lot of ways people kind of interesting ways people have approached reinforcement     
54:46     
learning and so in this case they're looking at evolutionary approaches so you know reinforcement     
54:54     
learning we've been able to do things with uh you know training models of video     
54:59     
games like Atari Starcraft chess and go which are board     
55:05     
games but we still can do that uh we can train reinforcement learning models to     
55:11     
play those games quite well um and then you know we can actually look at other types of problems     
55:17     
like robotic control uh highspeed drone control and controlling something like a     
55:23     
plasma reactor so very complex system Collective behaviors that exhibit     
55:29     
Collective behaviors or complex Dynamics um and you know people are even     
55:35     
applying reinforcement learning to things like chat Bots and so that's     
55:40     
interesting and then uh so interestingly elements of evolutionary computation     
55:46     
such as populationbased training which is a technique from jaderberg 20177 and quality diversity algorithms     
55:55     
from Pew 2016 represent key elements in some of these breakthroughs the combination of     
56:01     
evolutionary computation and reinforcement learning methods is not new and has gained more popularity     
56:07     
popularity and interest recently as researchers discovered the limitations of the individual     
56:13     
approaches so this is you know there are a bunch of like outstanding questions here uh there's curriculum learning and     
56:21     
environment generation there's neuroevolution meta learning with     
56:26     
Evolution quality diversity which is a a sort of technique that um I'm not going     
56:33     
to get into right now but it's a it's a popular technique in evolutionary computation and then other combinations     
56:39     
of evolutionary computation and reinforcement um and so yeah then there     
56:46     
are some things very specific to genetic programming um modelbased evolutionary     
56:53     
algorithms robot Evolution which is interesting uh representations for evolutionary     
56:59     
algorithms which is important in terms of how you build the representations themselves so evolutionary algorithms     
57:06     
really take a problem and they model it as um sort of a you know something     
57:11     
that's on an artificial chromosome where the problem is encoded in the in the     
57:16     
genes of the artificial chromosome and that genotype then describes the problem     
57:22     
and you basically can evolve the problem Space by mutation     
57:28     
recombination and um you know selection so you have these different things that     
57:33     
are operating on this problem space and the idea is that you be you can maybe     
57:38     
recombine the problem space or mutate the problem space in certain ways or you     
57:44     
know at least the representation of problems in that problem space so that you can reach Optimum and so this is     
57:52     
this is uh kind of what they're getting at here uh where you build these representations     
57:58     
more or less effectively if they're more effective you know it helps you with your Solutions if it if it's less     
58:05     
effective then it's harder to run the algorithm get a insightful out     
58:10     
outcome so successful and efficient use of evolutionary algorithms depends on     
58:15     
the choice of genotype the problem representation or the mapping from the genotype to phenotype which is uh uh you     
58:23     
know basically the representation in     
58:28     
binary binary values that make up some sort of gene or make up some sort of chromosome and then modeling that as as     
58:36     
an outcome uh in problem space and on the choice of search operators there     
58:42     
applied to genotypes and so they just kind of get into this architecture and how to design things and how to     
58:48     
implement them uh so yeah then there some other interesting uh there's of course the     
58:55     
obligatory language models uh application as well so that's you know that's the last tutorial uh so yeah a     
59:02     
lot of interesting tutorials uh this Workshop actually is really interesting this is the     
59:09     
graph-based genetic programming Workshop so this is about graph neural or well     
59:14     
it's not graph neural networks it's graph-based genetic programming so it's graph GP and we talked about of course     
59:21     
graph neural networks we're talking about hyper networks and graph neural networks this is something that um     
59:28     
really kind of gets at this idea of representing your problem and having this repres good representation but     
59:34     
taking it to another level and applying it to graphs so the evolution of graphs     
59:39     
basically and so you know they they talk about this well the classical way to represent programs in genetic     
59:46     
programming is by using an expression tree different uh genetic programming     
59:51     
variants with graph-based representations have been proposed grass     
59:56     
graph-based representations have led to novel applications of GP in circuit design cryptography image analysis and     
1:00:05     
more this Workshop aims to encourage this form of genetic programming by     
1:00:10     
considering graph-based methods from a unified perspective and to Bringing together researchers in this subfield of     
1:00:18     
research and so this is uh this is just kind of a call for papers but they do     
1:00:23     
have a program now which kind of talks about some of the things that people are going to be presenting on so there's     
1:00:29     
this idea of using evolutionary model merge this is a general method that uses     
1:00:35     
evolutionary techniques to efficiently discover the best ways to combine different models with diverse     
1:00:42     
capabilities using ideas similar to graph-based neuro architecture research so this is sort of The evolutionary     
1:00:48     
version of this and this is based on this work by Sana AI which of course is     
1:00:54     
a group uh run by David ha I think or at least he's involved in it and this is     
1:01:00     
where they're building these evolutionary models that are relevant to machine learning so Sak AI is being     
1:01:07     
represented here evolutionary robustness by WB langon he actually presented at um     
1:01:14     
the EI Workshop the uh embodied intelligence Workshop this last year and     
1:01:19     
I think this is similar to the work he presented there uh this is on of course genetic programming and other types of     
1:01:27     
evolutionary algorithms so there are different types of evolutionary algorithms genetic programming is a     
1:01:33     
subset of that and you know talks about how those methods have demonstrated     
1:01:38     
creativity which is something we talk about with respect to um deep learning     
1:01:44     
machine learning but is something that's very closely tied to genetic programming and that technique of um you know     
1:01:51     
modeling a system and being able to mutate it and recombine it in different ways     
1:01:57     
and so this kind of talks about uh kind of how you know there there these     
1:02:04     
results um you know the the GP is is quite popular uh and then this talks     
1:02:11     
also links to these long-term Evolution experiments of Rich Linsky and sort of     
1:02:17     
how Evolution can generate a lot of creativity a lot of variation in running     
1:02:23     
for a long time so this is kind of talking about how you know in genetic     
1:02:28     
programming populations tend to converge but in real Evolution populations don't     
1:02:35     
tend to converge and so there's this gap between sort of our Real World Systems     
1:02:40     
of evolution and the uh insulco aspects of evolution so it talks about mutation     
1:02:47     
testing which is a technique uh for kind of looking at software being robust at     
1:02:54     
different changes so if you mutate software can you continue to run that software or does it Crash and Burn and     
1:03:01     
so this is something that genetic programming or evolutionary algorithms     
1:03:06     
can provide insight into so this is an interesting area of research and then of     
1:03:12     
course this uh Mangrove architecture which is where you know if you're     
1:03:17     
looking at sustained Innovation or innovation over time or keeping     
1:03:22     
Innovation going uh in a system you can use these Mangrove architectures     
1:03:28     
which are composed of many small trees which are intimate with their environment so this means that you have     
1:03:35     
these local trees that sort of characterize local variation in the environment and then they're put     
1:03:41     
together like a mangrove tree which is kind of like joined together using a common n system so this     
1:03:48     
is an interesting kind of innovation here this is citing um some work by the     
1:03:55     
author     
1:04:00     
and yeah we won't get into that looks like that's okay so yeah then there are     
1:04:07     
some other or oral presentations on different methods directed as with program graphs uh search trajectory     
1:04:15     
networks and some other things as well so a lot of interesting methods there and techniques and of course     
1:04:22     
evolutionary algorithms are one approach to uh dealing with that complexity that we     
1:04:28     
see in our data sets and um you know it's it' be interesting to see how that     
1:04:34     
relates to some of the work we've been doing with graph Neal networks and some of these other things with uh     
1:04:40     
developmental trees and other things I think some of the techniques     
1:04:46     
are are being applied to the tties okay recognizing some of the     
1:04:53     
similar language yeah yeah gentic     
1:04:58     
algorithms yeah so they they're applying it to     
1:05:03     
10es yeah I'm not sure how successfully but um and I'm not into that field well     
1:05:13     
enough I don't yeah if I attended the conference     
1:05:20     
I probably wouldn't know what was going on yeah it's very specialized um but     
1:05:25     
yeah and you know the techniques are they can be you know slow to kind of     
1:05:32     
pick up like with a machine learning algorithm you can train the model and sometimes you just have to overtrain it     
1:05:38     
and really have a lot of data specified but with genetic algorithms you really     
1:05:43     
don't necessarily need to train it but you need to represent the problem well and then you need to be able to run it     
1:05:49     
for a long time to get a solution and you know so like I could take like a Tegrity structure     
1:05:56     
and map that problems based to like an artificial chromosome I could mutate it     
1:06:02     
to see you know if I change things does it still work you know is it and then there's a selection function form it's     
1:06:09     
called form Finding form Finding the best optimal form yeah yeah that sounds     
1:06:16     
like something that they've yeah they've done a lot of things with like it's uh it makes the whole whole thing more     
1:06:25     
involved in difficult yeah form Finding Integrity structures based on Force     
1:06:31     
density method well that and oh well anyway definitely there was     
1:06:38     
a genetic algorithm yeah     
1:06:47     
technique don't know yeah well I     
1:06:52     
mean I mean I know they've done lot of stuff with robotics and looking at the     
1:06:58     
shape of Robotics and sort of modeling you know how do you evolve different morphologies that are optimal so you     
1:07:06     
know you want to have an uh a morphology that um you know could walk or you can     
1:07:11     
have a morphology that reaches for things or a morphology that does you know like rolls over a rough terrain so     
1:07:18     
you can evolve different solutions for that from your representation you can mutate it recombine it and then say you     
1:07:25     
know I'll put a selection I'll put a selection operator on it and say you know find the the best uh phenotypes for     
1:07:34     
the score and then it generates a bunch of variation you know different solutions and then you can test those     
1:07:40     
you could model those and the real world and test them and so that's I mean but the thing is it takes a lot of effort     
1:07:47     
you know in terms of computation in terms of getting the representation right and then doing the testing in the     
1:07:53     
real world to see if it actually gives you what you want but the idea being that you generate these options     
1:08:00     
that are you know pretty good and then that that's kind of what you want     
1:08:06     
and it has this creative aspect that machine learning doesn't have and that it gives you these option you know it     
1:08:12     
doesn't focus on kind of like optimizing the model straight away it uh focuses on     
1:08:18     
giving sort of the best solution so you can do optimization but that's not really the     
1:08:24     
goal uh overall overall is to evolve different variants that are optimized     
1:08:31     
and then you know and people approach Us in different ways some people are just interested in sort of engineering     
1:08:38     
optimization and that's it other people are interested in the creative aspect and generating you know multiple     
1:08:44     
solutions for something seeing what those look like so oh well can you hear me yeah okay     
1:08:53     
that's nice my screen froze oh okay and my computer will freeze     
1:09:02     
after a bit I seem to have a problem     
1:09:08     
yeah okay so um I don't working at anything to say     
1:09:16     
or no just just listening in okay all right well thank you for     
1:09:22     
attending and um thank you Mah for your update and um see you next     
1:09:30     
week bye bye okay now I'd like to talk about a few papers um these are papers on C     
1:09:38     
elan's development that are really kind of relevant to some of the things we've been talking about with respect to     
1:09:45     
noise position of cells and sort of you know and then looking at uh lineage     
1:09:52     
trees and some of the metrics we can use for those so the connected to this discussion today about     
1:09:58     
hypergraphs this first paper is on novel metrics for lineage trees so we've done     
1:10:05     
some work on this where we've looked at mapping lineage trees the differentiation trees and quantifying     
1:10:11     
lot of this what they're interested in here is they're interested in looking at     
1:10:17     
uh new met novel metrics for celegans lineage trees which are of course easy     
1:10:22     
to study and so you know they they introduce some methods here that are     
1:10:27     
really intriguing so the abstract reads High throughput experimental approaches are     
1:10:35     
increasingly allowing for the quantitative description of cellular and organismal phenotypes throwing these     
1:10:41     
large volumes of complex data into meaningful measures that can drive biological Insight remains a central     
1:10:47     
challenge so one can resolve phenotypic measures for single cells onto their lineage history so what they're doing     
1:10:54     
here is is they're interested in taking single cell uh genomics and those types     
1:11:01     
of data and mapping them onto the lineage history of cells so we're actually looking at phenotypic measures     
1:11:08     
for single cells but we can also look at uh single cell measurements of uh     
1:11:15     
sequence uh nextg sequencing um so one can resolve     
1:11:21     
phenotypic measures for single cells onto their Lage history enabling joint consideration of heritable signals and     
1:11:28     
cell fate decisions so another thing you can do is take heritable signals so if     
1:11:35     
something exists in a in a Mother's cell it can be inherited by the daughter cells and that may have some relevance     
1:11:41     
to celf fate decisions so a Mother cell can transmit things to their daughter     
1:11:47     
cells those daughter cells then can differentiate and we want to know what those signals are so this is an     
1:11:52     
interesting point about how we might do that within the context of lineage indry     
1:11:58     
because it can really kind of Target some of those signals to specific cell fate     
1:12:04     
decisions most attempts to analyze this this type of data however discard much of the information content contained     
1:12:12     
within lineage trees in this work we introduce a generalized metric which we     
1:12:17     
turn Branch at a distance that allows us to compare any two embryos based on phenotypic measurements of individual     
1:12:23     
cells this approach allies with those phenotypic measurements to the     
1:12:29     
underlying lineage tree providing a flexible and intuitive framework quantitative comparisons between for     
1:12:36     
instance wild type and mutant developmental programs so when you have     
1:12:41     
a wild type which is the default gen genotype in a certain uh in a in a     
1:12:49     
individual or in a standard lineage tree and in celegans you see this as a     
1:12:55     
standard lineage tree because we do have a standard lineage tree that we have for wild type     
1:13:00     
genotypes uh you have the the type of genotype that we have characterized in     
1:13:05     
our data sets and so forth mutant developmental programs however produce     
1:13:10     
lineage trees that are incomplete sometimes entire sets of cells are missing sometimes the timing is     
1:13:17     
different of div Vision so we have mutant developmental programs and we can actually look at how those mutant     
1:13:25     
programs or at least say gene expression uh profiles for those mutant programs or     
1:13:32     
uh other types of measures can map to discrete changes in growth in division     
1:13:40     
rate and in the presence or absence of certain cells so in celegans we can look at this     
1:13:49     
uh quite uh you know quite directly using the lineage tree     
1:13:56     
we apply this novel metric to data on cell cycle timing for over 1300 wild     
1:14:01     
type and RNA treated seelan embryo so RNA is RNA interference so they use that     
1:14:09     
to knock down certain transcription factors you'll put in an antibody and     
1:14:14     
it'll knock down the transcription factor and you can or or you have a     
1:14:19     
reverse transcript tase RNA that can knock down the transcription factor and turn it off off     
1:14:26     
essentially and so we can do this for this is sort of generating a mutant     
1:14:32     
phenotype um our new metric revealed surprising heterogenity within this data     
1:14:37     
set including subtle batch effects and well type embryos and dramatic variability in RNA induced developmental     
1:14:45     
phenotypes so this is where we can look at some of this variability um and how these RI     
1:14:52     
treatments can really kind of change the nature of a lineage tree all of which     
1:14:58     
have been missed in previous analyses further investigation of these results suggest the novel quantitative link     
1:15:04     
between Pathways that govern cell fate decisions and Pathways that pattern cell cycle timing in the early embryo our     
1:15:11     
work demonstrates that the branch at a distance we propose in similar metrics like it have our have potential to     
1:15:18     
revolutionize our quantitative understanding of the organismal penit so this talks about lineage     
1:15:25     
tracing in seans and of course this is one of the main um organismal models in     
1:15:33     
which you know we really can learn a lot about lineage trees because there's a tight mapping between lineage tree and     
1:15:41     
this deterministic uh sulate and the timing of it so it's usually     
1:15:47     
deterministic across individuals and this is especially important one comparing like a wild type     
1:15:55     
to a mutant so we can look at the effects of mutations quite directly and it gives us a very strong signal so they     
1:16:03     
Define a metric on Spaces of cell lineages so they have to do this by kind     
1:16:08     
of taking the lineage tree thinking about the organization of that lineage     
1:16:13     
tree which is anterior to posterior so in other words the lineage tree unfolds     
1:16:20     
where the Le hand side is anterior and the right hand side is posterior and so     
1:16:25     
that gives us an orientation sort of an anatomical orientation to this uh to     
1:16:32     
this L tree and so we can use this to sort of derive these sort of tree ated     
1:16:38     
distances and so this reminds me of some of the stuff we were doing with differentiation trees although we didn't     
1:16:44     
do in the same way they're actually doing this graph ated distance from graph Theory and so they they derive     
1:16:51     
this measure C Tre at a distance which is based on Counting the minimum number of operations such as adding and or     
1:16:58     
removing a node or Edge that is needed to convert one tree to another so in in     
1:17:04     
differentiation trees we were reordering the tree and figuring out some of the local aspects of that reordering in this     
1:17:11     
case what they're doing is they're reordering the tree by adding or removing nodes or edges that can allow     
1:17:18     
us to convert one tree into another so you're just counting the number of changes you need to make to change the     
1:17:25     
tree to something else and that could be a mutant phenotype or it could be an     
1:17:30     
incomplete phenotype or whatever and so you know this is a good     
1:17:36     
way to do this you know this is this sort of reminds me also phal genetics where you kind of do these kind of     
1:17:43     
operations on uh Gene trees or species trees and so there are a whole I think     
1:17:49     
there are a whole host of computational techniques you can do with lineage trees that are really interesting     
1:17:56     
um so you know this this uh Tre ated distance is a very powerful tool it     
1:18:02     
essentially reduces the calculation in determining the number of nodes that are different between two trees a notion     
1:18:09     
similar to other measures so what so this is their figure one and this talks about these distance metrics or these     
1:18:14     
versions of distance metrics that they kind of give in this paper and so we     
1:18:20     
move from this tree at a distance to some other possible uh     
1:18:25     
distances that we can calculate on a lineage tree so uh the first one is tree edit     
1:18:31     
distance of course which we talked about this quantifies topological differences in nodes between two trees so we have     
1:18:39     
tree NX and Tre and Y Tre and X of course contains these cells uh Tre and Y     
1:18:46     
contains a different complement of cells and so when we knock certain cells out     
1:18:52     
we can get a different distance we basic basically look for the change between uh     
1:18:57     
NX and NY and we reorder it and we knock out cells and we get these diff this sort of     
1:19:05     
distance between the two trees so that could be APA and AP or aa aa and AAP and     
1:19:14     
that's those are the missing cells in each tree so NX has some missing cells     
1:19:20     
NY have some missing cells or NY might have cells that aren't in NX and vice     
1:19:25     
versa and that's their distance so they're looking at distances in terms of cells um in our we did some     
1:19:34     
experiments a long time ago where we looked at uh the information content of     
1:19:39     
lineage trees and this isn't the same thing as at a distance but it's interesting because again you know     
1:19:46     
you're using sort of your reordering the tree we were reordering the lineage tree into a differentiation tree and then     
1:19:53     
calculating the information content of that transformation so this is a very similar     
1:20:00     
thing um but it's it's but it's interesting how you can play with lineage trees and get these kinds of     
1:20:06     
information up the second is intersection Branch distance or IBD and this is the L     
1:20:13     
squared Norm between shared nodes value of trees so this is where you look at the union between Na and NB n Alpha and     
1:20:21     
N beta and it has so your just basic we intersecting two trees together you're     
1:20:27     
looking at the l s Norm between these shared node values so you have the shared node values between these two uh     
1:20:35     
trees Alpha and beta and then that gives you VX and VY or these values are VX and     
1:20:40     
v y and then this distance this intersection distance which is     
1:20:46     
the the norm of xus Y and so that's that's the way you get that um and so     
1:20:54     
this kind of show shows I think what this looks like so you have this these     
1:20:59     
numbers on the branches and these alternative orderings and you look at the Shared uh Branch dist or the shared     
1:21:08     
uh node values uh the third is Union branch distance so this is actually this is the     
1:21:15     
intersection I'm sorry this is the intersection this is the union so this is the intersection between Alpha and     
1:21:21     
Beta And this is the union between Alpha and beta so this is where you have n     
1:21:28     
Alpha and N beta and the union of those and this is where you have the L squ nor between all node values of trees so this     
1:21:34     
is actually taking instead of just taking the shared nodes between these two trees we're taking all node values     
1:21:41     
of trees and you're uniting them and I think the point here is to say that the intersection and Union are complementary     
1:21:49     
so the union is everything the intersection is just those things that are shared between trees and so that's     
1:21:55     
actually kind of like an information measure but it's not quite an information measure it's just kind of a     
1:22:00     
different way of looking at it in any case what you're doing is you're comparing Trex Tre Y and you're getting     
1:22:06     
information from each one and then you're kind of you know making a statement about what's shared what's not     
1:22:13     
shared and so forth and so their figure four summarizes um the union Ranch     
1:22:19     
distance and how it reveals heterogeneity in the RNA cell cycle time timing coordination so basically the     
1:22:28     
difference in the uh well type versus a mutant which is represented by these RNA     
1:22:34     
experiments is that it changes the cell division time and so this is something     
1:22:40     
that you know we can look at in terms of cell cycle timing and so this actually     
1:22:45     
takes the the Union branch distances and     
1:22:51     
combine you know creates this heat map that shows different between the mutant     
1:22:56     
and wild type phenotypes so this shows you in minutes from 0 to     
1:23:02     
1400 these differences across these different categories and so um this this     
1:23:09     
is the summary here you have the uh wild type embryos     
1:23:16     
you have this Tri ated distance the wild type you have uh these RNA embryos you     
1:23:23     
have the inter Branch distance to whale type and so that's what this is and then     
1:23:28     
you have this comparison of tre ated distance and intersection Branch Branch distance for RNA embryos so looking at     
1:23:36     
those mutant embryos you have a tri at it distance here which you're getting in this B uh I and I I I and then on the y-     
1:23:46     
axis you have mean intersection Branch distance which is what you have an II and IV so in one and three you have this     
1:23:55     
x axis and and two and four you have this y AIS and so this is a comparison     
1:24:01     
here you can see that there's a cluster down here with a smaller cluster over on this side so for triedit distance is     
1:24:08     
near zero there's a cluster and for tree at at distances between 100 and 200 there's another cluster with some     
1:24:14     
outliers going in this uh direction of a larger tree at a distance but you can     
1:24:20     
see that there are two groups here Flor uh this the mean intersection inch distance doesn't really predict very     
1:24:27     
much there you know these both of these clusters are at about a very low mean intersection bran distance so that means     
1:24:34     
that the Tre at distance is predicted most of this variation okay so the second paper and     
1:24:39     
this is quite a lengthy paper as well uh this is the terministic nature of     
1:24:45     
cellular position noise during seans zenesis narly in the meeting I kind of     
1:24:51     
went over some drawings of what I meant by cell position variation and maybe     
1:24:56     
some of strategies to control us in our data sets just of cell position what     
1:25:02     
this is though is it talks a little bit about the deterministic nature of this noise the abstract reads individuals     
1:25:09     
with identical genotypes exhibit great phenotypic variability known as biological noise which has broad     
1:25:16     
implications so this is where we have individual genotypes so we might have wild types of you know from different     
1:25:23     
individual maybe 10 wild type genotypes and there's variation in those wild type genotypes     
1:25:29     
but it's it's sort of random and it doesn't really there's no you know strong mutation or deviation from that     
1:25:37     
wild Tye um you do have defined mutants and C elegant for example and androp     
1:25:44     
that are backcrossed uh with respect to a specific functional mutation and so     
1:25:50     
that's really the difference between the wild tape and the and the uh mutant     
1:25:55     
genotypes but even within indivi identical genotypes they're so-called identical genotypes there exhibits great     
1:26:01     
phenotypic variability uh and there have been studies on this in seans at the N2 uh     
1:26:08     
wild type has a lot of variation across like you get it from all over the world it's it's the N2 uh genotype is supposed     
1:26:17     
to be from Hawaii but you can uh even those are are going to have a lot of     
1:26:22     
variation so this is all known is biological noise people study biological noise uh and there's of course intrinsic     
1:26:30     
biological noise and extrinsic biological noise the intrinsic noise is the noise that you have sort of due to     
1:26:38     
variation like what we're talking about here extrinsic noise is the noise introduced by the environment and so you     
1:26:44     
can have a lot of both in a data set what we're going to talk about here is biological intrinsic biological     
1:26:51     
noise while molecular level noise is been extensively studied in-depth analysis of cellular level noise is     
1:26:58     
challenging so this is where we're talking about molecular noise is the intrinsic noise and the ex in extrinsic     
1:27:05     
noise what they're talking about here also is cellular level noise so this is     
1:27:11     
um a little bit different this is talking about this noise and cellular position and again it can be driven by     
1:27:18     
noise in the developmental program as it work uh which is intrinsic work can be     
1:27:24     
driven by the environment extrinsic uh so this is this is uh kind of what we're     
1:27:29     
getting at but we're talking about the cellular level which is sort of manifest in a like something like a cell position     
1:27:36     
or a cell phen here we present a systems level quantitative and functional analysis of     
1:27:42     
noise in cellular position during embryogenesis an important phenotype indicating differentiation and     
1:27:50     
morphogenesis so this is uh noise and cellular position is uh really indicative of     
1:27:56     
differentiation of oogenesis we show that cellular position     
1:28:02     
noise is deterministic stringently regulated by intrinsic and extrinsic mechanisms so     
1:28:08     
cellular position noise actually is regulated by extrinsic mechanisms but     
1:28:14     
also intrinsic mechanisms noise level is determined by the cage identity and     
1:28:19     
there couple T developmental properties including embryonic localization cell contact and left right symmetry so this     
1:28:27     
is again where we have you know position is governed by Left Right symmetry of     
1:28:33     
course it's governed by cell contacts of what are the other cells coming into contact with that cell and its General     
1:28:41     
local localization in the embryo so remember earlier in the meeting I talked about these Point clouds you have     
1:28:49     
different observations of cells it could be within this point Cloud you end up with     
1:28:54     
centroid value which is sort of the mean of that point Cloud the mean position     
1:29:00     
now that CL Point Cloud isn't infinite it doesn't go across the entire embryo     
1:29:05     
it's going to be concentrated in a certain location in the embryo some of that is due to Left Right symmetry so we     
1:29:12     
expect like say um you know a cell that's on the left hand side to be on the left hand side and the right hand     
1:29:19     
side to be on the right hand side or uh ABA to be anterior an abbl to be     
1:29:25     
posterior so there are some Trends they don't just occur randomly but there's a lot of variation around those positions     
1:29:32     
cell contacts of course make these harder they they you know they're you     
1:29:38     
know the cell can't move around infinitely as well they contact other cells so their migration is sort of     
1:29:43     
constrained so we can actually understand this variation in that     
1:29:48     
context and indeed we shouldn't see just kind of a scatter of uh points we should     
1:29:55     
see maybe some patterns within those points as well this is what I was talking about with respect to the um the     
1:30:02     
hypernotes and some of the vocal sort of patterns within those hypernotes when we     
1:30:07     
make that um Tri uh tripartite coupling so we have like this uh mapping from the     
1:30:15     
mother to each daughter cell and we might have cells that are sort of in the left hand side of that space mapping to     
1:30:22     
daughters in the left hand side to that space so that's what I meant by that cells follow a     
1:30:29     
concordant high low or low high low pattern of noise Dynamics so this is where you have low noise High noise and     
1:30:36     
then low noise in fade specification triggers a global downregulation of noise that     
1:30:43     
provides a noise buffering strategy so this is where when you get to a certain     
1:30:48     
selfie there's a global down regulation of noise so noise is actually present in     
1:30:54     
Pur potent cell or a TOD potent cell at a higher level than what you would see     
1:30:59     
in a differentiated cell and the reason you have that is because those plur     
1:31:04     
potent cells those to potent cells you know they have to maintain this this stemness state but they also have to     
1:31:12     
sort of poise themselves for multiple Transformations so they can't be at like     
1:31:17     
in a strong sort of ground state and then uh differentiate into a new sub     
1:31:23     
cell type and the whole point of FL potency you have a cell that's poised to maybe go in many different directions     
1:31:30     
and so when it finally Finds Its sort of identity it downregulates everything and     
1:31:37     
in that way it can maintain that uh identity in the face of you know environmental pressures or whatever so     
1:31:44     
in cgans we know that cells are are uh deterministic with respect to their fate     
1:31:49     
but stem cells Flor poent cells in what they call rounder cells and sea elegans     
1:31:56     
maintain this higher level of noise and this allows you know the the daughter cells to have different Fates     
1:32:03     
so remember we have mother cells we have ancestral cells we have daughter cells the daughter cells from say a Founder     
1:32:10     
cell like AB can maybe take on many different Fates and so the noise has to     
1:32:16     
be at a higher level to Poise the cell for these different     
1:32:22     
changes uh cells follow con coordinate high low high low high low pattern so this is where you have this low noise     
1:32:29     
this High noise and this low noise so this is where we get this you     
1:32:35     
know we have low noise High noise low noise uh probably at the point of uh     
1:32:41     
differentiation you have this High noise or the at the point of uh fate     
1:32:47     
specification High noise and then low noise after differentiation noise is stringently     
1:32:53     
regul at throughout embryogenesis especially during cell division and cell adhesion and GAP Junctions function to     
1:33:00     
restrict noise so L cells have Gap Junctions and they're connected to one another through     
1:33:06     
that these are these electrical connections um you also have cell adhesion and cell division so cells just     
1:33:13     
can't float freely in the embryo they have to sort of stay anchored to you     
1:33:18     
know a position with respect to the rest of the cells in the embryo and then you have some div vision of course which     
1:33:24     
changes the complement of cells in the N so this these are all things that um you     
1:33:30     
know play a role in regulating noise over the course of embryogenesis collectively our study     
1:33:36     
reveals system properties and Regulatory mechanisms of cell wheel and always control during     
1:33:43     
development so this is a very detailed paper and it kind of gets into some of the specifics of     
1:33:49     
seans so in this paper they actually have an approach to looking at pairing cell positions between embryos so in     
1:33:57     
this case they've adopted a relative approach for representing a given target cells position is a vector of     
1:34:03     
geometrical distances from the Cent rate of its nucleus to those of all other coexisting cells so this is this type of     
1:34:10     
weighted average where you get this these geometrical distances and you use those as the weights to uh kind of     
1:34:18     
determine that that um centroid value so this is what I I was talking about with     
1:34:24     
respect to weighted averages we then put took CPN which is     
1:34:29     
this uh value is differences in that Vector between wild type embryos for     
1:34:34     
every cellid and realign time so they talk about intrinsic regulation sent     
1:34:40     
identity determine Sol position noise then they so they're asking the     
1:34:45     
question using these CPN measurements is CPN stochastic or     
1:34:51     
deterministic if deterministic predict CPN levels will be significantly associated with the developmental     
1:34:57     
properties of cells vast knowledge available concerning cegan cells and     
1:35:02     
embryogenesis allowed us to correlate CPN with diverse cell properties including cell minage cell fate 3D     
1:35:10     
localization and left right symmetry so they classified Leaf cells or the trace     
1:35:15     
terminal cells at the 350 cell stage into 12 groups based on the sublineages     
1:35:21     
they derived from and found that the level of CPN were sublineage specific so     
1:35:27     
this is where you have the sublineage specificity this is something we'll see     
1:35:32     
with respect to at point of 350 cells you have some differentiated cells some     
1:35:37     
not but again the founder cells are very general and as you move down low the H     
1:35:43     
tree the cells get more specific with respect to their ultimate fate so you     
1:35:48     
expect to see some changes that are suppl specific or for example cells from Ms     
1:35:55     
and P3 sublineages exhibited higher levels of CPN while cells from the abpr     
1:36:01     
and c sublineages exhibited lower levels to further elaborate the     
1:36:07     
relationship between CPN and cell lineage we search for CPN transition points on the cell lineage tree to find     
1:36:14     
as ancestral cells that give rise to two sublineages with significantly different     
1:36:19     
levels of CPN any transition points are identified suggesting CPN is strongly associated     
1:36:26     
with lineage emage as a bilateral animal the C elegan body plan is Left Right     
1:36:32     
symmetric so we expect that these pairs exist at the left side and the right side and they actually have similar     
1:36:39     
behaviors because they derive from the same other cell we ask whether these Left Right symmetric cell pairs exhibit     
1:36:45     
similar CPN levels corelation analysis indicated that the CPN levels of corresponding left and right cells were     
1:36:53     
high correlated moreover the divergency of CPN values between left right symmetric     
1:36:58     
spells was significantly lower than in mock cells so this is where we uh     
1:37:05     
observe this you know connection between the Left Right pairs in terms of function and so this is something that     
1:37:12     
we can expect because they're basically mirror images of it's worth mentioning     
1:37:18     
that this is not Universal that there are left right asymmetries as well so we     
1:37:24     
can focus on Left Right symmetries which represent a majority of the lineage tree but it's actually the asymmetric     
1:37:30     
relationships that are more interesting so this is where we get to temporal regulation in this low high low pattern     
1:37:37     
of noise Dynamics so having established the influence of lineage localization on CPN level we next s to answer another     
1:37:45     
question how do CP CPN levels change every time quantification of global CPN     
1:37:52     
at each embryogenetic time Point revealed a smooth and dynamical tempor dynamic temporal profile as shown in 5A     
1:38:00     
which we'll get to in a minute this profile follows a low high low pattern     
1:38:05     
that is during early embryogenesis Global CPN progressively increases with     
1:38:10     
time until mid embryogenesis approximately between the 64 and 128 cell stage or I guess in the     
1:38:18     
ab lineage so that's 256 or from 120 to     
1:38:24     
256 thereafter the CPN level gradually reduces and by the 350 cell stage it     
1:38:30     
reaches a level comparable to the beginning of embryogenesis so this is noise like what     
1:38:36     
I was saying about noise that noise you know cells are pois to make changes and     
1:38:41     
what they're saying here is that very early in the wind industry noise is low generally and then it's at a sort of a     
1:38:48     
high level as things are diverging and things are starting to differentiate and     
1:38:55     
then by the 350 cell stage we get this lower level of CPN that's similar to the     
1:39:02     
beginning of embryogenesis and so you know there's there are also linkages to Global     
1:39:10     
developmental patterns as well uh that we might be interested in in correlating     
1:39:15     
this way we found the pattern of noise change     
1:39:21     
to be tightly aligned the developmental stage rather than the absolute time of development so this means that there's     
1:39:28     
this developmental stage aspect that isn't that is sort of decoupled from time of development so if you change the     
1:39:34     
temperature that in which C elegant embryos are veed in if you change it from like 21 C which is near room     
1:39:41     
temperature to 16 C you can actually change sort of the growth of the embryo     
1:39:48     
the timing of the embryo so if I were to compare an embryo raised at 21 C with a     
1:39:53     
number are raised 16 see the timing would be stretched out it be delayed and     
1:39:59     
so the they're going to be changes in noise that reflect this delay timing at     
1:40:05     
16c so this is an interesting finding because there are some mutants that actually need to be reared at like 15 or     
1:40:12     
16 c um and you know they're they're viable if they're raised that that uh     
1:40:18     
temperature if they raised at room temperature they're not viable which is an interesting thing itself but it does     
1:40:24     
speak to this change of CPN changing of the pattern of CPN changes with respect     
1:40:30     
to rearing temperature and so as I said before fate specification triggers a     
1:40:35     
global downregulation of position noise we expect that position noise to be lowered as fate     
1:40:42     
specification um increases so when we have changes of Fate you know they     
1:40:50     
basically stop position noise so those cells have to be in a certain position     
1:40:56     
as they differentiate so once these cells differentiate they need to be in a specific position in terms of CPN CPN is     
1:41:03     
under stringent developmental constraint uh this is position noise being tightly controlled throughout     
1:41:10     
embryogenesis especially during cell division so we want to they they want to     
1:41:16     
test whether CPN is subject to tight constraint by comparing observed CPN levels to expected values based on the     
1:41:23     
Sy simulation so they looked at the simulation and they found that the expected CPN was higher than the     
1:41:29     
observed CPN for all cells suggesting that on the cells it's more tightly     
1:41:34     
regulated so in conclusion they they're finding shut light on the systems wide regulation of position noise first we     
1:41:41     
show that position noise is tightly controlled throughout embryogenesis especially during cell division to     
1:41:47     
maintain a reproducible cellular configuration in embryos so we should expect that because of course C elegans     
1:41:54     
we know that cells end up in a specific position and there's very little     
1:41:59     
variation across embryos at least in in theory except for in the defined mutants where there maybe     
1:42:06     
specific changes but even in the defined mutants we know what those changes are are very apparent and so this is     
1:42:13     
interesting from this perspective of how to uh SE elegans embryos maintain this     
1:42:18     
control over position and this of course speaks to variation of position as well because what they're looking at here is     
1:42:25     
the variation position with respect to the final outcome and what we're talking     
1:42:31     
about as something different which are the different independent observations and that noise but there there parallels     
1:42:37     
between them because those cells need to be in a specific place um it is well known that in     
1:42:44     
certain early blasting such as EMS in C Elegance um and ab AR the mitotic     
1:42:51     
spindle is tightly regulated during cell division to establish cell symmetry or to place daughter cells in     
1:42:58     
specific positions for Signal receiver our data extend previous findings by showing that div vision of most cells is     
1:43:04     
stringently regulated to place daughter cells at reproducible positions so the     
1:43:10     
daughter cells need to be in places sort of take their places and so we need to     
1:43:15     
have this sort of control that has some noise but it's also very tightly controlled and that cell movements     
1:43:22     
within a cycle are also subject to constraint although to a lesser extent compared to cell division second our     
1:43:28     
data predict the existence of cellon Age based intrinsic and cell location based extrinsic mechanisms in determining     
1:43:35     
position noise     
1:43:42     
level together our study provides insight into the systems properties of and of and Regulatory mechanisms guiding     
1:43:50     
cellular noise during invivo development so I hope that was an interesting     
1:43:55     
presentation on these two papers let me know if you have any questions     
