## TRANSCRIPT     
     
0:01     
hello hey hi how are you how are you I'm good     
0:07     
good hello M hi Bradley     
0:14     
hian great hello     
0:21     
Morgan another Susan hello     
0:27     
hi any updates from Susan or um no I'm supposed to be writing a     
0:35     
paper oh yeah so I I have to get going     
0:40     
and do that other than that no just yeah supposed to be sort of a     
0:47     
review paper oh really for a journal or yeah um on tissue and integrity for some     
0:56     
reason yeah that's good well welcome uh uh yeah last week thank you to mahul for     
1:03     
going over his uh hypergraphs work that he's been doing and he's been doing this     
1:09     
with other people and I like the direction it's going hopefully we can     
1:16     
continue down that road this summer um and of course we have the     
1:23     
divil graph repository for people who are interested in Google summer code and you know maybe     
1:30     
working on some parts of that that mahul um talked about last week okay so we had     
1:38     
a comment uh from soam this is uh Morgan had sent me this paper which shows how a hypergraph     
1:45     
extension makes a graph neural network better than a specialized topological     
1:51     
model yeah so I was wondering if we could do a similar thing for the uh     
1:57     
developmental graph neural networks which we have if it could extend it to     
2:02     
hypergraphs um uh maybe yeah could you put a link to     
2:08     
the actual paper in the chat is that an archive paper     
2:14     
yeah yeah all right so this is um the archive     
2:20     
paper that soam and uh Morgan are talking about uh     
2:26     
this is enhancing the utility of higher order information and relational     
2:33     
learning and uh so let's let's go through the abstract and see what they're talking about looks like they're     
2:38     
talking about multi-way networks the abstract reads um high order     
2:44     
information is crucial for relational learning in many domains where relationships extend beyond par wise     
2:52     
interactions so yeah we typically think of our standard graph um you know our     
2:58     
graph topology as a set of e ja connections there these hairwise     
3:04     
interactions and of course you can characterize things Beyond paway interactions by looking at say like     
3:10     
their neighborhood or their Community or you know some sort of higher order     
3:15     
construct and we're talking about using hyper noodes for that um in hypergraphs     
3:21     
so this is of course what they're getting into with hypergraphs hypergraphs provide a natural framework for modeling such     
3:28     
relationships which has motivated recent extensions of graph Network architecture to     
3:34     
hypergraphs however comparisons between hypergraph architectures and standard     
3:39     
graph level models remain Limited in this work we systematically     
3:44     
evaluate a selection of hypergraph level and graph level architectures to determine their     
3:51     
effectiveness and leveraging higher order information in relational learning so they're picking hyper graph level and     
3:58     
graph level architectur so I assume they're doing what they're doing with graph level is they're either     
4:04     
comparing them or they're looking at different attributes of their connectivity um our results show that     
4:11     
graph level architectures applied to hypergraph expansions often outperform hypergraph     
4:17     
level so this is where they have like a standard graph level architecture and     
4:22     
then a hypergraph expansion what they're how they're going to do that but that's they talk about     
4:29     
about that in the paper so this often outperforms hypergraph level ones even on inputs     
4:36     
that are naturally parameterized as hypergraphs as an alternate approach for leveraging higher order information we     
4:43     
propose hypergraph level encoding based on classical hypergraph characteristics     
4:48     
so hypergraph level encodings I guess being the you know things at the level of hyper graph so I think we've done     
4:55     
this a little bit with the developmental hyper graphs where you have     
5:00     
you know maybe your nodes are cells and your hyper nodes might be     
5:07     
tissues or different regions sometimes they could be like we talked about the last few weeks we've been talking about     
5:14     
lineage trees having sublineages as those hyper nodes and so you know we can     
5:21     
have these hypergraph level encodings um usually people kind of I     
5:27     
guess people kind of do that in hypergraphs but yeah this is this is     
5:32     
interesting how they're kind of approaching this while these encodings do not significantly improve hypergraph     
5:39     
architectures theal substantial performance gains when compined with graph level models our theoretical     
5:46     
analysis shows that the hypergraph level en codings uh probably incre or provably     
5:52     
increase representational power of message passing graph neural networks beyond that of the graph level     
5:58     
counterparts so message message passing graph neural networks you this is where you have um a graph and you're using a     
6:07     
message passing protocol and there's you know standard methodologies for that     
6:12     
basically you're measuring not just the connectivity but the messages pass between them which you know could be     
6:17     
like in a well in a neural network it's information being um processed by the     
6:24     
graph but sometimes it's you know in where there's some communication in the graph where there's activation in the     
6:31     
graphs of from one node to another so you have these message passing protocols that that are sort of designed for this     
6:37     
and it's not necessarily I don't know how suitable it is for biology certainly people use     
6:45     
message passing protocols for things like proteomics and other area domains of computational biology but you know     
6:53     
it's interesting to think about I don't know what the state-ofthe-art here is in message passing um in in biolog iCal     
7:00     
applications but it would be interesting to think about so they actually talk about     
7:06     
multi-way networks and they bring this up and this is something that um kind of     
7:12     
is an interesting topic it's kind of a niche topic so you know this is where they're     
7:19     
talking about high using graphs to represent high order information um the multi-way networks     
7:26     
arise in many domains in the social and Natural Sciences where Downstream tasks     
7:31     
depend on relationships between groups of entities rather than the pair wise relationships captured standard networks     
7:39     
so they're actually you know where we have this sort of nested set of information so if you have like for     
7:46     
example um a bunch of cells they have multiple     
7:51     
attributes and you can n you know you can build a a network based on one set     
7:58     
of attributes but you also have this these other alternate sets of attributes that you     
8:04     
can then also organize the these relationships but um you know another     
8:10     
thing is like when you have a cell a developmental cell it has a a certain     
8:16     
role in development and then it differentiates into some sort of an adult cell you can capture those     
8:23     
relationships with multi-way networks they site Bick 23 venson 21 and     
8:29     
sha 21 I'm not sure what those citations are what are higher order networks in cam     
8:37     
review is the bck from 23     
8:43     
and um Enon 21 is Horder network     
8:48     
analysis takes off fueled by classic idas a new data that's on the     
8:55     
archive and Sh 21 is signal processing     
9:00     
on higher order networks living on the edge and Beyond that's from signal processing so there's some interesting     
9:07     
papers here a lot of interesting uh uh Network Theory papers in here as well I     
9:14     
see so anyways um this enhanced and flexibility has motivated a growing body     
9:20     
of literature on extending classical graph Network architectures to hypergraphs including message passing     
9:27     
and Transformer based models so there's message passing and Transformer based models message passing     
9:34     
is this unig GNN a unified framework for graph and hypergraph neural     
9:40     
networks and Transformer based models is     
9:45     
li24 which is this hypergraph transformer for semi-supervised     
9:51     
classification so we're talking largely about graph we're kind of uh segwaying     
9:57     
to graph Neal Networks typical delation studies compare hypergraph architectures     
10:03     
against each other but not against standard graph neural networks here we perform a comparison of selection of     
10:09     
both types of architectures we observe that graph level architecture strictly from current     
10:15     
hyper level ones even if the input data is naturally parameterized as a hyper     
10:20     
graph in that case the graph nural network is applied to the hypergraphs thak expansion a natural     
10:27     
reparameterization okay so that's where we have I guess     
10:33     
uh we I don't know what I'm not familiar with the term cleek expansion but     
10:39     
basically we reparameterizing the graph so um and then so they raised this     
10:44     
question how can higher order relational information be effectively utilized for learn learning so we have all sorts of     
10:52     
information in these these graphs and the question is you know basically we     
10:57     
have this not just set of uh relations but this relational set     
11:03     
of relations that makes any sense um where each node has like different     
11:09     
attributes and you can have these alternate topologies     
11:16     
emerge so let me see if they have so they have a bunch of things in architectures here we kind of describe     
11:22     
them overview of architectures it's a math about hyper level uh hypergraph level architecture     
11:30     
here Transformer based graph Neal networks message passing graph neural     
11:36     
networks and then they talk about these encodings that they're using they using     
11:41     
structural and positional encodings so this is where you have structural information and positional     
11:49     
information um and both of these would be relevant to developmental biology by     
11:55     
the way so the structural information would be you know I guess     
12:01     
um it could be Omen clature it could be tissue type it could be other things     
12:07     
whereas positional would be something about like it's position along an anatomical Axis or its coordinates and     
12:15     
space so we have these different encodings um and we can then sort of use     
12:23     
that as a way to as a Way Forward en codings can capture either local or Global properties the input graph so     
12:30     
you're actually taking an encoding of the connectivity Matrix if     
12:35     
I'm understanding this correctly and it depends on the sort of the set of relations you're interested in and I     
12:43     
don't know if you have multiple encodings or not but I like this idea of having an encoding on top of the     
12:50     
connectivity graph uh empirical evidence demonstrates     
12:55     
that incorporating these positional and structural encoding sign ific L improve the performance of     
13:01     
gnn's okay so you actually can use uh different there are different     
13:06     
types of approaches like gra graph plans and other types of random walk     
13:12     
methods that can you have help here um then first I talk about     
13:17     
representational power expressivity uh which functions can and cannot be learned by the model so this     
13:25     
is of course you know we want we want to keep that in mind     
13:30     
um so I guess this is an example here um so this is figure     
13:38     
one a pair of graphs from the bre basic category the graphs liftings the hyper     
13:46     
Edge sizes of no degrees so these are like I guess uh yeah so your top left is your     
13:56     
basic category your graph liftings are the top right hyperedge sizes are bottom     
14:02     
left and no degrees are bottom right so you have uh Count versus hyperedge size for     
14:09     
the first two no degree for the second two on the bottom at the top we have two     
14:15     
graphs graph a and graph B and then on the upper right hyper graph a and Hyper graph B so for the graphs versus the     
14:22     
hyper graphs uh we look at hyper Edge a or hyper graph a edge degrees distribution     
14:30     
so this is 133 hyper edges in this and we have or actually     
14:35     
wait a minute this is graph hyper Edge I don't know why they're doing it like this I guess it's they're taking uh     
14:44     
hyper graphs from graphs here and graphs from hyper graphs here but anyways they show these um for     
14:51     
these different I guess they're uh similar topologies and they're showing the different hypergraph um uh     
14:59     
hyperd distributions and then for this who have these hyper graphs or these     
15:05     
groupings and then you have um so you have the hyper graph Edge and     
15:12     
node degrees here you have the graph uh this or the graph representation up     
15:18     
at the top and the hypergraph ones up here so they all result in these different sizes and degrees     
15:26     
okay so that's that's kind of I don't really understand it I guess Without Really diving into the paper but that's     
15:33     
kind of one of the comparisons they're making uh this this actually     
15:42     
is let's see so they do a set of     
15:48     
experiments and then their discussion let's get to the discussion so we can think about this a little bit     
15:54     
more in this study we investigated the performance of hypergraph level architectures     
15:59     
in comparison with graph level architectures for multi-way relational working tasks additionally we proposed     
16:06     
hypergraph level en codings as an alternate approach to leveraging high order relational information so that's     
16:13     
really we're interested in um there's some lessons for model design some     
16:19     
limitations so okay when they say lifted they mean reparameterized so a key limitation of     
16:26     
the study is the lack of a bench Mark consisting of true hyper graph structure     
16:32     
data many of the existing data sets consist of graphs that are reparameterized or lifted to hyper     
16:38     
graphs so these are where you have graphs like if we had a graph of the     
16:44     
devor group and we had connections between people and the devor group we     
16:50     
could then lift those data and Creed and this is kind of what we're doing with the developmental hyper graphs we're     
16:57     
lifting that reparameterizing that to hypergraphs so you know that's that but     
17:04     
then I guess what they're suggesting is is that um you want to have a true     
17:09     
Benchmark that is a actual hypergraph to to ensure that that lifting process that     
17:15     
reparameterization is Is On Target this suggests the establishment     
17:20     
of better Bunch marks is a key direction for future work and they're interested largely here     
17:26     
in topological deep learning or at least that you know and that's something that we've talked about as well that's kind     
17:32     
of a goal for a lot of these approaches is to get like these topological representations and topological data     
17:39     
analysis and all this uh we envision future benchmarks     
17:45     
that are based on scientific data such as these citations here were multi-way interactions that are     
17:52     
naturally parameterized as hypergraphs are known to arise another limitation of     
17:58     
the study arises in the choice of hypergraph architectures while our selection was     
18:04     
Guided by top performing models and reaches and benchmarks these two citations here a more comprehensive     
18:10     
analysis further strengthen the validity of reported     
18:16     
observations so basically they uh what they do here raises questions about the     
18:22     
effectiveness of existing message passing schemes and hypergraphs uh a possible lens for such     
18:29     
an investigation which is this where you want to investigate the uh     
18:34     
architecture's ability to effectively incode air order information could be graph reasoning     
18:41     
tasks suggested in this citation luo 2023 um Additionally the negative     
18:48     
results observed regarding hypergraph level encodings with hypergraph level architectures W further     
18:55     
exploration specifically understand specifically understanding how to effectively augment hypergraph inputs     
19:03     
with structural and positional information that can be leverage by hypergraph level architectures was a     
19:09     
promising direction for future study so this and then of course they talk about the different top logical domains of     
19:15     
interest and then that's the paper um so that's that's interesting thank you for bringing that paper to my attention     
19:24     
let's see if we have things in the uh chat here so we we     
19:29     
have uh this this link from Morgan melany weber.com one of the authors of     
19:35     
this paper so this was the third author melany Weber from Harvard let's see the     
19:43     
Publications uh yes this is pretty uh much this sort of topological graph neural network type     
19:50     
stuff here's some citations under her Publications that might be of interest     
19:56     
here we also have this uh this is her GitHub     
20:02     
repository this is actually hyper graphing codings and so this is actually the repository for     
20:10     
gol which was actually for this paper so this is their repository     
20:17     
um and this shows okay so this shows some simulations here so this these are the hyper graphing     
20:23     
codings um and just showing so this is     
20:28     
okay so this is I guess a graph this is the hyper graph uh embedding and then this is a random     
20:36     
Walker on the hyper graph so yeah and they're they're doing     
20:42     
in the graph is they're doing a random walk you can also do a random walk on the hypergraph and then you have this     
20:50     
hypergraph random walk here so thank you for those links in the paper     
20:55     
again um so did we have any comments or questions questions about that no I just     
21:01     
thought U yeah it's a from that that same kind     
21:08     
of community that does um what are they called like Symmetry and oh yeah     
21:16     
um I've got their thing here Symmetry and geometry and neural     
21:22     
representations yeah oh yeah that that's the yeah you know and and it's the     
21:29     
yeah just just like her her CV is is you know     
21:35     
absolutely um very typical for that that Community     
21:40     
yeah yeah uh um and um yeah but     
21:46     
uh interested well like like they say you know it's it's about I mean     
21:56     
the the um improve mement is is seen in these message passing or Transformer     
22:04     
graph neural networks and you know and and baset that have     
22:10     
been what was the term they used re recast re reparameterization     
22:16     
renormalization I guess you know as as hypergraphs yeah     
22:24     
yeah so just yeah     
22:31     
yeah I think that's an important point and we haven't talked about that in that context uh so far because we've talked     
22:38     
about we've done kind of the operational stuff of creating hyper graphs and graph     
22:43     
data I mean we have like the connectivity matrices and then we have these hyper graphs that we've created     
22:50     
and of course your graph nodes represent say cells and then the hypergraphs     
22:55     
represent categories sometimes they represent you know other things and you can have     
23:01     
things within cells so each cell could also be um a hyper node with different     
23:08     
attributes in them so like you could have you know different phys you know U     
23:13     
mechanical properties different types of proteins represented different types of gene     
23:18     
expression represented and then so all these things are kind of nested categories so your     
23:25     
hyper nodes are basically some set of nodes and how they're structured but you know     
23:31     
you also have to think about this reparameterization or renormalization every time you move up a     
23:38     
level you have or every time you're using some sort of you know set of     
23:43     
relations so say we were interested in like for every cell in the celegans     
23:49     
we're interested in the complement of gene expression we have this for different genes we've seen this before     
23:55     
many times where each cell will have you know you'll be interested in gene expression for different genes maybe     
24:03     
different um you know different like I guess complements of genes and their     
24:08     
expression and you'll but the point is is that those are different sets of     
24:13     
relations and so then that can be you know your cells can be     
24:19     
hypernodes your compliment genes can be hypernodes and you can treat it like that but then you have to also     
24:25     
renormalize the graph I guess for each time you each time you do     
24:31     
that and then you also ideally would have to also have a benchmark from what I'm understanding they also talked about     
24:38     
how you need to properly Benchmark these against something else and in our case I don't know what we would Benchmark it     
24:44     
against um a lot of times people will in network science I know they'll use like     
24:49     
random networks or they might use some other type of network that's where you know     
24:57     
the connectivity is known you could perhaps create pseudo data to     
25:02     
have something that you know kind of know know what the distribution is so     
25:07     
you can compare it with what your result is for your experimental graph so there     
25:13     
are a lot of ways you can make comparisons and and I think it's just kind of speaking to some of these other     
25:19     
ways for so instead of just saying we're going to create a hyper graph from our graph we're creating a     
25:27     
hyper graph we have multiple sets of relations we can create the hypergraph     
25:32     
from and we need to renormalize that and then we need to also compare it to something else to make sure it's it     
25:38     
deviates random in the ways we expect so I mean there are a lot of things to     
25:44     
consider and um yeah there is this community symmetry geometry neural representations group they're out of     
25:52     
they're often represented at NPS so they have usually run a workshop there every     
25:57     
year and you know there's also the graph neural networks conference where people     
26:03     
talk about symmetry they talk about hypergraphs I've seen people talk about hypergraphs at that     
26:09     
conference and message passing is a huge topic as well so yeah this is this is all very     
26:16     
good stuff um I guess soam and mahul uh if you have what do you have     
26:21     
any comments or questions uh yeah so I I miss a few like     
26:29     
little contexts in between uh but I was thinking about benchmarking so what could how could we do Benchmark in our     
26:35     
case whether it can be based on the techniques we are creating graphs or hyper graphs uh we can't Benchmark Based     
26:43     
on data definitely so what I can what I can think of is like different techniques or different uh distance     
26:50     
thresholds like we were discussing yes like last week based on those things we can do some kind of     
26:57     
benchmarking yeah yeah it's usually you know it would be an idealized measure I     
27:04     
guess would be a good way to do because it's really hard to do a biological Benchmark you could do like another     
27:10     
species like you could create graphs of like another species but there are a lot of difficulties with making like good     
27:18     
inferences from that um you could have a lot of times in     
27:23     
celan's research and this is just kind of from experiment m al perspective but     
27:29     
they'll use like a wild type and then have like mutant but of course the wild     
27:34     
type is what we're kind of dealing with so we don't have a good Benchmark     
27:40     
against a wild type wild type just meaning a normal sort of celegans as     
27:46     
opposed to like a mutant where they're things that deviate from like the normal     
27:52     
genotype so you know we have but I think a benchmark would be maybe something     
27:57     
where yeah you have like an idealized distance threshold maybe their distance     
28:02     
thresholds Are All Uniform maybe they're you know kind of according to a normal distribution or     
28:09     
something like that and then you test what you have in your data set and you     
28:14     
know there's no biological relevance to to maybe a benchmark but it does give     
28:19     
you something to base your comparison off of like you know     
28:25     
how uh you know how to if we have different samples like if we took a bunch of cell     
28:31     
tracking data and we broke it down into 10 parts and we just use that as as our     
28:37     
sort of our test cases how would that all compare to a benchmark or you know     
28:42     
how does you know I don't know have to think about that more a     
28:48     
bit Yeah makes sense makes sense probably I would also try to think about it and find some relevant later okay     
28:56     
yeah that's good yeah again it's going to be a little tougher with Biology because you know um I think a lot of the     
29:04     
stuff in the graph neural networks literature is really kind of theoretical you know it's like kind of like     
29:09     
theoretical machine learning so people aren't like actually throwing data sets at it per se they're just working on the     
29:17     
architectures and they're working on maybe at best they're working with very highly idealized data sets we not     
29:24     
actually like trying to get per uh performance metrics and optimize them with whatever they do     
29:31     
in regular machine applied machine     
29:36     
learning yeah all right so yeah we keep losing soam but I     
29:43     
wanted to ask soam about this um but anyways yeah we can come back to this later if you want     
29:50     
I think that's a good thing to read uh for our you know people are interested     
29:55     
in working in hypergraphs it brings up a lot of interesting issues and maybe we'll follow up on that next     
30:02     
time um I I'll go over it again and see if we can if I can find something that's     
30:08     
complimentary to that or if you know there's some other issues we want to raise and think about that be good as     
30:17     
well so thank you again to Morgan and mul and soam for bringing that forward     
30:23     
and discussing so I wanted to talk about this paper that came out I think it was     
30:28     
this last week it's a really interesting paper this is more along the lines of     
30:34     
like bio complexity so I present you with this figure and this is the paper     
30:39     
this is a paper the title was life sets off a Cascade of     
30:44     
machines and U machines being I guess biological     
30:50     
processes and so this figure is from the paper and this graph okay so the x-axis     
31:00     
is log normalized size over meter or size in meters or size over meters so     
31:07     
it's basically the size of this uh phenomena or the scale of phenomena so     
31:14     
you have from left to right you have atoms macromolecules self- reproducing machine     
31:21     
and that's in the realm of procario as we'll get to in a minute so we have     
31:27     
spores uh other cellular system you know we have Cellular     
31:33     
Systems uh protoc cells things like that and then we have     
31:39     
development and then we have Evolution and ecology which kind of extend out to     
31:44     
the largest scale so in blue they kind of describe specific     
31:50     
systems uh at atoms we have electrons protons photons then we have moving up a little     
31:57     
bit small molecules and water and then we have proteins DNA RNA super molecular     
32:05     
complexes viruses and organel super molecular complexes viruses and organel are all these in     
32:11     
this macro molecules category then we have membranes so we're starting to move into     
32:17     
cells and then procaryotes so procaryotes being a type of cell in     
32:23     
between membrane and protot we could put um these protocells     
32:28     
we could put um maybe some other types of cells you know so we have     
32:34     
procaryotes we have ukar um so we have viruses down below     
32:41     
organel organel being like parts of a of a cell a eukaryotic cell we have     
32:48     
membranes then we have procaryotes then we have ukar so UK carots are more     
32:53     
complex than procaryotes they're up on this um other other axis which is log     
33:01     
normalized time and that's in seconds so the time scale is compared to the size     
33:08     
scale and then from UK carots we have multicellular we have colonies so     
33:15     
multicell and colonies are kind of you know they're similar in in a number of     
33:20     
ways when they say colonies they mean like cells that live in colonies multicellular just means multicellular     
33:26     
Aggregates and then populations that's usually some sort of organism ecosystem and     
33:34     
biosphere so and then we have these two Trends so the first trend is this micro     
33:40     
Cascade and that's uh where time is equivalent to uh L to the     
33:47     
4th so this is exponential this is the early part of this curve and then we have the sort of     
33:54     
logarithmic phase which is macro Cascade which is where time is approximate to L so we     
34:03     
have time approximate to L the 4th time approximate to L and these are micro and     
34:08     
macro Cascades respectively so this is interesting that we get this phase transition here at this area of self-     
34:16     
reproducing machine so this is a very intriguing uh figure and so let's get     
34:22     
into these articles to tell us more and this was something something I think     
34:28     
Morgan posted in one of his slack channels and I've seen this around too     
34:35     
um this is from Derek's mind blog this is something to check out this article     
34:41     
here is uh life sets off a Cascade of machines and so uh a fascinating pnas     
34:51     
article by tusi and laacher which we'll get to in a minute offers an oversimplified language of life     
34:58     
it is a long article but I found it a very worthwhile read so the significance of this article this paper follows an     
35:05     
idea by leits that life can be seen as an infinite Cascade of machine making     
35:10     
machines down to at Atomic machines it proposes an oversimplified language of     
35:16     
Life elating certain scaling aspects and the key step of self- reproduction with     
35:22     
a singular point at one micron and a thousand seconds so what they're basically saying in this figure and this     
35:29     
is a blurred version of the figure is that we start with atoms and we and we     
35:34     
can we have we can take this reductionist view we can go all the way down to atoms and these are components to these     
35:42     
machines and so atoms and macromolecules are all components to a machine and when     
35:49     
you get up to what they call the self- reproducing machines category which is incidentally where this phase transition     
35:55     
occurs we get these sort of these uh reproducing machines or machine making     
36:01     
machines and so this is you know something where I think a lot of people have kind of uh noticed that there is     
36:08     
this sort of hierarchy of systems in life so life is organized as they say um     
36:17     
vertically meaning that you have this vertical organization from Individual     
36:22     
units to these large scale structures that are emerging phenomenal so you know     
36:30     
you can go to any biology textbook and see the sort of vert vertical organization of life but then the     
36:37     
question is is where do you get self reproduction that is where do you start     
36:42     
to get things that sort of are self-perpetuating one might argue that you see that with like DNA and RNA and     
36:50     
in fact in early life you might see that as like the primary mode of self- reproduction but what they're talking     
36:56     
about here are these machines means so you have to have all sorts of different     
37:03     
components to have the self-perpetuating machine that produces metabolism and reproduces itself in full     
37:12     
and everything else so you see these self- reproducing machines right around the time Pro carots emerge so that's     
37:19     
where sort of they they put this sort of phas transition between all these     
37:25     
different processes that perpetuate these     
37:31     
um these uh self reproducing machines so development perpetuate self- reproducing     
37:37     
machines Evolution ecology also do this whereas macro molecules and atoms may be     
37:43     
will to reproduced but not in this sort of systemic systemic sense okay so here's the paper uh in     
37:49     
pnas this is ly Ander they talk about multiscale     
37:55     
organization dense with inter connected cycles and Loops is one of living Matter's most striking features this     
38:02     
paper proposes a view but is also the most fundamental one concretely we speculate that life can be presented as     
38:09     
a hierarchical Cascade of machines making machines the simplified language of     
38:15     
machines is false about the reality of living matter but we hope it is still a useful vehicle to talk about the extreme     
38:22     
varieties of life so this is an important Point people like use machine metaphors for biology especially when     
38:28     
you get down to the molecular scale in the case of molecular machines we do have molecular machines you can think     
38:34     
about it like a machine is being highly efficient and things like that life doesn't really have those same     
38:39     
properties but they're using this as sort of a vehicle to talk about you know     
38:45     
these processes hierarchical cascadic processes making other processes and     
38:51     
that would be very similar in our cybernetics group we     
38:56     
talked about we talked talk about closure of systems so systemic closure and um this is I guess kind of     
39:04     
coming from the same perspective where you have these self-perpetuating systems     
39:09     
with these processes that can sort of reproduce themselves so a lot of tend you'll have     
39:16     
processes that are sort of you know kind of uh not self-perpetuating you put energy into a     
39:23     
system and it runs until the energy runs out and self perpetuating system you get     
39:29     
multiple systems that don't require inputs from the outside world that you can have things that run kind of on     
39:35     
their own so like something like reproduction or you know if you have a     
39:41     
seed you can carry a seed around your pocket put it in the ground the Sprout     
39:46     
and that genetic information is there as long as it doesn't get degraded by uh     
39:51     
VAR like like UV radiation or by being dissolved in water it can plant itself     
39:58     
in different places and grow so these are all things that are you know kind of what they're getting at with this when     
40:04     
they say machines making machines is simply a metaphor thus the paper is more about     
40:10     
assenting certain scaling aspects of life rather than about in the intri     
40:15     
infinite intricacy of life itself and we hope there should be a mathematical language that captures life more fully     
40:22     
than a purely realistic view such languages should fit their subject living matter like the formal language     
40:29     
laier defies fit his Elementary theory of chemistry uh and this this is described     
40:37     
as that the art of reasoning reduces to a well formed language so that's what they're getting at here um to begin this     
40:44     
discussion let us gaze for a moment at the rudimentary bacterium and uh imagine     
40:49     
for now this minute creature that has two infinite Cascades emerg and that's just shown in figure     
40:56     
one so they're talking about bacteria they're talking about this micro Cascade     
41:02     
that comes from back so this is procaryotes there are two Cascades that kind of emerge from bacteria and this is     
41:09     
from the perspective of bacteria there's a micro Cascade which goes from the     
41:15     
structure of the M procaryote cell down to its smallest Parts atoms and then we     
41:22     
have this other Cascade this macro Cascade that goes out towards development of     
41:27     
Evolution and ecology so that's they're kind of using procaryotes as sort of the     
41:34     
focal level of organization for self- reproducing machines     
41:40     
so um so the micro Cascade proceeds from the that the procaryotic cell down to     
41:49     
its tiniest Parts which are these the membrane the molecular machines inside     
41:54     
the membrane ribosomes the cytoskeleton the submachines that make proteins RNA     
42:01     
and DNA the parts that make the submachines amino acids nucleotides and lipids and all the way down to atoms     
42:08     
electrons protons and photons and then of course the macro Cascade in the other direction assembles bacteria into larger     
42:16     
more intricate organizational machines so these are communities of bacteria eukaryotic cells formed by the symbiosis     
42:24     
of ARA and bacteria tissues made of eukaryotic cells organs constructed from     
42:31     
tissues organisms composed of organs populations of organisms     
42:37     
ecosystems and the biosphere     
42:43     
okay so in exploring this Cascade our starting point is the general notion of a machine and the specific questions of     
42:50     
what makes living machines unique the Cascade of machines appears as a solution to the problem of survival or     
42:57     
self- reproduction realized in the world of salty water we then ask how the machine self organized into a Cascade     
43:05     
and elaborate on its scaling especially on what determines its fundamental space and time     
43:11     
scales this is followed by a discussion of salty water world in which the Cascade emerges and evolves we examine     
43:18     
what this physical boundary condition implies realizing in realizing vinan self-reproducing machine particularly     
43:25     
the role of electrodynamics forces we conclude t with tentative thoughts on the Divergence of the macro Cascade so     
43:32     
this is interesting they kind of use self vman self- reproducing machine as a     
43:38     
sort of maybe a physical boundary condition for what self- reproduction entails so     
43:44     
if you're not familiar with vanoyan self- reproducing machine it's this uh     
43:49     
cellular automa tool that vanoyan developed to look at uh different structures and how they can self-     
43:56     
reproduce what kinds of things need to be in place for self- reproduction so this is uh you know kind     
44:02     
of getting into machines in their self- reproduction     
44:09     
um so to follow leadit is inquiry into living machines we must first explain     
44:15     
what machine means broadly speaking a machine is an object that transforms a physical system non-randomly to perform     
44:22     
a definite function of these features of the machine two require first     
44:28     
clarification so the first one is non-randomness so you have to have this     
44:33     
non-random comp it can't just be a random process machines might be very noisy     
44:40     
especially molecular so machines that are living machines tend to be noisy but they also tend to have a high rate of     
44:48     
sort of fidelity so like if you're looking at something like translation in the ribosome that machine is very noisy     
44:56     
but it also has a a pretty good level of fidelity enough to produce um amino     
45:02     
acids and proteins that can uh you know encode things that are needed for this     
45:08     
larger machine which is the cell um yet even the noisiest machine     
45:13     
must have a non-random bias or else it cannot function so there has to be some sort of non-random bias and indeed in     
45:20     
the example of ribosomes we see this non-random bias in the uh in the TRNA     
45:25     
pools that take uh codons from DNA or from the RNA     
45:32     
and translate it into amino acids so this is you know again we have this     
45:37     
non-random bias molecular Motors move radically within a definite range average     
45:44     
Direction ion pumps trans transport charges intermittently but consistently across the same polarity of membrane     
45:50     
potential so there's this sort of uh random walk going on but the random walk     
45:55     
is biased in a way that leads to this sort of outcome and the division Machinery     
46:02     
splits cells and never merges them back we see how machines inject local order into a universe drifting towards Global     
46:09     
disorder and so this is where uh we have to talk about machines being     
46:16     
dissipative in the thermodynamic sense second we need to clarify the     
46:21     
notion of functionality machines transform the system in a predictable manner allowing them to perform tasks by     
46:29     
the scale of the task the scale of the machine so for example nanometric pumps     
46:34     
Cony ions microtubules assemble into a micronized machine that organizes the     
46:40     
chromosomes in the cell and cells themselves from tissues and this applies     
46:45     
up to the scale of the biosphere the existence of a function assuming the purpose or goal these     
46:52     
external human Notions are beside the point all we know is that the machine push the system along specific     
46:59     
non-random paths in SpaceTime and these paths are functional this is an interesting uh thing about purpose and     
47:06     
goal because there there is a sort of debate in     
47:12     
historically about what they call telomic systems where you actually at     
47:18     
some point these kind of systems evolve some sort of gold directedness so in other words you go     
47:25     
from like uh a random walk to what we     
47:30     
might call a ratchet and they're like all sorts of molecular ratchets that you can observe and that's what I was     
47:35     
talking about back here where you have U processes that are noisy but yet go in     
47:43     
One Direction they allow for the sort of fidelity of a process that gives you something at a higher level so like the     
47:50     
example of the ribosomes or some other system that they     
47:56     
have this sort of of this bias and then you end up sometimes with gold directedness so in organisms you have     
48:03     
gold directed Behavior where organisms are moving towards food sources or light     
48:08     
sources and so at different levels you get this increasing go     
48:14     
directedness um from basically random processes and the question is how does     
48:20     
that happen and there's some uh there's a there's a literature on toonomic systems which are these program     
48:27     
that evolve in in these kinds of systems that allower no directedness but that's that's kind of     
48:33     
an aside from this article I just wanted to to mention     
48:38     
that okay so all we know is that this machine pushes a system along specific     
48:44     
non-random paths in SpaceTime and that these paths are functional this is where thinking about     
48:50     
life in the language of machines as leits did lead us away from the realm of pure physics and chemistry where     
48:57     
function is a meaningless word and into the realm of engineering and Technology where function is everything so I think     
49:04     
there's this aspect of like when people think about machines and biology that we have to understand that     
49:12     
we're thinking about machines in an engineering sense and there's certain baggage that comes along with an     
49:17     
engineering approach to function there's also this uh baggage of function as     
49:23     
being this thing that's sort of you know you know that we kind of ascribe characteristics to it so when we use the     
49:30     
term function we kind of think of like maybe human behavior or something that     
49:37     
is you know everything has a role and that's just not the way biology is so we     
49:42     
have to think about like when we say biological machine how does that deviate from like engineering and Technology     
49:49     
versions of function how does that deviate say from like human societal versions of function or human behavior     
49:56     
versions of function so it is it is a kind of a different you know we can use the same     
50:02     
language or that but it's a different set of things that we're kind of interested in so when Mo molecules     
50:08     
Collide and exchange electrons and atoms this chemical reaction serves no particular function by itself yet if     
50:15     
this Collision occurs in a living machine say at the catalytic site of an enzyme then it acquires a function     
50:21     
through its context so again you have these random collisions that can occur     
50:27     
you a chemical reaction it doesn't really do anything but if it occurs in a living machine in a in a system where     
50:34     
there are different types of you where it's heterogeneous where there are different types of processes that are kind of     
50:40     
interlined in the same space then it has some sort of function and so maybe     
50:46     
function isn't the right word maybe we need to rethink the language there um     
50:52     
and maybe it might clarify things a bit so what is the function of life and they     
50:58     
argue that the universal function of life is survival so something has to survive to be functional or something     
51:05     
has to be relevant to survival be functional and that makes some sense because if you think about the example     
51:12     
we just gave um you know you have this process that's nominally random but it     
51:18     
works towards improving sort of the I guess the survival or the fitness of say the cell     
51:26     
if you can trigger a a some sort of process some sort of metabolic process     
51:33     
then you can survive and if process that can be biased towards activating that metabolic process is probably better or     
51:40     
more functional than one that doesn't this Global task unifies all     
51:46     
living machines thus we can propose looking at the living organism as a survival machine that is part of a     
51:52     
well-coordinated double Cascade and this double Cascade is from the figure survival is the physical     
51:59     
notion of continuity in space and time systems with continuous SpaceTime trajectories survive and the living     
52:06     
machine excels at selecting these continuous trajectories as compared to random physical processes other systems     
52:13     
with different organizing principles might have emerged and disappeared but we observe only the ones that persisted     
52:19     
over time those it's that survived so what we see in biology in especially     
52:24     
with respect to these biological maches maches a biological function is everything that survived so we could     
52:30     
have had cells of purely random processes we could build say for example     
52:35     
a protocell with random processes in and that doesn't mean that the protocell can     
52:40     
reproduce and sustain itself it just means that you can put a bunch of chemicals in a bag and have a series of     
52:46     
reactions that aren't linked and you know don't have this sort of Auto     
52:52     
autocatalytic potential which means that is to say that one one process initiates     
52:57     
another process initiates another process um so there's this sort of cycle     
53:03     
that is tightly integrated and contributes to its reproduction and it     
53:09     
survival so we always have this Observer Bias um you know on what we see in in     
53:16     
nature only two extreme Solutions of the problem of survival are known the standard solution is physical robustness     
53:23     
as ancient rocks survive through our geological eons a river flowing along slowly changing paths The Other Extreme     
53:30     
is the infinite Cascade of life turned by self-replicating machines so we have physical robustness     
53:37     
which is um in you know physical systems in Geo morphological systems and the     
53:45     
others are these Cascades of Life driven by self-replicating machines for the living machine survival     
53:51     
is a subtler meaning with a different notion of continuity uh self-replication in     
53:56     
machines must always remain continuously functional implying that all levels of the Cascade of machines must always be     
54:03     
in synchron all fluxes of matter energy and information flowing through the levels in exchange with the environment     
54:09     
must be balanced requiring numerous feedback loops second the machine must     
54:14     
produce new copies of itself and the lineage of machines ensures continuity over long time scales not continuity of     
54:21     
matter but rather continuity of functional organization through the repeated cycle of self production so     
54:29     
this is where we have um this aspect of     
54:34     
continuity as well as function so things must be continuously functional you     
54:40     
can't have a Cas machines that are asynchronous or kind of trigger at     
54:45     
different times things these processes all have to be synchronized and in this sort of cycle that that you know where     
54:52     
everything is tightly ordered in time so all you can't just have random fluxes     
54:58     
you have to have this order and this is something that just you know doesn't happen it emerges over time and     
55:04     
contributes to survival and then of course you need to be able to produce reproduce new copies of     
55:10     
yourself and being able to do this over long time scales so we see reproduction     
55:16     
with DNA and RNA but at the level of say like a cell     
55:21     
and they use eukaryotic cells as an example here this is where you start to get this entire sort of set of reactions     
55:30     
that are linked so you not only get DNA and RNA that are reproduced but the     
55:35     
entire cell itself a cell can divide and differentiate     
55:41     
eventually so it can reproduce itself in a numerous     
55:47     
oils so then they talk about vanman self- reproducing machine um and this kind of is a     
55:55     
computational approach uh which of course v no John V noyman developed U     
56:01     
the concept of the Machining Envision was simple it is a chimer of two conjoined submachines immersed in a sea     
56:07     
of Elementary building blocks the first is a Constructor that collects blocks from the sea and assembles them into an     
56:14     
offspring machine following the directions encoded along the blueprint here the second sub machine is a copier     
56:21     
so you have a Constructor and a copier The copier replicates the tape and passes it onto The Offspring machine     
56:28     
which is ready to produce the Next Generation and there's a loop of doubling reiterates the infinity as long     
56:35     
as it is fed with buildings so this is kind of like something that you very sort of     
56:42     
computational approach to thinking about self- reproduction but it's a way that     
56:47     
you can think about it so this is on the left you have vanon nyman's uh     
56:53     
architecture for a self- reproducing machine you have a Constructor and a copier in the machine and then you have     
57:00     
this tape and you can think of this as maybe like RNA where you can you know     
57:05     
make copies of the RNA or make uh from an RNA to a protein through this     
57:11     
Constructor and cop you're copying information then you're constructing a new sequence of information so you have     
57:19     
the parent which copies the machine The Offspring which is the you know where     
57:24     
the Constructor reproduces the machine so it's it's basically copying and     
57:30     
constructing and those are two different processes they're not the same process     
57:35     
you have to be able to copy or Faithfully copy something you don't want too many mutations and then you have to     
57:41     
construct it so you have to know the order and you have to know sort of the geometry of The     
57:47     
Offspring and so this produces you know some are a reliable reproducible um Next Generation and this     
57:55     
Con menes along the second trajectory that we talked about the mapper     
58:01     
trajectory and then this of course is an example of a self reproducing machine in     
58:06     
a Cell where we have a membrane with DNA in the cells and then RNA and then this is where we have cell     
58:15     
division so we can see this parent Offspring process of the self-     
58:20     
reproducing machine where we have a Constructor and a copier and the     
58:26     
membrane is sort of you know creating two cells all the contents of this cell     
58:32     
are segregating either side of the cell of the original cell and the membrane is     
58:37     
enclosing these into two separate cells at the end of this process so we can map     
58:43     
a vanoyan self- reproducing machine to a bacterial uh cell division a cell     
58:53     
doubling so uh this just kind of goes over more things about complexity and     
58:59     
self- reproduction talking about v nyman's machine and kind of contemplating the role of this sort of     
59:06     
reproduction and uh you know in in biology in biological machines and then     
59:12     
talking about loops and cycles and feedbacks which are of course important for linking together these processes uh     
59:20     
because a process of course an isolation can have internal feedback but when     
59:25     
coupled up other processes can form these self- sustaining reactions and     
59:31     
these self- sustaining machine sets of machines that's say like a     
59:37     
cell and so you get these Cascades of machines and then you get these uh     
59:42     
doubling Cascades and you get further complexity from that um okay so that's     
59:50     
um I don't think there's anything else in the paper I wanted to cover it just really gets into more     
59:56     
things about like information Theory protein machines and then cells life's     
1:00:03     
Quant which is talking about the cell as the fundamental unit of biology and then of course this figure     
1:00:10     
here which is uh sort of the criticality of this so we have uh this example     
1:00:17     
from um biology where we have the size L     
1:00:23     
which is in microns and the doubling time which is in iess seconds uh and then we have the membran     
1:00:31     
cost here the low surface volume reduces flux Mass here so these different     
1:00:36     
aspects of the cellular uh morphology or physiology are     
1:00:42     
part of this curve so you have the membrane cost here where surface volume reduces flux mass and there's an optimal     
1:00:49     
point in this curve which is where you find ribosome Healy production time so the doubling time is 10 the 3 the size     
1:00:57     
in microns is 10 to the zero so this is like a theoretical curve that describes     
1:01:03     
a lot of things going on in the cell so doubling time is a function of cell size     
1:01:08     
at the critical size L subc is around one micron the microbial cell achieves     
1:01:14     
the fastest double in which is the lower balance set by the sulur production time of the r cell so the microbial cell is     
1:01:21     
sort of this lower bound of what you know I guess theoretically the lower     
1:01:28     
bound which you can have for these self reproducing machines larger cells where     
1:01:33     
L is larger than LC C has a lower surface volume ratio and therefore     
1:01:39     
sulfur produce more slowly in smaller cells where L subc is larger than it the     
1:01:45     
cost of producing the membrane with around 10 microns or changing width of around 10 micr or natur     
1:01:52     
nanometers uh slows down the doubling as this relation holds so this is kind of     
1:01:59     
thinking about this this is L subc at this optimal point this is T subc here     
1:02:05     
for the doubling time so the size and the doubling time are optimized in the     
1:02:11     
um in the procaryotic cell and then they talk about feeding     
1:02:17     
the Cascades and this kind of talks about some of these processes and the optimal energy flux needed and then     
1:02:24     
finally talking about salty water and sort of where this the boundaries of this Cascade are so if you think about a     
1:02:31     
procaryotic cell in salty water that's the the the sort of the boundary     
1:02:37     
condition for this these linked Cascades so at the beginning of Life we had these     
1:02:42     
Cascades we had iron world we had these Cascades that were enclosed by a membrane the membrane enclosed these     
1:02:49     
processes and salty water was outside and so this is something that     
1:02:55     
provides Ed energy and nutrition to this emerging cell so they talk about that     
1:03:01     
here um so that's I think all for that paper um that's something that I wanted     
1:03:07     
to go over I thought it was a fascinating paper and it has a lot of uh relevance to early life so we've been     
1:03:13     
you know doing some early life research in the group um it also has relevance to biophysics and bio complexity and it     
1:03:20     
really you know helps you think I think about this idea of systemic closure and how you go from processes that are sort     
1:03:29     
of out in you know that are sort of independent to processes that are linked in a living system and that are self-     
1:03:36     
sustaining and self- reproducing and then thinking about the computational analogy to that which is     
1:03:42     
the Von noyman self-replicating machine which is in this cellular automat bace     
1:03:48     
but then also describes this process it abstracts this process and serves a sort of almost like an embedding for early     
1:04:00     
okay do we have any comments or questions about that really interesting I I haven't uh I'm on my school run here     
1:04:09     
so I I can't do it justice but um yeah was really interested to to dig into     
1:04:15     
that and uh seemed like a a really interesting     
1:04:21     
paper oh I I put a comment in the chat     
1:04:27     
okay um so Susan says dick found an interesting paper went signaling     
1:04:32     
modulates mechano transduction in the epidermis to drive their follicle regeneration okay that's     
1:04:40     
that's something we should check out well it fits in with what I was     
1:04:45     
doing with epidermis oh right yeah and the elasticity     
1:04:51     
so apparently hair grows at 5 to 15     
1:04:56     
kilop pascals yeah looks interesting so uh wind signaling modulates the process or     
1:05:05     
the elasticity or um yeah it's it it's hard to say it's     
1:05:12     
one of those chicken and egg things is it wi that's causing the um adherence     
1:05:21     
Junctions to form so then there's more pull on the cell like it     
1:05:28     
cramps or is it is it um these differentiation waves like is it is it     
1:05:36     
um cell cramps or     
1:05:42     
um I don't know the the um adherence Junctions connected to the cells     
1:05:52     
um um actomyosin ring on the top because this happens in     
1:05:58     
epithelial cells so this is an epithelial cell thing and so is is that     
1:06:05     
what what causes wit to be produced or if you have     
1:06:10     
wi then you get more adherence Junctions and therefore you get more cramping I     
1:06:16     
don't know it's all like a chicken and egg thing like kind of a circular process yeah     
1:06:25     
yeah and uh I guess Dick's comment here is it says wind     
1:06:33     
signaling um mechano regulatory role     
1:06:38     
that manipulates the Machinery of mechano transduction to drive     
1:06:45     
regeneration or vice versa so it might     
1:06:52     
be like they they're saying wit is the key but it might be that the mechano     
1:07:00     
transduction produces wit which produces more     
1:07:06     
connections and that's true if you if you have more connections if I mean if     
1:07:11     
you have more Force you get more Connections In A Cell it automatically produces them I don't     
1:07:20     
know I was just sort of being a biologist in a     
1:07:26     
and reading about observation and sticking them in my paper and saying see this happens yeah um yeah it says um     
1:07:36     
under intense mechanical load such as those seen during bone     
1:07:42     
formation and tumorogenesis beta cinin translocates to the nucleus in a force     
1:07:50     
dependent manner so it's it's a changing     
1:07:56     
um the production of something using our DNA um and yeah that also could be     
1:08:04     
during a cramp like um if this if the     
1:08:10     
cells cramp or the tissue that has it under goes a cramp um then that that's     
1:08:17     
an intense mechanical load automatically yeah so that fits with     
1:08:24     
that so that's why we're saying differentiation waves because they're cell cramps     
1:08:32     
right anyway it was it's a it's an interesting paper yeah it sounds like it     
1:08:38     
now maybe we'll come back to that next week but thanks for yeah thanks for over     
1:08:46     
the that one might it might be worth money if you can figure it out because everybody wants to grow hair over there     
1:08:53     
and not over here right everybody's fussy about their hair and     
1:08:59     
it's just yeah so you just like crimp your skin with clampers and you like you     
1:09:05     
just yeah crimping your skin and you'll get hair yeah my suggestion is add more     
1:09:12     
elastin to your aging scalp yeah mine too like r     
1:09:25     
that's uh I guess um what I I've been looking at as as an aside here I'm     
1:09:34     
supposed to be like to say writing writing a paper     
1:09:39     
yeah all right well that sounds great um thank you everyone for attending um     
1:09:46     
that's all for today uh see you next week okay bye thanks bye
