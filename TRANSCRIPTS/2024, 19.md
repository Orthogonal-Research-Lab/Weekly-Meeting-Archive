     
Transcript     
0:02     
hello Mah uh hi Bry all     
0:09     
right so yeah uh so how how have you     
0:14     
been yeah I'm better I'm actually very good okay     
0:20     
good uh but is so you hav oh go ahead uh actually just one small issue     
0:27     
so I have been working on the data processing part I'm actually little lagging but I'm doing a lot lot of     
0:32     
readings so for from like Sunday yesterday uh my Ser my lab server is not     
0:37     
working and and my laptop is little not very good for computation so I use it on     
0:42     
my server so I don't have any code to show but I have some readings which I     
0:48     
want to convey few doubts okay yeah go ahead     
0:53     
yeah what are the     
1:03     
yeah sure hi hi everyone hello     
1:10     
hi I'm in Regina Saskatchewan okay     
1:18     
yeah so so did you want to share your screen Mah or     
1:25     
yeah I'll um I'll turn down my band     
1:31     
okay and it's on my other computer so I'm in I8 for some reason great we can     
1:37     
change that on the     
1:44     
Fly uh I hope my screen is visible yes okay great so I'll just introduce my     
1:52     
project uh for those who are not aware of it because I haven't done it formally in the meeting ever so uh what I'm what     
2:00     
planning to do in this summmer Google summer of code is uh develop developing     
2:06     
develop a modeling for cigance which I have named as hyper devra uh to try to     
2:12     
capture the development of C Elegance in terms of hypergraph convolution so my idea is based on actually Bradley's     
2:19     
paper so this is the paper which which title is hypergraphs demonstrate uh ants     
2:26     
during divers integration so D integration is what what entry so basically what I plan to do during this     
2:33     
Summers is uh develop uh two hypergraphs based on the based on the C Elegance     
2:39     
data so the first is uh line lineage hypergraph and the second is spatial so     
2:44     
spatial as the name says it it actually incorporates the spatial aspect of the     
2:49     
graph how it grows and for example a certain cell will have other cells which are closed by so we will try to uh make     
2:57     
connections between them and lineage hypergraph is something like this this picture actually describes it very well     
3:04     
how the cells divides uh so the other goal is to uh Inc incorporate this sort     
3:11     
of tree into a uh through a lineage hypergraph so that is the plan and this     
3:19     
is the paper which is in the reference and then so for the first week what I     
3:26     
have done is I have done the analysis part on this dat so I was going through what previously     
3:33     
uh participants have done in during G so I found Jang Le's work very much I would     
3:40     
like to on which I would like to build on what he did was he did uh so his work     
3:46     
his work's goal was mainly centered towards cell uh cell tracking in gns which was if I'm not wrong uh trying to     
3:54     
emulate this paper graph neural network for S tracking and microscopic videos so     
3:59     
what he did was uh yeah so they had their code available but Jang tried to     
4:06     
uh use the data uh which is this raw data which has these columns so first is     
4:12     
cell time XYZ coordinates 3D and and size of the     
4:17     
coordinates uh so he actually using KNN based methods he constructed a temporal     
4:23     
graph so temporal as in uh since we have time aspect also he made a temporal     
4:28     
graph and which is actually Direct D in nature so why directed so what he tried was he tried to build these kind of     
4:36     
relation incorporate these kind of relationships also so daughter and cell will have like a direction because uh     
4:42     
yeah because they daughter and cell both are not equal so that's why a directed graph because it it will have a directed     
4:48     
directed Edge so I I looked in how he has made temporal graphs he has used the     
4:54     
library called DGL which I will talk later and then this was the first uh uh     
4:59     
his contribution first part which is there in this file data sets.i I looked into it and then in second he actually     
5:07     
what he mentioned was he has restructured the code basically the graph he constructed for the cell     
5:14     
tracking GNN so he reconstructed this in the data sets 1. py and in the third     
5:21     
part which he contributed he made a model called C.P so in this directory     
5:27     
daph models there were two models one was a simple G model I think he made it for basic testing and but what he said     
5:35     
in contribution was there's another file called c. py in which he has emulated uh     
5:40     
the code from uh basically recreated reimplemented the code from self tring GN so I am yet to look at this part part     
5:47     
of the code but this is for a long-term plan so I can get ideas so yeah so this     
5:52     
is his work but what I so what I plan to do is I also plan to do a similar thing     
5:58     
make the graph but it is actually it's actually a hypergraph so those who might     
6:04     
not be aware of hyper I'll just show image it will actually     
6:10     
clear some doubts about it yeah so I have made yeah so this is     
6:17     
some code I was trying to do so what you would generally see so this is basically a hypergraph image so what generally you     
6:24     
would see is in a normal graph actually a better yeah so let's say this is a     
6:30     
normal graph right so here it has nodes and edges and by the way edges are directed here by this Arrow you can see     
6:38     
so this is a normal graph uh in which we have nodes and edges and you can see     
6:43     
that edges have like two connections it's like di connection two nodes are conect to a single edge but in hyper     
6:49     
graph we can have multiple nodes in a single hyper Edge we call it hyper as in     
6:55     
call hyper plane for a simple plane right it's culating its     
7:00     
Dimension so that is what I plan to do uh using using this hypergraph like specture I plan to create a spatial and     
7:08     
as I mentioned and lineage as well so and and then I plan to find some uh     
7:15     
Downstream task which I'm not really sure of right now so that's why I'll read this paper uh which Jang used so I     
7:21     
can get more ideas later on but yeah one for for like next four or five weeks I have this plan to emulate uh put this     
7:29     
into a hyper now coming to the next part so this is what I have done I don't have     
7:35     
any code to show right now I had done the pre-pressing but because of like VPN issues uh I'm not able to access in my     
7:42     
labs uh lab server from yesterday VPN is not working from like college weekend     
7:47     
but I'm going to college in next two days so I'll be able to directly use uh server so no will be required so will be     
7:55     
more helpful yeah so coming next actually I would need inputs also in this particular case I plan to use this     
8:01     
particular data set only in which there are like I think 25,000 yeah so around 20 25,000 total     
8:09     
number of cells and it also has like division information also because we can     
8:15     
decode which cells are being divided as far as I understand correct so how cells are     
8:21     
divided so we have this special information from this data this is one     
8:26     
aspect second uh information we have is time so right now I'm not sure how to encode time I have put in my last week     
8:34     
but I will figure out out by that time how to encode Time by reading some other papers so these two and then third is uh     
8:40     
this uh Dimension information like where it's located and then fourth is size so     
8:46     
this can be actually interesting attribute to observe in uh inance maybe I will have to read more     
8:53     
literature how it can be useful and also like probably other attributes which I'll find through the data sets which     
8:59     
I'll need some inputs L on but coming to next part uh yeah so what my plan for this     
9:08     
coming week is so first I'll just uh uh I'll try to emulate     
9:16     
first yeah so first plan is uh like to to do the complete pre-processing part     
9:22     
and put the code on like uh GitHub uh which I was doing playing with the data     
9:28     
trying to understand everything and second is I want to figure out how do I want to encode this graph hyper graph     
9:33     
like for spatial and and for lineage so first I want to Target lineage now so I     
9:39     
was actually going through which Library should I use so I'll so what Jiang has used is Jiang has used     
9:48     
something called DGL Library so as previously I mentioned it is called Deep graph Library which is based on P torch     
9:55     
only so it's actually very good and it has been ging a lot of traction recently so the benefit of this is first I'll     
10:01     
explain the benefit why I like DGL so first Jiang first reason is Jiang has     
10:07     
already used it and second is uh so like when we create a graph neural network or     
10:13     
a simple graph uh so we have like information passing right from one node     
10:19     
to another so that can that can visualize in two steps so one is first is message passing view so in this what     
10:26     
we see is in the first in the center node right so Center node is getting information from all the surrounding     
10:33     
nodes so this is one point of view and in the second point of view is like     
10:38     
little mathematical so what happens is we try to decode this graph which is like a b CDE e no just we try to decode     
10:47     
this edges into this uh adjacency Matrix one means there's an edge and zero means     
10:52     
there's no Edge so it can be represented mathematically like this and there there are going to be normalization also in     
10:59     
this function so but yeah so based on this uh initially I actually was     
11:04     
thinking of using python geometric so Pyon geometric only offers coding in this message passing view but in this     
11:10     
particular djl Library it offers to code in these two different views one is message passing View and they have given     
11:16     
the code also so this is what actually P uses py geometric uses normally and     
11:22     
there is this another view of coding which is this uh Matrix view uh so     
11:27     
Matrix view as in so we have this equation right d d to^ minus one y 2 and 8 and then     
11:35     
again d^ 1 - 2 X and W so this is some like matrices actually     
11:40     
so so you will see that this is the inverse and then     
11:47     
uh okay I think it is OPP yeah so yeah what it is trying is like it is coding     
11:54     
it in different views and different ways and personally like uh so since 5G uses only this method to code uh and DGL     
12:02     
offers these two kinds of views and this second view Matrix view is actually little simpler to code like at least to     
12:07     
imagine so I feel d d even though it's a new library it's very interesting that     
12:13     
it supports both the views of coding so yeah and also another benefit of pyo     
12:19     
geometric is it offers a hypergraph neural network uh code also like there     
12:24     
is one I think one model and also good support for uh coding in     
12:30     
hypergraphs and which is still not very good in py Geometry uh I'll come to it     
12:36     
next geometri so this is the benefit of this particular Library so there are two more libraries which I tell and I will     
12:43     
ask for any review so and yeah and also I was trying to encode this data so when     
12:48     
I try to encode this hypergraph uh this is how it will look like so I played it     
12:54     
with a with a data set called ka ka is a very data set so I can get Ed with this     
12:59     
particular library and I did this in collab because I didn't had proper server to work     
13:07     
with yeah so this site data set and yeah so I just did     
13:13     
normal training not not on C just so I can get hang off this library and then     
13:19     
yeah so this is one Library which I want to try with and second is f out getic so it only has this hypergraph con which is     
13:25     
coming from this particular paper hypergraph hypergraph attention I come to this St so what what this 5G offers     
13:32     
is I'm actually first of all very comfortable in this in 5G I've used 5G a lot like for like last one year I'm     
13:38     
using it so one positive point about this is this and but second negative     
13:44     
point is uh it only offers this hypergraph Con uh GNN like uh model     
13:51     
other than that it doesn't support hypergraphs very well recently there was there's a PR but it's still not merged     
13:58     
in like May so I can probably use that particular code and try to do but still     
14:03     
it doesn't support hyper very well but I can do it but it will take some extra not required so yeah so now coming     
14:11     
to this paper hypergraph convolution and hypergraph attention I read this paper it was very interesting right so this is     
14:18     
a 2020 paper from University of Oxford uh so what it offers is uh what it tells     
14:25     
us I'll just symmetric part so it says we can WR a graph NE hyper graph NE Network in two ways so one is symmetric     
14:33     
symmetric normalization so you see that this is the mathematical equation of this in which we have D inverse and D     
14:39     
inverse so when we multiply D inverse together it will be D power minus one half will get added but they have kept     
14:47     
it as two different terms you will see this's reason for that and then second     
14:52     
is non symmetric or you can also call row normalization it's also called     
14:57     
asymmetric so it has a little different equation so here you see that there is single D and it has a power of one so I     
15:05     
actually read it I read it up a little bit uh so what I have written it down in     
15:11     
my notes so it has its own benefits so first let's go to symmetric     
15:16     
normalization benefits it says that uh uh like weights so yeah so what it says     
15:24     
is uh the ization is useful in which the     
15:30     
graph is undirected so for example basically it considers both input and output dimensions of a edge of     
15:37     
a Vertex equally on the other hand asymmetric normalization is actually very useful in which we have like     
15:43     
directed graph so and that is because uh it has some uh directional influence     
15:50     
also so this is what I wrote so like it can be very useful to create information for models in linage     
15:56     
graphs uh yeah in lineage hyper graph basically where directionality is an     
16:03     
influence from like mother to daughter so I thought this particular style of normalization and coding this can be     
16:10     
useful in like coding writing lineage hyper graph and     
16:15     
yeah so this is about this paper uh and when I was seeing this hypergraph pa uh     
16:21     
they actually implement it in asymmetric fashion only which which we want so but     
16:27     
this particular model is not available in uh this DGL Library it is available     
16:33     
in P so P has little one benefit in this particular part maybe if needed we will     
16:40     
try to code but otherwise I think we should directly P for this particular situation so yeah these are two     
16:46     
libraries just one small Library another thing so there is another Library called this DGL so it is actually written by     
16:54     
the contributors of a lab called imoon from China I don't the wi University so     
17:00     
it it this library is only focused on hypergraphs so it offers very interesting things so it offers like it     
17:07     
Bridges the gap of graphs and Hyper graphs very well I was seeing it schol and it has some nice visualizations for     
17:12     
hyper graphs also so which is lack which is lacking in this DGL and PG both I     
17:19     
didn't see any visualization hypergraph so yeah visualizations like     
17:24     
this so it it actually supports graph hyper graph hyper graph has at least good visualization then direct hyper     
17:31     
graph also also and by which isi for us     
17:36     
so but yeah it's it's a fairly new library and it only has like a limited contributors it's not very well accepted     
17:43     
till yet so maybe if I if I get into an error it will harder for me to debug     
17:50     
right little like cor of uh yeah mostly this only I just     
17:57     
needed some feedback on like a few things so now coming to like feedback which I wanted so like the data set     
18:05     
which I showed which Jang has used so is that data set enough for me and like uh     
18:12     
connected to this question so it it only had attribute of size of the cell right like it only had like extra attribute     
18:19     
size and hery and one more thing     
18:29     
yeah so timei size and dimens and where it is     
18:35     
located so do you think any other uh uh any other attribute can be useful which     
18:41     
I can get from any other data set these two questions     
18:47     
right okay yeah let me answer those um so with respect to number one I think     
18:54     
the data set is good for right now uh you know I would try I think that's     
19:01     
probably you know we can get things from microscopy data we can segment those     
19:07     
images if we need to but I think for right now this is a good set of data to     
19:13     
just kind of work things out with to see how they uh you know can be you know I     
19:20     
think just getting the lineage tree structure down is good and then of course with the XYZ coordinates uh     
19:27     
you'll have spatial information as well so you know what you'll do and I'll show you in a minute when you're done taking     
19:33     
notes what each variable means but this is good so this is the yeah this is the     
19:39     
data set so it's cell time XYZ and size     
19:44     
and this is in the CSV format so you can't see the sort of the rows or the     
19:51     
columns um so cell is so the the if on the very     
19:57     
left actually and it's not labeled but you see these numbers going from zero     
20:02     
down you know sequentially down to 40 those are what you might call um time     
20:11     
like sort of uh stages of the lineage tree so if     
20:17     
you go up to the time you'll notice that the time it's lagged by one so for time     
20:24     
one there's a zero and that zero is like kind of the stage of the lineage tree so     
20:31     
basically what it's counting are these division events or things like that um     
20:37     
so I would I don't I wouldn't worry about the First Column I'd use the time     
20:42     
in the time column which is column three so that's one two 3 four five the time     
20:48     
for each for these observations and this is from a specific data set um are the     
20:55     
observations that they made so what they did was they filmed a movie of embryogenesis they started with uh     
21:03     
sort of the division event between the fertilized egg and then the two cells     
21:09     
that result from that and so right here you see that one of them goes as ab and     
21:15     
then the other one is P1 which isn't on this list right now it's way down somewhere so you you have this time     
21:23     
point one it starts with sort of the existence of ab and P1 in that frame and     
21:29     
then each one of those time steps is a certain frame and time that was taken of     
21:35     
that embryo so you you go down I think like the time goes down quite a ways and     
21:41     
it's just distinct you know distinct events that's that are being observed yeah you'll see that it goes down     
21:48     
to uh and so in each one of those time in each one of those time Horizons you     
21:55     
have a certain number of cells that exist in the time Horizon and so that means that you're going to     
22:00     
have cells single cells like this cell here that you see that you have highlighted it's going to exist at     
22:07     
multiple time Horizons and that's because the cell persists for a while     
22:13     
and then the cell divides and then you have another cell that persists for or its daughter cells persist for a while     
22:19     
and they divide so you can actually get a sense of like um it's not in minutes     
22:27     
but it's in these frames frames or these time Horizons now there is data and I can     
22:35     
give I can give you a link to the data set but there there's an interactive uh     
22:40     
lineage tree that that's online and that interactive lineage tree has the time     
22:46     
and minutes for which the cell divides so you can map this time variable to     
22:52     
minutes you just have to find the division points and calibrate it accordingly and that might be a little     
22:58     
bit better for like you know uh making some biologically meaningful statement     
23:03     
so I would work with this this time variable right now and then we can put     
23:09     
the minutes on later oh okay sure yeah so we can start     
23:15     
with this one yeah bradle one question in this I so you mentioned that this     
23:20     
let's say this particular cell will only exist for time very specific moment right so at let's say 92 unit of time     
23:29     
yeah so this cell a a l a a a p it exists from 92 down to I guess past 107     
23:39     
yeah to 120 and so that means that from time     
23:44     
Horizon 92 to time Horizon 120 it exists as a single cell and then you'll see     
23:51     
that it it divides it divides into Al aaap PA a that's one of the daughters     
23:57     
and that persists from for a while so you have like you know in each time in each time Horizon you have a certain     
24:04     
number of cells and you'll find if you go through the data set and you do a search like all cells for this time     
24:11     
period or this you know number point in time you have like certain cells that     
24:17     
come up and you can actually just take that as like a snapshot uh at one point in time so you know there are things you     
24:25     
can do to process the data you can uh you know you can collapse the cells down     
24:30     
to sort of their interval you can do all sorts of things um but basically the     
24:36     
structure of the Lage tree is where you have a cell that divides into two     
24:41     
daughter cells and then that cell each one of those dotters divides into two cells and so on and so forth sometimes     
24:48     
you get asymmetric divisions where only one daughter per persists in the lineage tree sometimes they differentiate and     
24:56     
then in this data set it means that that cell terminates so we can go over the Lage     
25:02     
tree maybe next week too and get us or at least the interactive lineage tree     
25:07     
and get a sense of what this looks like because I don't have it right now where I can go over it but um sure so go back     
25:15     
up to the top of this file okay brly one small thing in here     
25:21     
so I also notice that size is also changing for every cell with time yeah     
25:26     
yeah we're going to get to that okay okay go up to the top again um yeah so we have cell and the     
25:33     
cell ID is a nomenclature that they use and that's not important right now but     
25:39     
what's important to know is that that's going to be that's going to identify the cell and it's going to be deterministic     
25:46     
so AB will always result in ab and AP or ABP and ab a uh it's always the same     
25:55     
thing in any cgans we observe and you know that so that makes your job a lot easier that's why we're working on cigan     
26:02     
lineage tree um and so the the nomeclature is you know available you     
26:08     
know the descriptions so that keep that in mind so we have cell time and then     
26:14     
XYZ are coordinates that are collected by like when they do the cell tracking     
26:20     
when they capture the data it's basically a coordinate in that space so     
26:26     
the coordinate system is relative to the uh the microscopy image so you can     
26:34     
actually normalize this by just kind of you know finding the center point or the     
26:41     
uh you know the median point of the image and then you know plotting out the     
26:47     
image the uh coordinate system as like a left right forward backward thing from a     
26:54     
center point so I we actually have a method to do that we have some data that     
27:00     
shows these uh coordinates and so then know coordinate system basically is um     
27:07     
you know what what'll happen in these images and you're going to notice this with size too is that it's variable so     
27:14     
it it kind of looks like it moves around the coordinate system which is you know     
27:19     
kind of a problem the cells are always kind of Shifting around and migrating     
27:24     
from image to image so what you might typically do for a single cell you might     
27:30     
take a mean position like for X Y and Z for ab you might take all those four     
27:36     
observations that you have here and find the mean value and use that as the position so we're just basically trying     
27:43     
to approximate the position we have to normalize it to like a center point and     
27:48     
then we can you know uh use that as a way to to characterize the position and     
27:55     
it you know it helps because you have that spatial information so you can use this for the spatial graph as well as     
28:01     
the lineage graph um and then we get to size which is that last     
28:07     
variable go ahead yeah I got a question uh if you     
28:13     
take the uh those are different times you take the     
28:18     
times and make a little pathway in space yeah narrow on     
28:24     
it uh do you see any coordinated motion between well uh yes you do it's it's not it's     
28:32     
noisy and we have to like uh work on that I think this is something that I think gahong did or it was either     
28:39     
jiahong or um uh hansu where we had like     
28:45     
um basically do this sort of cell tracking where you take the position and you just plot it out so it kind of like     
28:52     
is this zigzaggy line that occurs over time of the middle of the nucleus     
28:58     
yeah so that's that's what it's measuring um it's kind of like the middle or the nucleus well we'll call it     
29:05     
a centroid I think we're recalling that one of the papers um so it's the centroid of the     
29:11     
cell um and then it because we're using XYZ coordinates they're actually what     
29:17     
I'm saying is that if you made each one into a set of vectors yeah in space yeah     
29:24     
and you animated them uh would you see coordinate would you see flow     
29:29     
patterns uh you may we I we did this like for the cell tracking     
29:35     
information uh and it didn't I mean it kind of maybe looked like a flow you could probably do the analysis I'll have     
29:42     
to go back and see if I can find the data because I know that it could have actually been San that did this because     
29:49     
he was working with the data and trying to extract some of the cell tracking so we have these trajectories that are kind     
29:55     
of like this and you might actually get some flow pth patterns and you could analyze it with some other technique I     
30:01     
know people use l Grans for that a lot and things like that and you can actually animate like um you know find     
30:09     
some pattern in it um because if you just look at it by eye it's not going to give you anything     
30:16     
yeah but yeah they may because the cells are migrating down and like I said you     
30:22     
know at each time point the cell is at a different position and a lot of that is noisy so you if you filter the noise out     
30:30     
you know we might actually be able to get some sort of interesting flow     
30:36     
pattern this is for particular NE right this is for     
30:42     
seans particular individual right uh no actually they uh I think they've they've     
30:47     
collected data for a number of nematodes but the process is so uniform that it's     
30:53     
it it doesn't really matter too much I mean they're all wild type so they're not mutant phenotypes they basically all     
31:01     
uh unfold the same way in the same other coordinates would be very similar from     
31:06     
one individual to another yeah yeah there okay and it yeah the I mean it's     
31:13     
it's it's going to be you know there's going to be like a mean value anyways because there's a lot of technical     
31:19     
variation and as well as by some biological variation but not very much     
31:25     
it's going to be pretty consistent so the the final word on size is that     
31:32     
they take basically we decided that it was like the area around the nucleus so     
31:39     
it's like you have this what they call a fluorescent signal and they measure the area of that fluorescent signal and that     
31:46     
is around in the nucleus but around the nucleus and so it's not the size of the     
31:51     
entire cell because it's hard to estimate the size of the cell but they do vary by this I guess by the size of     
31:58     
the cell and so you're getting that size difference size variation and so that's what size is and     
32:07     
it's useful size information is useful for a number of things um you can look     
32:13     
at differences between daughter cells you can look at their size over like     
32:19     
time these different time points you but most importantly you want to look at the     
32:24     
size across different cells like what's the mean size between ab and ab AA and     
32:31     
that sort of thing and they get like the they measure this as sort of I guess a uniform size so when they first divide     
32:39     
they're like half the size and then they expand out so that's about the same size for every cell but anyways it's it's uh     
32:47     
yeah so that might be useful but that's that's what we have and then like I said we have other types of things that we     
32:53     
can look towards we have does size reflect asymmetric     
32:59     
divisions um it may um I don't remember we did some work on that in our 2016     
33:06     
paper on uh you know where we did this and the     
33:12     
ciona and test analice and I I know there are some asymmetric uh size     
33:18     
differentials uh it's not the best data set to find that sort of stuff but I think we did find some um like     
33:26     
asymmetrical relationships and some symmetrical relationships but I have to go back to     
33:33     
that paper     
33:39     
Okay okay so I think that's good that's a good data set and again we have other     
33:45     
data sets we can use I would just work with this first and see how far you can get on it if you can like you know     
33:51     
benchmark or you can at least you know get some results where you say okay now I have this graph I can generate or I     
33:58     
have this um you know I can work within the bounds of this     
34:06     
toolbox sure okay uh so uh you have mentioned hierarchy of division right so     
34:12     
where can I get that list in which the rules of hery are there uh so yeah the     
34:18     
the well there is a published lineage tree and uh I can send you the link to     
34:24     
the interactive lineage tree uh there's also so you know this is all published stuff this is the basically the     
34:31     
hierarchies if you look at the nomenclature you see AB it's like this what they call the two cell stage and ab     
34:39     
uh divides into ABA and ABP and and so there are these divisions from the two cell stage where they're dividing     
34:47     
anterior posterior which means to the front and to the back and then left and right which is at the left and to the     
34:54     
right and so you'll get these cells that are dividing from that two cell stage     
34:59     
and they're dividing out like anterior posterior left right and then there're     
35:04     
also some other founder cells in there where they switch the nomenclature in AB it doesn't do that but in other     
35:10     
sublineages it does and so you you know the rules are basically that you start     
35:15     
with two cells then you go to four cells and then you go to and it keeps uh doubling until you get down to cells     
35:23     
that divide like one daughter is alive and one daughter dies or the cells differentiate or something     
35:30     
like that and so that's the basically a binary tree that has you know     
35:37     
um divides at some time and then you know yeah that that's actually the that     
35:45     
image is from that interactive tree so yeah you start with P seen     
35:50     
that and so yeah okay that's that's basically the     
35:56     
structure so it's binary yeah binary uh structure computationally it's a     
36:02     
binary structure that divides it a certain it's variable rate because it's     
36:08     
not always the same rate okay sure sure thanks uh thanks for the input and last     
36:15     
uh thing so I wanted to ask like it's going to be like I think for decision to     
36:21     
be taken by me only but how should I go forward like which Library 5G or DGL or the other one so what I thought was I     
36:27     
will create uh the Jupiter notebook for like for like for other people in both     
36:33     
the libraries so it's like easier for later on people but for starting I thought which one should I start or     
36:39     
should I start with both any points well yeah I might try both of them just to     
36:45     
see what they what they can do you I think you did a very good job of recapping kind of what they are but you     
36:52     
know you you got to work with them a little bit on the same data set to see okay this is what I can generate uh with     
36:58     
this this toolbox versus the other one um is it you know and think about like     
37:04     
you know how how easy would it be to plug in data uh maybe not in this data in the     
37:10     
case of this data set but but what are we doing because it looks like you mentioned there was a difference between     
37:16     
pigl and DGL in terms of uh I think it was dpiper graph that was where you     
37:22     
could use one but not the other yeah yeah so is easier than 5G     
37:29     
right yeah yeah so I mean you know that's that just I would maybe you know kind of evaluate both of them and maybe     
37:36     
create a table where you kind of give the pros and cons of each and then you     
37:42     
know some like maybe like next week you decide I'm GNA use this one or I might     
37:47     
use I might try both and I might just like use one in a limited fashion but go to the other one you know that's the     
37:55     
thing about tools like you don't know how well they're supported you know maintained but sometimes they have     
38:03     
benefits that are very clear sometimes you can do one thing in one toolbox and another thing in another toolbox and     
38:09     
that's okay because you know in Python we can just import stuff depending on what we want to do yeah so it's not not     
38:17     
impossible but yeah just make sure that you know kind of what each toolbox can     
38:22     
do and make kind of an assessment of you know is this something that's going to be around for a while because sometimes     
38:29     
they're they're not and that's also another problem you know if you're creating a project now what happens in     
38:37     
two years will still be able to run so that's that's an important thing right     
38:42     
yeah yeah that's great Point actually yeah     
38:47     
yeah okay I just any other points any other thing which you think can be     
38:54     
useful no I think that no I think it was a very good overview of the different toolboxes I wasn't aware of all the     
39:00     
things and I think it's another important point you made was this uh difference between doing with graphs as     
39:06     
sort of this Matrix and dealing with as message passing so in graph neural networks they use message passing a lot     
39:13     
which is where you you know in in The Matrix view you're interested in those     
39:18     
relationships between nodes and in this case hyper nodes but in message passing     
39:24     
you're interested in sort of the evolution of the network like what is being passed between the     
39:30     
nodes so like you know in this case what is being passed between the nodes that's an interesting question I mean I guess     
39:37     
DNA is being passed between the nodes they proteins being passed and when we say nodes we mean what's being passed     
39:43     
down from mother cells to daughter cells and so we have DNA we have some proteins     
39:50     
there's stuff in the nucleus that gets passed down so we can actually use message passing sort of as a second     
39:57     
layer to the connectivity Matrix so if I said like AB was connected to Aba and     
40:05     
ABP those are daughter cells um that's that's like a matrix operation you just     
40:11     
say it's either zero or one right but then you have message passing so what is     
40:16     
the message between those two and we might be interested in like a certain set of genes or a certain protein or     
40:22     
whatever and that would be like something we could say that message is passed down the lineage tree     
40:27     
down to other cells and so that might be interesting to have as like to take both     
40:33     
approaches to say nominally it's a matrix but there's message passing how     
40:39     
do we you know how do we represent that Bradley a question uh most     
40:45     
organisms have a temperature range of which the embryos develop yeah okay uh     
40:52     
has anyone taken uh neod to one or Other     
40:57     
Extreme high or low extreme to see how the uh embryogenesis falls     
41:04     
apart uh which of these parameters might change yeah I know that like when um     
41:11     
they want to get males and seans there's this protocol for re rearing the eggs at     
41:16     
like an elevated temperature and so it like creates more males somehow so like     
41:21     
they they have experiments like that there are other mutants and seans that you have to raise at like     
41:27     
15 degrees instead of 25 degrees or they're tolerant to nobody has tracked     
41:33     
the cells at the edges of the temperature AG I don't think they have     
41:38     
yeah I think it's like people generally just use the 25c and I think they view     
41:44     
like temperature deviations as bad um I know that but there are very interesting     
41:50     
things going on those temperature ranges I know for like dropa I don't think people have tracked it though with     
41:56     
respect to that like I know you're kind of what you're saying is that like if you raise the temperature what would the     
42:03     
embryogenesis look like could you track the cells are why fall apart at the at     
42:09     
the limits yeah yeah I don't think people have done that but there may be a     
42:14     
data set out there but I'm I'm not familiar with it is there anyone you can encourage to     
42:19     
do it well you know     
42:25     
um um that happen in Axel as well like could     
42:31     
you uh check to see if the I don't know the cell divisions change ah there we go     
42:39     
we have somebody Toge to do it yeah there I have Axel so yes so if you if     
42:46     
you looked at the temperature extrems you might be able to find how the embryogenesis falls apart and why yeah     
42:52     
there you go an experiment I could do at home yeah well if you can get     
42:58     
temperature control yeah yeah like last week we talked about heat shock protein     
43:03     
which is hsp90 and one of the sort of the Hallmark experiments there was to raise     
43:10     
uh I think it was drop at an elevated temperature and then there's this heat shock response that's sort of this     
43:17     
robust response to some stress there's so there are a whole set of experiments there but I don't think people have     
43:23     
really done anything with cell tracking with respect to that     
43:29     
yeah okay well these curves are rather Universal     
43:34     
yeah yeah uh it's almost a a simple box     
43:41     
yeah so so there may be some Universal thing that's happening at the edges of     
43:46     
the Box yeah yeah that'd be great to look     
43:55     
at hope you don't mind me stopping stepping in Hi how are you how's it     
44:00     
going I'd be happy to give an update on the work on the breast cancer par the CT yeah sure yeah thank you all so much for     
44:07     
the update this is all some really exciting stuff too by the way um okay yeah so Di and I just met uh last week     
44:13     
already I mean as we as we meet once a week uh some notes we had taken uh pushing things forward was trying to     
44:19     
decide oh yeah by the way um so yes and I did I did discuss with uh with oral last uh     
44:25     
Friday some future steps moving forward in terms of like uh excuse me um what we     
44:31     
should be using what software what tools what things should be using and once again so ultimately just just to get     
44:36     
just a back up and give a brief overview for those of you who maybe aren't so familiar or you know um hav't caught on     
44:42     
with up to speed on what we've been talking about so far this is part of the overall project basically about     
44:47     
detecting and curing a very large amount of breast cancer say 85 to 90% of breast cancer fers making things more faster     
44:53     
more efficient with the current technology we can do this lowcost low dose by taking advantage of dynamic     
44:59     
steerable patterns of um these fancy particles called Gus or Jos spheres and     
45:05     
the x-ray technology excuse me being able to optimize the optimize the X-ray steering     
45:13     
technology um take of these uh of of these Tor detection Tor detection     
45:19     
Protocols of this computer tomography approach so you know a lot of this involves taking these um     
45:27     
excuse me okay so so Di and I been working together on this using things like the mark algorithm using um using     
45:35     
Mark using rbct and um being able to move from 2D to 3D and eventually you'd     
45:40     
want to see if this sort of technology or this sort of tools and setup could even be seen in being able to create a     
45:46     
patent to re see what startup Venture Capital venture capitalist approaches we could use in really revolutionizing the     
45:53     
ways breast cancer is detected so at this point I've been trying around a host in a     
45:59     
suite of different tools and software um was already using Ma and P of programming languages but there were     
46:06     
a few different tools Bradley and some other people put in touch me with me like I did look into coso console     
46:11     
console but that requir is money and actually Cel probably not the best run for us to use after taking advice from     
46:17     
people here the 3D slicer the 3D slicer technology seems to be the best okay     
46:23     
yeah so that's what I'm going to use in do moving forward uh what is 3D     
46:32     
slicer 3D slicer is an image Computing platform open source for visualizing processing and U uh medical biomedical     
46:40     
and 3D images and meshes so you know it's a software it's a platform there's a very very strong community and while     
46:47     
it's normally used in um Anatomy like neurophysiology maybe bone Anatomy uh     
46:53     
the brain anatomy things like that um you know for not just like tissues but for skeletal     
46:59     
things too I think based off doing doing a bit of research the 3D size technology is     
47:04     
actually the best excuse me is the best for integrating machine learning Frameworks     
47:11     
and um visualizing as well too it has some very very strong interactivity with um deuring models that can work directly     
47:17     
within the environment itself and um I'm trying to look at the best ways to set it up on my computer so that's where I     
47:24     
am right now so okay that sounds yeah with like we're taking advantage of     
47:30     
like you know being able to perform the calculations on the precise mathematical level and seeing the best ways of you     
47:37     
know testing about say like the limits of perhaps uh different software you     
47:42     
know um liit to different software and Hardware see how this technology can be implemented into     
47:49     
actual into the actual CT devices themselves so best CH computed uh     
47:55     
technology devices too so like a lot of being able to even estimate like the power potential of like industrial CD     
48:01     
scanners um you know how we could go about approaching hospitals research labs and institutions with this sort of     
48:08     
Technology but all of that is far down the line right now we're getting work done we're publishing things providing stuff and zck and I have been making     
48:15     
some really great conversations and progress moving back and forth so yeah well that sounds great yeah I look     
48:22     
forward to seeing some demos and things like that in the future and you know it's like you know picking the right     
48:28     
software is always important because you you know you don't want to go down the path too far down the path and say oh     
48:33     
this is not the right environment to work in um so I know like uh Nick was     
48:39     
having a lot of issues with Mathematica for a while trying to figure out how to do things in it and yes it's really kind     
48:46     
of just you know sometimes it's really you go with the tool that you know and     
48:52     
yeah console another example yeah using console I think it's the finite element     
48:58     
analysis I don't it could be another program right is using finite element     
49:04     
analysis and I would still have the same problem Yeah well yeah find that's     
49:09     
another like the meth the method is really involved and tricky to get it to run right yeah so yeah yeah though my     
49:17     
work around is take one force at a time and let it settle and then put another     
49:24     
force in then let it settle so I'm long summer of feeding data into it     
49:32     
one little Force at a time a graph anyway I I'll do it but     
49:42     
it's it's kind of boring yeah yeah oh     
49:50     
well yeah yeah yeah I really really don't have     
49:55     
Club so but you know program is the program the software is the software you got to figure you know always think about the big picture the end goal your     
50:01     
functionality your capabilities and your needs even people ask me like like you say what's your favorite programming language and I always like hesitate yes     
50:08     
I'm like I don't I just don't even choose you could say I guess I guess I could say like python but it's like you know it's about the math it's about the     
50:14     
research about the science and what he needs you to get things done I know especially like for me with a lot with     
50:20     
um with some of the groups I work for and some of what I've been looking for to get to uh do things you know like it's a difficult balancing out being     
50:27     
able to balance like like fundamental Math and Science then like the like the languages to express that then like     
50:33     
software and tools and then like overall globally like what are you trying to do in name with what you're trying with uh     
50:39     
your work so yeah good yeah I found a the program for     
50:47     
um mat lab that um draws 10 seties and allows you to put forces on them um but     
50:55     
considering what I'm doing I have to then analyze the software to see what it's doing so more work     
51:05     
yeah yes so yeah but this is all really     
51:10     
really exciting I would love to even you know really really really take a full-fledged approach to this and     
51:15     
visualizing things you know not just for computers and even taking advantage of AR and VR technology too if scientists     
51:21     
and researchers who are performing these things so I guess the um Bre Bre can     
51:27     
diagnosis it who actually does that by the way is it the physician there's a physician it's a radiologist radiologist     
51:35     
right and there that is a physician right like the radiologist is a okay there we go I always think of it's like     
51:40     
the like the radiologist or the medical physicist or more like what the researchers you know who who actually     
51:46     
does the the breast cancer screeding but I'm pretty it is the radiologist correct yeah you know so yeah anyways you know     
51:54     
that's that's all always important to we there there's another set of people called the radiation therapists yep yep     
52:02     
and we're basically trying to replace     
52:08     
both birds with one big Stone one dynamically steerable Stone yeah the the     
52:14     
the trick is uh that we don't we take advantage of the breast compression     
52:20     
technology so the tumor doesn't move we don't have to find it twice     
52:30     
okay so that's that's basically big difference here is a i i     
52:36     
you I've had students work on image registration and it's a huge field uh     
52:41     
but basically you're trying to make up for motion of the patient and so if you can eliminate the     
52:47     
motion then uh uh you you've got a big problem solved yeah yeah yeah I mean I think I     
52:55     
think being a also limit it as much as or you know right automating it optimizing the detection approaches you     
53:01     
know treating it in some ways like a classification problem and of course you know just just going over things SPS     
53:06     
more time like when I when I speak to people about this project and it's like they um they mention uh like but the     
53:12     
women I talk to they always want to know is this going to make things like easier and I'm like yes absolutely less less     
53:19     
compression they always ask that so uh yes I have a stupid question on that we     
53:27     
we're dealing right now with square arrays uh I it would seem to be a     
53:34     
trivial problem to deal with a square array with a triangle on     
53:40     
top yeah uh and uh in that case the     
53:48     
compression uh the device we we that uh marello uh designed is a rotation and so     
53:56     
so you can see that if you can end up with a triangle on top if you don't go down to 5 cm which is     
54:04     
our our standard compression yeah yeah okay uh the problem what what people are     
54:11     
doing now is they compress the breast based on the force involved not     
54:18     
geometry and that would lead to a Tilt     
54:23     
at the at one end yeah and so you'd have a square with a triangle on top for two     
54:29     
Dimensions yeah I can I can look into that actually I can take note of that see how's the actual R geometry of the     
54:35     
confession itself too you know yeah yeah so I I think women will accept the force     
54:42     
based compression more than the geometry based compression okay okay I know     
54:47     
they've improved it they said they've improved it it doesn't matter much for the     
54:53     
standard mography because we just the picture a two-dimensional picture anyway but it does matter oh it does matter for     
55:01     
what you're doing yeah yeah okay okay yeah I mean like um at least at     
55:09     
least from you know my limited personal experience people that I spoking to it's um that's that's what they want to know     
55:15     
is there going to be less less you know yeah that's     
55:21     
important yeah yeah there's a cive woman who just refused tophy B on     
55:28     
compression yep because it hurts and small class of     
55:34     
technicians who are uh what do you what you call them they they compress to make     
55:41     
sure it hurts oh well that's only if they're in a bad mood yes     
55:53     
technici okay okay so I think standardization based on the     
55:58     
force used is probably best because the te texture varies between breasts and     
56:03     
the pain threshold [Music]     
56:10     
St yeah okay okay okay so that be that' be a big selling point from the point of view of people actually using it so if I     
56:17     
were to start something we to start talking to people the neurot and Medtech from the perspective I'll be on     
56:23     
the west coast do not work with people there so     
56:29     
yeah that's great yeah yeah California's I'm going to be so much happier in     
56:34     
California than Indiana by the way absolutely absolutely indana I'm in     
56:40     
Indiana yeah VI I I'm in Indiana Oh     
56:45     
Indiana I thought you said IND no happy there I hear     
56:53     
expensive yeah been toana a couple of times it's a nice     
57:00     
place cool cool yeah Jess said he might be in San Diego or San Jose the     
57:09     
summer okay okay yeah yeah all     
57:18     
right there any other updates or things people wanted to mention before we     
57:24     
go um well the matrices you're using um look like the matrices they use for     
57:33     
the T secrity or at least they're similar oh yeah     
57:38     
yeah yeah there there are some definitely some interesting things uh having to do with you know     
57:45     
once we get these models up and running to look at like tensegrity especially with message passing you could use like     
57:51     
that technique to look at tensegrities or look at forces being transmitted     
57:57     
uh in these networks so you know that's that's going to be like one of the maybe the outcomes of this as we can do things     
58:03     
like that and in cells and and in larger systems like that I'll     
58:11     
I'll I have to figure out my uh connectivity matri or my my cell that     
58:18     
I've developed yeah yeah speaking of compression     
58:27     
one of the curious things you can do with an embryo is confine it between two     
58:33     
surfaces of perhaps two surfaces that are at least oxy REM and then see how the cells rearrange     
58:42     
oh yeah yeah well you could even do that with an nematode     
58:47     
right okay uh I if you if you force an to to be flat what do you get VI org     
58:59     
yeah it' be interest yeah be uh     
59:05     
yeah they they're U they permanently rearrange right after a while it's 10 o'     
59:13     
uh I'm not sure well I'm trying to remember the experiment we did I we have an abstract on on compression of an     
59:20     
embrya but yeah it's it's a technique used for     
59:27     
cell tracking experiment oh well they didn't cell track anyway they uh squash it between     
59:34     
two plates and measure the viscal elasticity yeah so there there are uh     
59:42     
papers about that out there yeah we we had an abstract there basically uh I     
59:47     
think if you compressed an embryo for a short time it recovered and then     
59:52     
released it it recovered completely but it it did it for six hours it it was it     
59:58     
was quite different um what I've been told is that tissue acts like a an elastic substance     
1:00:07     
on the short short time scales and as a fluid on long time scales oh yeah we had     
1:00:14     
a dramatic uh experiment we tried to uh hold an embryo on top of a glass tiny     
1:00:21     
glass tube by using negative pressure so it wouldn't fall off and we could try to     
1:00:28     
visualize the whole thing it was a failure because the ambrio went right down the     
1:00:36     
tube acting like a fluid again     
1:00:42     
yes yeah yeah I mean it's like um you know what how how however you want to     
1:00:48     
even measure the state like what what state what state the liquid is in or what state the material is in that's     
1:00:55     
that's its own to so yeah yeah yeah there I mean we actually have a lot of     
1:01:01     
stuff with like soft materials and embryogenesis where people have looked     
1:01:08     
at the phase transitions between the y y yeah yeah I think Z and I we were talking about that earlier like um a     
1:01:14     
more approach on like embryogenesis of uh of the worm before before we took on this breast cancer product before then     
1:01:20     
so yeah yeah all all excit all exciting stuff all really happy with these things     
1:01:27     
um like um happy to see where so so by the way yeah so I'm moving out to Sacrament California so I can work um     
1:01:33     
fulltime with the company and this can definitely be something I do part-time like 10 to 15 hours a week too so that's     
1:01:40     
where things are going and things can still be kept up Kevin speed on those things okay so sounds     
1:01:47     
good that's it for today um yeah thanks for     
1:01:53     
attending so I wanted to talk about a few more items today first one is um something on biomolecular modeling so     
1:02:01     
this paper is the ugly bad and good stories of large scale biomolecular     
1:02:08     
simulations and so this paper is uh published in current opinion and structural biology they kind of go over     
1:02:15     
some of the modeling techniques and some of the failures some of the promising     
1:02:21     
Avenues and so forth so I think this uh paper kind of highlights a lot of the     
1:02:27     
aspects of modeling in the biomolecular space but also speaks to modeling more     
1:02:33     
generally as a technique in biology and how you know this can be so the abstract     
1:02:41     
reads molecular modeling of large biomolecular assemblies exemplifies a     
1:02:46     
disruptive area holding both promises and contentions propelled by P and access     
1:02:54     
scale Computing so these are large scale Computing platforms several simulation     
1:03:00     
methodologies have now matured into userfriendly tools that are successfully employed but for modeling viruses     
1:03:07     
membranous nanoc constructs and key pieces of the genetic Machinery so we're modeling very small things we have very     
1:03:14     
large amounts of compute and we're going to take the small things and model them over time we're going to model the     
1:03:21     
details of many of these very small things in a cell     
1:03:27     
we presentent three unifying biophysical themes that emanate from some of the most recent multi-million atom     
1:03:34     
simulation Endeavors so this is where we have a lot of these very small things     
1:03:39     
and so we're trying to simulate them in very large numbers a couple weeks ago we talked about modeling the minimal cell     
1:03:47     
and how that is a challenge that people have used uh different types of biomolecular tools to try to address and     
1:03:55     
so this kind of connects to them the way they approach these problems um despite connecting molecular     
1:04:02     
changes with phenotypic outcomes the quality measures of these simulations remains     
1:04:08     
questionable so what they're basically saying is that these sorts of simulations aren't necessarily topnotch     
1:04:15     
yet or at least you know in this where we are with this kind of endeavor so we have to first of all think about     
1:04:22     
molecular changes and how they relate to the sort of the next level up which is this phenotypic outcome but then we also     
1:04:30     
have uh you know measures how do we evaluate these kinds of simulations how     
1:04:35     
do we evaluate the outcomes we discussed the existing and upcoming strategies for     
1:04:40     
constructing representative ensembles of large systems so you're using these very     
1:04:46     
small parts not only as sort of these things that collectively Express a phenotype but you're looking at them as     
1:04:53     
ensembles statistical ensembles that allow you to characterize these large     
1:04:59     
systems but also we look at how new Computing technologies will boost this area and then make a point that     
1:05:05     
integrative modeling Guided by experimental data is the future of biomolecular computations so we need to     
1:05:12     
have integrative models we need to have all these small parts connect them through some sort of phenotype and     
1:05:19     
evaluate them and then basically validate it with experimental data make     
1:05:24     
sure that the parameters are correct so it's really like any other modeling Endeavor and this is to an     
1:05:32     
audience of searer biologist of course you know you have to speak to the experimental aspect occasionally we can     
1:05:38     
do modeling without valida we can often do modeling without validating it through experiment but a lot of these     
1:05:45     
simulations are really calibrated through experimental data so what they're basically getting at here is     
1:05:51     
they're assessing the state of the field and they're saying that we have different types of data     
1:05:56     
we also have deep learning and basically what we can do is we can approximate the atomic structures of many biomolecular     
1:06:04     
complexes so we get down to the atom scale but we also know that structures     
1:06:09     
alone provide limited information about function so if we're thinking about phenotypes we're thinking about function     
1:06:15     
we can't just say that we let's model struct biomolecular structures down to     
1:06:20     
the atomic level and leave it at that we also have to think about the structure     
1:06:26     
uh of many of these biomolecules interacting even if we have atomic scale     
1:06:31     
resolution it doesn't necessarily help us in making those uh you know     
1:06:37     
assessments now they also biomolecules interact with     
1:06:42     
large scale networks and that provides sort of the information about the phenotype as well so you need to have     
1:06:50     
not only the high resolution imaging but you need to have the network uh simulation as well so we also have     
1:06:58     
things like multikill algorithms and Computing Hardware we can look at molecular simulations at these very     
1:07:04     
small scales and we can actually say things that we couldn't say necessarily with just Imaging alone or or sort of     
1:07:12     
our models before we had this high resolution approach you know that that     
1:07:17     
maybe they're incomplete or not sufficient so what they're doing here is     
1:07:25     
they're talking about leveraging all atom AA or course scen CG models but     
1:07:31     
also multiphysics molecular Dynamics simulation so these are MD models so     
1:07:36     
basically you can take something that's at the smallest sort of scale possible which are these Atomic     
1:07:42     
simulations you can also corar grain these simulations so you can take you know uh you can model things that aren't     
1:07:50     
at the highest resolution but they can be modeled at a scale that's use ful to     
1:07:57     
the problem but it doesn't use as many computational resources or you can look at molecular     
1:08:02     
Dynamics which is where you're looking at something like protein fing and then something like that you need to consider     
1:08:08     
different types of physical regimes different parts you need to maybe take some atomic scale uh simulations and     
1:08:16     
combine them with course grain simulations and they have different physics and you put them together and so     
1:08:23     
multiphysics uh simulations that's something that we've talked about with respect to console but there other     
1:08:28     
molecular Dynamics packages particularly for biomolecules     
1:08:34     
that are quite well Advanced and developed so this leads us to this     
1:08:39     
computational microscopy of heterogeneous macromolecular complexes so we can     
1:08:46     
simulate things like the ribosome prosome nuclear pore complexes virus     
1:08:52     
capsids uh entire genes and other types some environments within cells uh and so     
1:08:58     
these are types of things we can simulate again we're using some you know atomic scale simulations or screen     
1:09:05     
simulations molecular dynamic simulations or putting all this together to infer some sort of     
1:09:13     
pheny so now they talk about a story of hits and misses so they talk about how indeed these models can provide     
1:09:19     
biological insights at different scales but these the merits of these     
1:09:25     
type of simulations are worth reflecting on so we don't necessarily You know despite how impressive they are they     
1:09:31     
don't necessarily deliver what maybe we need or maybe what's promised so they point to this     
1:09:37     
discrepancy between biological and simulatable time scales which is aply summarized by this     
1:09:44     
equation here so this basically describes biological time scale or the     
1:09:49     
time scale of interest and the thing we can simulate so sometimes we don't have     
1:09:55     
enough computational power to simulate the entire time scale we're interested in sometimes our simulations of raising     
1:10:03     
a course screen model exceed the biological phenomena of Interest so if     
1:10:08     
we have to cor screen things to one millisecond if what we're looking at is sub millisecond resolution and we can't     
1:10:14     
really say much about it so they give an example here     
1:10:20     
simulating A M millisecond scale event of a 1 million atom system system     
1:10:26     
with a ftoc timestep molecular Dynamics on a pedop flop architecture so     
1:10:32     
these are all components that we're bringing in so we have our event that we're interested in this is at the     
1:10:38     
millisecond scale we have the system of a great number of atoms we have this     
1:10:45     
time step of a ftoc and we have this large computational     
1:10:50     
architecture now given that all those pieces even though have a lot of     
1:10:55     
computational power this problem will still take 159 days and so if this only     
1:11:03     
scales up with complexity so as we add to our system in terms of size and 1 million atoms is not a huge system in     
1:11:10     
terms of biology it's a huge system in terms of atomic simulation but not     
1:11:16     
necessarily in terms of a biological uh phenomena of Interest our time scales are pretty     
1:11:23     
short so if we're interested in something in a larg time scale something like differentiation of a cell we need     
1:11:29     
more computational power and then of course we have you know the time steps     
1:11:35     
we talked about this even with seigan EO Genesis that the time step is you know     
1:11:42     
the amount of time that we have from observation to Observation or in a     
1:11:48     
simulation from event to event so you know we have that sort of time     
1:11:54     
limitation it's not a continuous system in the sense that we can you     
1:12:00     
know things can happen like whenever you have to S you have to build your samples     
1:12:07     
and simulate accordingly so that's that's what we're dealing with um and so     
1:12:12     
this is really kind of insufficient for a lot of things in biology especially when we get up to phyp because we get up     
1:12:19     
to phenotype we have these very large systems even if we get down to our     
1:12:47     
so this is really insufficient for a lot of what we're doing in biology so even     
1:12:53     
when we have with biologically very small systems there's actually a lot     
1:12:59     
that we can simulate at that sort of reductionist level and you know we have     
1:13:05     
to really kind of pick our tools and you know we get a lot of really interesting information about     
1:13:12     
what's going on biomolecularly but the phenotypic level is not necessarily going     
1:13:18     
to so more dtic are estimates for simulating firing neurons in a human brain and of course in a human brain we     
1:13:26     
have a great many neurons but we have even more atoms and so if we use the same     
1:13:32     
parameters as we did for that biomolecular simulation to simulate firing neurons in a human brain or we     
1:13:39     
have an estimate of around 10 to the 26th atoms it will take up to 10 to the 23rd     
1:13:45     
years to simulate so obviously you can't sit around and wait for that     
1:13:50     
simulation so you know we might ask the philosophical question question about     
1:13:56     
the value of simulation systems in which the majority of the atoms are bulk water molecules often contribute only     
1:14:03     
parametrically to the biophysical properties so you know even when we can simulate every atom or we can simulate     
1:14:10     
every biomolecule the question is should we do that maybe we can you know ignore or avoid certain     
1:14:18     
things simulating certain things in the interests of computational efficiency     
1:14:23     
but also just in the interests of you know not worrying about things that don't really have an effect on the     
1:14:32     
system limited time scale larger system simulations are also plagued by a lack of convergence in capturing     
1:14:40     
diffusive events so this is where we're looking at diffusion we're looking at things moving     
1:14:47     
around the cell Force we know that like we can simulate things in a static environment but what about when things     
1:14:53     
move around and so this is where we need to get     
1:14:59     
uh we need to be able to not only model the fusion in terms of things moving     
1:15:04     
around in space but emergent properties so when things of course interact they     
1:15:10     
have certain effects in the phenotype if you have biomolecules interacting those     
1:15:16     
things interact additively or do they have this imer set of properties so these are things we need to think about     
1:15:22     
as well so another thing they talk about is where you're looking at things of like a particular protein and if you     
1:15:29     
were to Model A say a cell you're going to have a lot of copies of the same protein so the question is you know is     
1:15:36     
that just redundant information or do we need to model indeed model every protein so this is a     
1:15:43     
question you know you might ask if you want to make your simulation more tractable smaller still get the same     
1:15:49     
result or actually an informative result so uh in some cases copies of a     
1:15:55     
particular protein in a system or simulating many replicas of a system increases the statistical information     
1:16:03     
but this is not helpful for properties that are only apparent over longer time scales so we can actually increase our     
1:16:10     
information by having many copies of a particular protein in some cases because     
1:16:15     
it increases our statistical information we know for example that if we have an ensemble and we have C A Certain copy     
1:16:23     
number uh set of protein so if we have like 20 proteins we want to see what the changes in the copy     
1:16:30     
number are over time or in a you know in a in a cell or some part of a cell uh     
1:16:37     
then that's information that's that's quite valuable statistical information looking at changes in that     
1:16:44     
Ensemble but over a longer time scales that may not be useful so having copy     
1:16:49     
number information it might not be useful because maybe it's over time it's just this stochastic flux and maybe we     
1:16:57     
we're just interested in maybe one sort of isolated uh process that happens over     
1:17:03     
maybe like a you know a millisecond time scale but if we simulate a cell over     
1:17:09     
days the you know that that process repeats and so that measurement becomes     
1:17:15     
more or less useless so for example simulating slow interface Dynamics or     
1:17:20     
modeling of viscous properties arising from the collective interactions that many system components offer exemplary challenges to     
1:17:28     
molecular Dynamics approaches so this is where you know sometimes you want to take your simulation and run it again     
1:17:34     
and again to make sure that you have the sort of consensus view of the model and     
1:17:40     
so this is another thing where we have a problem uh in terms of like running the     
1:17:45     
simulation again and again and getting a consensus or convergent outcome and so     
1:17:51     
this is important of course for reproducibility so if you want to reproduce your simulation or you want to     
1:17:58     
you know say definitively that this is what's going on in the cell lot of times we don't have that kind of     
1:18:04     
reproducibility and so another point they make is about looking at sort of the resolution of these systems and how     
1:18:10     
useful they are they to sort of say you know make inferences about what's happening in the cell so unlike     
1:18:17     
relatively simple single protein systems cell scale systems are highly complex     
1:18:23     
thereby complicating Direct comparison of theory with experiments cell level results are often not useful to validate     
1:18:30     
simulations because they tend to have a smaller number of data points as a very complex origin you know you think about     
1:18:35     
like making comparison Apples to Apples comparisons and in these simulations you're modeling a lot of different     
1:18:42     
things so how many data points do you have that are directly comparable or how many data points do you have that are     
1:18:49     
sort of where you can make a connection to the phenotype and this is a problem     
1:18:54     
with some of these simulations especially when you're trying to simulate a lot of different things that whats it's nice to have that sort of you     
1:19:02     
know all the different parts of the system but actually being able to put something into practice is a little     
1:19:11     
different uh consider simulating AP activity of an enzyme as a function of environmental conditions for this case     
1:19:19     
is difficult to pinpoint whether within a chain of computational results specific issues have emerged     
1:19:25     
from the convergence of the ATP hydrolysis computations force field discrepancies between protein and the     
1:19:31     
wans scaling mismatch between the rate of ATP turnover and that of the environmental changes or compensation of     
1:19:39     
Errors so there are a lot of different things going on sometimes you can't pinpoint what some you know what some     
1:19:46     
error is due to or what some result is do to that's attribution is the problem     
1:19:51     
here     
1:19:58     
so these are the types of simulations they're talking about and if you go back a couple weeks we were talking about     
1:20:03     
molecular simulations and simulating the minimal cell and so a lot of these     
1:20:08     
platforms and a lot of these types of simulations kind of come out of that area so there are things like lamps an     
1:20:17     
AMD Promax uh this is where they have these Atomic skill simulations where they're     
1:20:23     
modeling things like the HIV capsid the HPV capsid uh hbv capsid purples bacteria     
1:20:34     
chromator uh cerin repeats and so forth the gapor     
1:20:40     
gene then we have these uh CG simulations uh so these     
1:20:50     
are these are the coar grain simulations we talked about so this is using gromax     
1:20:56     
Nan and lamps of course the same platforms but when now we're course graening things from the atomic scale     
1:21:02     
out to other you know we're combining like things and we putting things into bigger bins so these are you know things     
1:21:09     
like outer membrane proteins lipids proteins and carbon nanot tubes and     
1:21:15     
membranes uh outer me membrane vesicles inward rectifier pottassium     
1:21:21     
Channel and so forth so this is you know sort of an example of these different     
1:21:26     
types of models with different simulation lengths as appropriate to the problem and of course you see you know     
1:21:33     
that a lot of these platforms are very diverse in terms of the kind of simulation lengths you can run things at     
1:21:40     
but it does have an effect on like if you were to combine some of these models you know would you be able to make     
1:21:46     
inferences at the phenotypic level of the cell and that's ultimately the goal     
1:21:51     
here the goal is to put these simulations together into cell or into a     
1:21:56     
biological context so we can model HIV capsid for example but it doesn't     
1:22:01     
operate in isolation we need to bring it together with other biological     
1:22:07     
subsystems so this part's interesting early surprises in the enhanced sampling of confined     
1:22:13     
subsystems so this is in the early days of biomolecular computations of     
1:22:19     
multi-million or larger size systems so this was the early days of these very large sort of molecular simulations     
1:22:26     
where we had multiples of millions of components and despite the AFF     
1:22:32     
forementioned outstanding questions of simulation convergence and quality a number of studies performed over the     
1:22:38     
past five years have paved the way to billion atoms at molecular Dynamic simulations so one of these was to model     
1:22:45     
the structure of the complete Gat for genene which was mentioned in the     
1:22:50     
table uh there is remarkable similar similar in the properties derived from     
1:22:56     
these studies of very different biological systems three common themes emerge so we get insights into the     
1:23:03     
mobility and binding of water and ionic species and confine protein     
1:23:09     
environments the second is rare subst substrate binding mechanisms and then three is stability     
1:23:15     
and diffusion of proteins in crowded environments so one theme revolves around this idea of solution Dynamics     
1:23:21     
and confined environments so recognizing that a majority of     
1:23:27     
simulation systems are composed of ions and solvent molecules the Dynamics of which are characterized by rapid picoc     
1:23:37     
scale relaxation times which may scale up to Nan on the protein surfaces the     
1:23:43     
statistics from microsc scale molecular Dynamic simulations was clustered defined ion localization sites in the     
1:23:51     
interfaces of virus capsids um In The Same Spirit a rapid exchange     
1:23:57     
of water molecules across an empty polio virus was determined the exchange rate is so high that all water molecules     
1:24:04     
inside the capsid can be replaced by new ones from the outside in around 25 micro     
1:24:10     
microc seconds explaining the capsid's tolerance to high pressures such finite size effects of     
1:24:16     
the nanom container cannot be reproduced with periodic representations of the isolated smaller subunits where the     
1:24:24     
spatial correlations and the impact of confinement on transport rates are lost     
1:24:29     
thus these atomic scale or uh G grain simulations are confirmation correlated     
1:24:36     
transport uh thus uh atomic scale or RIS grein scale simulations of     
1:24:43     
confirmational correlated transporter solvent molecules and ions across symmetric or asymmetric Forest protein     
1:24:50     
animal containers have gained popularity over the past few years so this tells us one of the reasons why we use     
1:24:57     
the types of techniques that we use because they're useful for looking at some of these problems and giving us     
1:25:05     
insights okay now I'd like to talk about something um on where we are with deep     
1:25:10     
learning for biochemistry so earlier we were talking about graph neural networks and some of     
1:25:16     
the techniques for that but now we're going to talk a little bit about deep learning of biochemistry right so this is based on a blog post I lot on no now     
1:25:24     
and this is called so where are we with deep learning for bioca so as you know uh may be aware we     
1:25:32     
have a lot of applications of deep learning to biochemistry we have uh a lot of     
1:25:39     
applications to protein folding and protein prediction and so this is kind of going     
1:25:46     
over some of these findings and where the state of the field is so you know     
1:25:53     
we're using different tools like P torch and other things and you know it's kind     
1:25:58     
of important to kind of frame our investigations using deep learning     
1:26:05     
something it's biological about starts we have 40 million compounds in their screenings in pupm Zab bases of newly     
1:26:11     
sequenced that every year the full transcriptome of Aging mice and comprehensive maps of the EPO so we have     
1:26:19     
all these different types of data that are available that are you know I guess you could say cheap because they're     
1:26:25     
sitting out on the internet waiting to be analyzed and mean people analyze them but we can do so much more with them and     
1:26:33     
so we have all these large databases and things like that um yet     
1:26:39     
machine learning and deep learning haven't generated much radical Insight from all of this quite     
1:26:44     
yet so despite all the data that we have and despite the advanced deep learning     
1:26:50     
techniques machine learning techniques we haven't really been able to generate much     
1:26:55     
insight and I would include computational biology in that mix because we do a lot of computational     
1:27:02     
studies and it's really difficult to find Insight or to gain Insight from some of these     
1:27:08     
investigations you know we might be able to analyze things for certain genes or certain patterns but you know knowing     
1:27:15     
exactly what that means in the context of the broader scope of biology is has     
1:27:21     
been somewhat elusive so that's what they mean by radical Insight computational tools are     
1:27:27     
currently in an assistant role at best and arguably there's nothing new here so you know you might think of     
1:27:33     
computational tools as the savior of the field or as this thing that's going to     
1:27:39     
solve the field but in fact up till now they've just basically been another tool     
1:27:44     
much like any sort of new imaging technology or new analytical Technologies so it's important to     
1:27:51     
realize where we actually are with this I think there's a temptation to think about computational tools as     
1:27:58     
being you know super Advanced and really you know much further along than they     
1:28:04     
actually are computer rated drug design has been around since the     
1:28:09     
1960s so this is something that we do with machine learning and people give     
1:28:15     
presentations on it as it's this really you know that it's going to solve all     
1:28:21     
the problems of your design but we've been doing this sort of stuff since the 1960s and in fact you know     
1:28:27     
it's not really one of the reasons people Embrace deep learning and machine learning other than being trendy is to     
1:28:35     
actually gain new insights so you know we haven't really gotten that structure     
1:28:41     
activity relationship analysis and chemoinformatics both computationally     
1:28:47     
heavy we've also been around for a while so this structure activity relationship     
1:28:52     
analysis is this is the Wikipedia step for this this is the relationship between the     
1:28:58     
chemical structure of a molecule and its biological activity this idea was first     
1:29:04     
presented by Alexander chome Brown and Thomas Richard Frasier least as early as     
1:29:11     
1868 uh so this is you know something that's been around since the 19th century this is something that you     
1:29:17     
basically take the structure of a system look at its activity and iners some sort     
1:29:22     
of function eventually so this is something that's been around since the 19th century and so these but these of     
1:29:29     
course if you want to really want to gain insights they're very computational heavy problems you can put them on very     
1:29:35     
powerful computers and in fact these fields have been sort of U you know held     
1:29:42     
up by the lack of computational power so having very powerful computers is is a     
1:29:48     
plus but it's as it turns out it hasn't really yielded it much Insight yet     
1:29:55     
bi biologists regularly use machine learning techniques since the 90s biologists have regularly used     
1:30:03     
machine learning techniques since the 90s whereas some say today's ml is tomorrow statistics so people have been     
1:30:10     
using machine learning techniques like uh regularization and regression and     
1:30:15     
things like that since the 90s on computers and and so you know we we have     
1:30:21     
it's been around for a while it's it's I guess you could say it's it's mature enough to gain some you know to show     
1:30:28     
some insights and it really hasn't and that maybe is because you know people     
1:30:33     
are always trying new techniques but again the stuff that we do today is going to be statistics of tomorrow     
1:30:40     
they're going to be the mainstream statistics of tomorrow so maybe we just need to wait and see the first mon caral     
1:30:46     
simulation of protein folding appeared in 1975 so you know doing this uh these     
1:30:53     
type of techniques on a computer 1975 computers were not very advanced in     
1:30:58     
terms of their computational power but nevertheless people were able to do these kinds of uh studies this one is     
1:31:06     
from pnas this is model of protein folding inclusion of short medium and     
1:31:11     
long range interactions this is from 1975 so even then they were doing things     
1:31:17     
with you know it's all about setting up the mathematical problem and really you     
1:31:22     
know it's not about the computational power so much as creating a nice mathematical representation of the     
1:31:28     
problem so when you can do that you're actually ahead of the game and so this is kind of the point that they're making     
1:31:34     
is that we've been able to do the you know throw these problems at a computer for a while now and we still haven't     
1:31:41     
really gotten a lot of insight the first Maximum likelihood maximization algorithms were used for     
1:31:48     
the construction of evolutionary trees and that happened first in 1981 and that's this paper     
1:31:55     
uh by Joe felenstein evolutionary trism DNA sequences a maximum likelihood     
1:32:03     
approach uh the first algorithm for classifying protein structures appeared around     
1:32:09     
1987 and that's this paper tertiary templates for proteins use of packing     
1:32:15     
Criterion in the enumeration of allowed sequences there are different structural classes like Ponder and Richards and     
1:32:22     
again in journal molecular biology in 1987 not to mention all the sequence     
1:32:28     
alignment algorithms that were getting developed by the early 70s and so that is you know there were a     
1:32:34     
lot of algorithms that were developed really early on to deal with this idea of sequence alignment are taking     
1:32:42     
sequences and aligning them and seeing where they kind of fit together and so     
1:32:47     
the nman and on algorithm goes dates back to 1970 the Smith and Waterman     
1:32:53     
algorithm dates back to 1981 and of course our computational power has been     
1:32:58     
building since then so you know sometimes people develop algorithms that aren't really suited to the hardware of     
1:33:06     
the time but still you know these are algorithms that are that could been run back then on the computers that they had     
1:33:13     
back then so these are things that you know either we have like sort of these     
1:33:18     
fundamental techniques and we're just waiting for computational power or where     
1:33:24     
we have the computational our infrastructure and then we've gotten the computational     
1:33:29     
power and now we're just waiting for the inside so that's what they're kind of getting at with this despite this I've been periodically     
1:33:36     
noticing an impression that for ages biology and chemistry have been done with rocks and sticks and caves until     
1:33:43     
deep learning was brought to wet Labs by computer scientists A Kinder Prometheus bringing fire to mankind I think this is     
1:33:50     
a really interesting point because this is a this s summarizes the attitude in a lot of biological Labs that biology is     
1:33:58     
not about math or computation and it's really people from outside the field are     
1:34:03     
bring this stuff in and this is just kind of something that's recent before then it was like the Dark Ages I think a     
1:34:10     
lot of that is overplayed I think there is of course the wet lab culture where you're doing things with wet lab stuff     
1:34:17     
and you're not really worried about simulating your results but of course you need to have experiments you need to     
1:34:24     
have you know that sort of Outlook the biological Outlook you need to know what     
1:34:29     
possible in the biology before you go to to doing things on a computer and in fact bringing in     
1:34:37     
techniques from the outside can be sometimes challenging because you don't know what the relevant problems are but     
1:34:44     
this attitude this is this is a good way to put this it's a kind of Prometheus bringing fire to     
1:34:50     
Mankind and Mankind being humankind but but other than that it's also kind of     
1:34:56     
this idea that it's a divine intervention by computational scientists among the poor     
1:35:03     
biologists uh in any case here I tried to address that impression and go over all the possible reasons or why biology     
1:35:11     
didn't quite become easier with advancements and deep learning and novel architectures which might or might not     
1:35:17     
be instructive for what we should do about about it moving forward so again     
1:35:22     
you know this is like this kind of goes through some of these outstanding issues     
1:35:27     
things that maybe we need to think about as computational scientists things that biologists bring     
1:35:33     
to the table but also you know if we're doing computational biology or we're     
1:35:38     
doing deep learning in biology or biocham whatever it is we really need to understand the systems that we have and     
1:35:46     
so in the last paper we talked about um some of these issues with biomolecular     
1:35:52     
systems and how it's not just as simple as simulating everything at high     
1:35:57     
resolution and just putting it together because that doesn't necessarily cut it     
1:36:02     
you really need to have a very distinct methodology for     
1:36:07     
this so the first thing you talk about is representations let's look at the elephant in the room molecular     
1:36:15     
representations this is where how the data looks when you input it into your model it is what you assign your     
1:36:22     
properties to is what you're predicting labels for so really your model is only     
1:36:27     
as good as the input data the input data has to be represented in a way that's aable to the model so this is what     
1:36:34     
they're getting at with this whole aspect of representations it's not something that say if you're popping you     
1:36:40     
know numbers into an algorithm you're necessarily thinking about representation is     
1:36:47     
important from the prediction of inval properties to of Target toxicities to clearance to halflife predic s to organ     
1:36:55     
specific effects there's been a lot of published in this direction for of AI for dark     
1:37:00     
optimization such work requires mining enormous data sets from the databases     
1:37:06     
that exist but you know this is just an aggregation of data this doesn't give us     
1:37:11     
any framework for the data and so there are people who have worked on ontologies and things like that and those are     
1:37:17     
useful to some extent to tell you something about the data but there weally need better     
1:37:25     
representations better sort of U schemas that we can use to deal with this and     
1:37:30     
something it's biological relevant not just things that are you know kind of made up representation and stored     
1:37:38     
information differ between databases more generally molecular structures are represented as one of the     
1:37:44     
following uh fingerprints so this is a binary encoding we saying whether something is     
1:37:50     
present or absent string representation which write down the structure as a string of     
1:37:56     
characters graphs which are treating which treat atoms as nodes and connections sges and you can scale your     
1:38:03     
graph accordingly so it could be cells it could be uh biomolecules it could be atoms that are these nodes and then you     
1:38:10     
have connections between them which are the interactions column Maps or Matrix     
1:38:16     
matrices of colic part potentials between atoms with the potential energy of the free atom along the diagonal     
1:38:24     
and then Atomic distance maps are coordinates of each individual atom so in dorm we have these kinds of     
1:38:31     
representations and this is why you know we we approach problems the way we do we     
1:38:36     
have uh microscopy image that we could you know uh extract features from but     
1:38:43     
then we can we also have these Frameworks uh for kind of interpreting those results we     
1:38:50     
have fortunately sort of a middle layer which is you know things like you know     
1:38:57     
lineage trees and um differentiation trees and maps and things like that that     
1:39:02     
we use to sort of guide how these things unfold in development and so we we put     
1:39:08     
these kind of together and we try to inform you know some of the     
1:39:13     
computational work with what's actually going on in the     
1:39:20     
biology so the second thing is that molecular to still bottleneck the resolution of     
1:39:26     
things so this person uh this is a very     
1:39:33     
okay my personal biological red pill happened when I learned that uh scr and     
1:39:38     
a which is a tech nextg sequencing technique captures only 30% of the     
1:39:44     
transcripts in the cell in the best case scenario so this is uh where basically     
1:39:50     
you're only capturing about 30% of a cell state if you consider if you consider xgen sequencing to fully     
1:39:57     
capture the state of a cell which of course it doesn't but of course you know if you're thinking about like you know uh if     
1:40:05     
you're thinking about the best case scenario where your cell is homogeneous your system is homogeneous like we     
1:40:12     
assume cgans is homogeneous or relatively homogeneous then maybe that's     
1:40:17     
good enough but it's probably not because the signal from every individual     
1:40:23     
cell or every individual source is weak and we can only really make an assessment when we aggregate everything     
1:40:30     
but of course when you aggregate things you're knocking out a lot of necessary variation so you're just taking a mean     
1:40:38     
or an average and that's not necessarily good because you when the system takes     
1:40:43     
on the sorts of out of distribution States or you know uh     
1:40:50     
non uh equilibrium states that biological systems often do then you     
1:40:55     
have a problem so this is the kind of thing that we also have to worry about we     
1:41:01     
still have this bottleneck where we have a bottleneck at resolution but we also have this bottleneck at variation and     
1:41:08     
now we can't really capture the variation sometimes it's because we don't have the uh you know we need more     
1:41:14     
resolution sometimes we need more even more computational power whatever so we have these bottlenecks related to     
1:41:21     
sampling and some of the information we're getting out of our biological techniques and our     
1:41:28     
systems the third one is information will get distorted and lost even before you can say pie torch so this is where     
1:41:35     
we can just plug things into pytorch and be satisfied or we can think about all the information we're losing on the way     
1:41:42     
from the data collection down the pipeline to these analyses tool these     
1:41:48     
analysis tools and so you know there are a lot of things that happen in bio logical lab settings that basically     
1:41:56     
destroy information before it's even measured so you have things in your     
1:42:01     
samples that happen all the time where you freeze the sample you thaw the sample you get degradation of the RNA     
1:42:08     
rna's very susceptible to degradation you get degradation of the nucleic acids     
1:42:15     
all sorts of things cells die if you're measuring things the cell population     
1:42:20     
cell population is always changing as you sample it and so forth so there's a     
1:42:25     
lot there a lot of problems with technical variation where you know it's     
1:42:30     
something that happens to your sample you can't control you don't even know what happened and it's affecting your     
1:42:37     
measurements and it's affecting your how effective your techniques     
1:42:43     
are so they kind of go through this uh so you can fight for yours for one% accuracy improvement with batch     
1:42:49     
Corrections and better architectures but you will be matching accuracy and something that's very messy to start     
1:42:55     
with so it's really hard to get good     
1:43:01     
accuracy in in this sort of thing so this is maybe one of the reasons why we haven't gotten a lot of     
1:43:06     
insights the last part is biology is more than just a sum of its parts and it argues that you know the whole     
1:43:13     
connection between DNA and mRNA proteins metab to metabolites and then     
1:43:20     
phenotypes is that it's more than the some of the parts so you have all these     
1:43:25     
components in the cell and there's this Ensemble of different macro molecules     
1:43:32     
you have maybe copies of those macro molecules and that's interesting but they also produce a     
1:43:38     
phenotype with that phenotype isn't just the sum of the distributions of the macro molecules it's that it produces     
1:43:44     
something new and those new things can be modified accordingly so if you have a     
1:43:51     
number of proteins if you have a certain copy number of proteins that can contribute to the phenotype and not     
1:43:56     
necessarily in a linear fashion so that's something to think about um you     
1:44:02     
know we we're really we have a millions of variables interacting sometimes they're interacting in nonlinear ways     
1:44:08     
sometimes they acting linearly and we can't really reconstruct the system     
1:44:13     
because we don't know what all those interactions look like so we can measure the states of all these things RNA DNA     
1:44:20     
chemical modifications and in fact there a lot of groups now doing just that     
1:44:25     
where they're getting this just this multitude of measurements and they're reducing dimensionality and they're     
1:44:31     
plotting it on a pretty picture um but that doesn't really tell you anything about what's actually going on in the     
1:44:38     
phenotype it just gives you like kind of an assessment of     
1:44:44     
state so you know this is the thing about deep learning is that sometimes people mistake deep learning for being     
1:44:51     
sort of the solution to this problem uh you know this is where deep learning comes in no nonlinear systems modeled in     
1:44:58     
a nonlinear way there is so much multiomic we can borrow things like Sensor Fusion and research of so this is     
1:45:04     
just talking about how we can use Advanced Techniques to overcome a lot of this uh these problems of integrating     
1:45:12     
data and variation and all this we we can use a technique called Data Fusion which they use in self-driving cars uh     
1:45:20     
data Fusion can be done and is done in biology but not by retrospectively integrating all the data we ever     
1:45:25     
collected your tools in your setting should account for future integration but you know when you have disconnected     
1:45:32     
experiments when they're in different contexts the technical Corrections don't     
1:45:37     
necessarily help you so just we just have a lot of problems with integrating different data     
1:45:44     
sets um you know sometimes we have the separation of time scales problem uh     
1:45:49     
sparcity of measurements and the differences between different cells even     
1:45:55     
when they have like in C Elegance when they have a very predictable Behavior they're not always the same we do have     
1:46:01     
some variation um so this paper on Separation on time scales is this time scale     
1:46:08     
separation paper michelis and menton's old idea still bearing fruit this is from Jeremy     
1:46:15     
gandera uh this is from 2014 says talks     
1:46:20     
about the michalis and Menton model this is model introduced to biochemistry the idea of time scale separation which a     
1:46:27     
part of the system is assumed to be operating sufficiently fast compared to the rest so that may be taken to     
1:46:33     
research reach a new steady state so this goes back to the paper we talked about before where we talked about you     
1:46:40     
know modeling things at different time scales is your simulation capturing the time scale of interest is it     
1:46:46     
undershooting or overshooting the time scale and so there are a lot of conceptual ideas out in the literature     
1:46:53     
and a lot of people who do a lot of work with deep learning or with machine learning I don't think they're     
1:47:00     
sufficiently appreciating some of that old work and getting you know kind of engaging with it so there's a     
1:47:06     
scholarship problem as well so in summary this person is     
1:47:11     
pessimistic uh but don't think I don't see the positive side of things uh they     
1:47:17     
their best best way to describe this approach or this set of opinions is short-term pessimist long-term     
1:47:24     
Optimist I favor using DML to complement the experiment not substitute it my     
1:47:30     
other hope is at the field we collectively stop focusing only on things that ml does well in biology like     
1:47:36     
correlations but start focusing and things we actually need to do like obtaining the right type of data and     
1:47:44     
using techniques that are more focused towards you know integration and things like that so this is an interesting     
1:47:50     
article uh and taking together with the other paper and taking together with     
1:47:56     
this paper the ugly baded good stories of large scale by molecular simulations I think it frames the challenges of the     
1:48:02     
field well so thank you for paying attention and I hope you learn something
