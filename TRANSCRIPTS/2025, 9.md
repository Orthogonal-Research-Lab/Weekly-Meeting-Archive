
Transcript
0:01
hello sir Hi how are you good
0:06
sir looks like you want to give an update or yeah so so basically I was uh
0:13
looking into a few things uh like I was talking uh you know two weeks earlier
0:19
probably so I was reading a couple of papers for uh you know this uh
0:25
biological inspired neural networks I've been seeing them and I have recently seen uh know this uh update for the G
0:35
and there are like two projects that have been opened up this space so so I
0:40
wanted to you know focus on the the two things the first is I wanted to see what
0:46
can I contribute to the project and parall read about the you know the biologically inspired neural network so
0:53
anyway the the motivation to do like the biologically inspired neural networks
0:58
that that anyway keeps going so so research is a long run so we have so much time for that so I wanted to do
1:04
that Pary but uh so in in this time line I wanted to focus on the you know the
1:11
the project part what can you know new things can be added into that space so I wanted to look at look into that too so
1:18
that could be know I mean like my initial side of things and uh probably
1:25
in like a two or 3 days I have uh read the paper that that was uh you know uh
1:32
was published so in the archive you have given me no that's the biological
1:38
inspired Network so it was 2017 paper like Ann and BNN like biological neur
1:44
networks so I I have seen certain literature from neuro computation that
1:50
paper and I have also seen certain literature from like other journals and conferences I wanted to add up into that
1:58
paper like few things got you know emerged into like like the recent line of neural networks like hypergraphs and
2:04
all and there are like certain unsupervised or self-supervised methods which actually do not need labels to
2:11
learn certain features there are so I wanted to you know extend the line of
2:17
work in that you know that paper so I've seen certain gaps that we can address
2:23
and improve that work on so I've I've I've seen some space where we can you know add some new things into that so
2:30
I'll probably share with you in a couple of days like what we can do in
2:38
that yes
2:45
yeah okay does anyone else have anything any updates they'd like to
2:51
share so yeah let's get started all right so
2:56
um first of all I wanted to give a congratulations to two people who have
3:02
been here in the group haven't been around for a couple years but
3:07
uh so this is mayuk and Mayock Deb their brothers and myuk is uh in the School of
3:16
Psychology at Georgia Tech cognition and brain science and
3:23
uh my knock I guess is still an independent contributor and they published this uh
3:29
archive paper on topon Nets and it's topon Nets High performing
3:38
vision and language models of brand like topography so my has been pretty active
3:43
in this space of uh you know machine learning and thinking deeply about like machine
3:49
learning and deep learning and you know he's I think someone interested in
3:55
biological neural networks being in a brain and nition and Marine Science
4:01
Program but he's he's done some pretty interesting writing on this so this is
4:07
uh this article let me zoom
4:13
in so I'll read the abstract neurons in the
4:19
brain are organized such that nearby cells tend to share similar functions AI models lack this
4:25
organization and past efforts to induce topography have often the trade-offs
4:30
between topography and task performance in this work we present Topo
4:35
loss new loss function that promotes spatially organized topographic representations
4:42
and AI models without significantly sacrificing Tas performance tole loss is
4:48
highly adaptable and can be seamlessly integrated into the training of leading
4:53
model architectures we validate our method and both Vision it's resonet 18 reset 50
5:01
vit and language models GPT neo1 125m nanog
5:07
GPT so they they basically have this method they validate it in these different models and they call it Topo
5:14
Nets topon Nets are the highest performing supervised topographic models that
5:20
date exhibiting brain-like properties such as localized feature processing lower dimensionality and increased
5:27
efficiency topon Nets also predict respon respes in the brain replicate the
5:32
key topographic signatures observed in the brain's visual language cores together this work establishes a
5:39
robust and generalizable framework for integrating topography and leading model
5:45
[Music] architectures advancing the development of high performing models it was more
5:52
closely emulate the computational strategies of the human brain so that's
5:57
his uh or the abstract and they're at this uh I guess
6:04
this is the myty lab this is myty the person running the lab
6:10
here as the third author so this is very much this sort of
6:17
comparison between biological and artificial neural networks neurons in
6:23
the brain are not tossed around haphazardly which is you know what they mean by that is they're spatially
6:30
organized if you look at a micrograph of the brain you know it looks somewhat
6:36
halfhazard at least at first appearance but there is a spatial organization that is occurs in the brain
6:43
just like in the embryo you have this spatial organization uh they're spatially
6:48
organized such that nearby cells perform similar functions topographic organization is a core feature of brains
6:56
meaning that you get different are regions that do different things things or you have different cell types in
7:01
different regions you don't find any one cell in any other region that sort of
7:06
thing in visual cortex this organization is evident in microscale pin wheel
7:12
patterns for orientation selectivity so this is uh in visual cortex where you have these pin wheel
7:19
patterns that organize for you know being allowing in organism to orient to
7:26
different places visually and they have this tuning function and orientation
7:33
selectivity occurs at macr skill uh allows for macr skill category selective
7:38
regions for faces bodies scenes Etc and then large scale organizational biases
7:44
for real world shape size curvature and animacy which means movement so there's
7:50
this there these sort of representations or I don't want to say representations as much as maybe
7:58
spatial uh uh structures or spatial orientations of collective behaviors of
8:05
cells that work towards this sort of uh functional aspect and the functional
8:12
aspect of course is for different things like different
8:17
objects and so you should expect a certain response from the network for these different functions so for face
8:25
perception for body perception for scene perception Etc and then these different
8:30
physic you know physical features in the image Beyond Vision in the language
8:36
cortex recent Studies have also identified clustering of neurons with distinct temporal integration Windows
8:43
unlike the brain most artificial neural networks or these a&n models lack any
8:48
kind of system systematic organization of units so this is where you know these artificial neural networks
8:55
are sort of periodic uh but also So Random at the same time so you don't
9:02
have very much structure you don't have this sort of you know these regions or these structures that yield some sort of
9:09
function based on their topography P so this is um uh what they're kind of
9:16
focusing on here in this work we introduced topal loss a new brain inspired inductive bias that can be
9:23
easily integrated into the training hos current AI architectures or Ann
9:28
architectur both convolutional and Transformers the resulting model topon Nets exhibit brain
9:35
like topography characterized by localized low dimensional and efficient
9:40
representations they'll show that in figure one inducing topography into artificial neural networks has been
9:47
fervent to be challenging two major strategies iner so this is in General
9:52
trying to it you know create the these sort of heterogeneous Anns so the first
9:58
is post talk topography where you have uh reorganizing units that fit into
10:05
these pre-train models using methods like self-organized Maps so you have these self-organized maps that play a
10:13
role in guiding learning especially free training so self-organizing maps are
10:22
kind of an older tool um from coonin and then developed later on and they're
10:29
basically basically these um uh these these unsupervised learning methods
10:35
where you have an array of cells and they kind of take on this topography
10:40
based on what what they've been exposed to or what they're learning so you actually have this pattern that emerges
10:47
from this specific uh training regimen or training data set that it's exposed
10:53
to so that's post talk topography basically where you don't Define a topography in advance and you let it
10:59
sort of emerge from the interactions the resulting Maps or the
11:05
resulting models exhibit topographic signatures but the underlying representations remain unchanged from
11:11
the original model so you have this sort of basic goal that you want you know
11:16
this representation and you try to match it and get these topographic
11:21
structures consequently the functional advantages of topography such as reduced
11:27
dimensionality and increased effic efficiency you're not realized the second strategy jointly optimized
11:33
topography incorporates an additional topographic loss during model train these models induce topography by either
11:40
one explicitly matching the brain spatial correlation structure two imposing distance
11:47
dependent constraints or three encouraging information redundancy so these are the
11:53
this is the jointly optimized topography approach uh these approaches suffer a significant can trade off so
12:01
they're better I guess than the uh self-organized Maps but and they're better than sort of
12:08
a homogeneous random network but there is this tradeoff so
12:14
the tradeoff is the ability of the model to learn task relevant representations so sometimes it's
12:20
compromised and sometimes you know you get really good performance but you also get
12:26
this uh this test relevancy
12:32
so they perform poorly on engineering metrics like performance on image net
12:37
for example and or show diminished capacity to predict brain de most prior work in the space has been
12:44
F focused exclusively on Vision models and the one attempt at imparting topography to language models focused on
12:51
self attention maps of Bert which is a a AI model it's a language
12:57
model uh which resulted an only modest topographic organization in the open
13:03
space so this is where you have this trade off between performance and
13:11
specificity and it's hard to kind of get a balance between them to summarize there is currently no unified strategy
13:17
to apply topography across common Ann architectures uh such as convolutional
13:24
Nets and Transformers and domains vision and language so you have these different
13:29
sort of architectures you have these different domains of learning and there's no strategy to
13:35
apply these across these common architectures and domains that can deliver High performing models together
13:42
with the functional benefits of topography so we're looking for the performance gains that we might get but
13:49
also this functional benefit so you know why use a topographic approach if you
13:55
can just use a sort of a random approach better what's the benefit of it okay so
14:01
figure one is here this is uh basically talking about topon Nets this shows
14:08
transformation from unstructured Baseline models on the left to organized topographic representations on the right
14:16
so you have these unstructured models on the left which are the Baseline models you have topographic Vision models on
14:22
the top such as confidence and Transformers this includes resonet 18s
14:29
resonet 50s vits so they're different types of Transformers different types of neural
14:36
Nets uh convolutional neuron Nets uh and this just shows that there's
14:42
no structure to it it's just that it's learning and then you have Topo Nets on
14:48
the right which are models trained with toppo loss rain like topographic organization so you have these regions
14:55
that are devoted to specific things and they're connect connected together like you would find in the
15:01
brain so that's the difference there as it's learning it's uh us it utilizing
15:07
the structure to improve learning or to constrain learning or hopefully to allow
15:15
for generalizing um and so I mean the goal would be to make it look kind of like a
15:23
mamalian brain basically or any kind of brain where you have regions which are most brains
15:29
um you have regions you know language centers or vision centers or things like
15:34
that and the information gets passed between these centers basically um and then on the bottom we
15:41
have topographic language models which are Transformers uh GPT Neo 125m and Nano
15:49
gpts on the left again we have the unstructured models which are these where you have no organization which you
15:56
have learning and then on the right of top graphically organized units which
16:01
have this again the structure and again you want to
16:06
achieve you know uh test in in variant learning and you want to have something
16:14
hopefully that integrates a lot of the multimodal information so ideally you'd like to have a structure that would
16:20
allow for both Visual and language learning instead of having a visual Vision model on language model um so
16:29
that's figure one and looking through the rest of the paper they kind of go through and Define
16:37
their cortical sheet so their first task was Define to define a two-dimensional sheet of cells
16:45
at where we could apply our topographic loss or topal loss algorithm to demonstrate that topal loss generalizes
16:51
across different domains we applied it to both language and Visions for language we train GPT neo1 125 M on the
17:00
Wikipedia data set and nanog GPT models on 10 billion tokens of the fine web edu
17:07
so this is where um we have Wikipedia data set and fine web edu those are the
17:14
the data sets and then the models are GPT Neo and nanog GPT uh for vision we train topographic
17:22
resonet 18 to allow comparisons with previous topographic models and resonet
17:28
50 bb32 models to further evaluate generalization of topal loss for larger
17:35
models and architectures All Vision models were trained on a supervised 1,00 W
17:41
classification task on image net together these languages and vision models allow us to robustly evaluate
17:47
toal loss so that's uh their defining their portal sheet then they Define the
17:55
cortical sheet in different in the different models so in the language models and vits you have this linear
18:01
layer with I input units and O output units we reshape its weight Matrix to a
18:07
cortical sheet so this is from W to C and this includes like
18:14
from output times input so the interactions there to H * W * D so H * W
18:23
* D H and H * W is the area of the sheet so it's the height and the width
18:29
that corresponds to the number of output units and the and yeah the corresponds to the number
18:36
of output units so it has this two-dimensional shape and then depth D
18:42
corresponds to the number of input units or I so basically you take o and I and
18:47
you transform it into this two-dimensional sheet with a depth uh to
18:53
maximize the number of neighbors for each element in the cortical sheet we chose H and W to be as close to each
18:58
other is possible so you want a square sheet or close to a square thereby minimizing the
19:04
perimeter each element in the cortical sheet now represents the weights associated with a single output unit or
19:11
a neuron in the original linear so they're making this transformation from the weight Matrix which has the weights
19:18
for each sort of you know this random sort of um stationary Matrix to this
19:27
structured non stationary Matrix in this way this sort of geometric way and you can imagine you
19:34
probably do this in different ways um this is not the only way to do in in
19:39
convolutional models or you know what we basically think of as Vision models for
19:45
a convolutional layer with C input input channels and C output output channels so
19:50
again you have that c parameter which is input and output output channels on a chronal size of K by K we project its
19:57
weight tensor and a cortical sheets so the we tensor is W the cortical sheet is
20:02
C where the area area corresponds to the number of output channels H * W and the
20:09
depth is defined as D which is equal to the number of input channels and times
20:15
the kernel size basically and in previous as in previous work rearranged the model units
20:22
dimensional kernels on a two di two dimensional cortical sheet more detailed explanation is provided later so that's
20:30
uh and then inducing Topography is where they do the total loss so the Second
20:37
Step introducing introduces topal loss to the reshaped cortical sheet promoting
20:42
topographic organization we achieved this by maximizing the cosine similarity
20:47
between the original cortical sheet and its blurred version so they blur the original cortical sheet and then I guess
20:54
they're going to recover and and look at the walls this suppresses the high fre frequency noise leaving behind only the
21:00
most important and meaningful information this idea was motivated by synaptic pruning in the brain which
21:07
eliminates noisy high frequency neural connections refining the biological Network structure although note we do
21:14
not explicitly remove any weights here so they have this uh this sort of the smoothing
21:20
process where they get rid a high frequency NOS the blurring of a 2d signal can be defined using a down
21:27
sampling function an upsampling function as follows so they have this blur um
21:34
equation and so they have the down sampling factors along height and width Dimensions to encourage smoothness in
21:40
the cortical sheet you maximize the cosine similarity between C and its blurred vision C Prime across cortical
21:47
sheet layer Maps this process smoothens representations and encourages
21:52
topographic organization the Topo loss is defined this way Al Topo in in this uh equation
22:00
here this tempal loss is integrated with the original training loss as follows so it's the training plus uh TOA Topo which
22:10
is the total um to is a scaling factor that controls the strength of the topographic
22:15
effect higher values encourage stronger topographic organization in the model so
22:22
again we're taking topograph toal loss we're integrating with the original training loss of the original l model
22:29
and we're doing it that way so you know they have this training
22:35
uh for vision in for uh language models and they specified
22:42
here uh we also have effective dimensionality uh which is this uh
22:49
Lambda I indicates igen values and the number of values so you have a number of
22:55
igen values that you're looking at for the dimensions this metric measures the spread of the Y
23:01
Spectrum resets we follow the procedure outline in with this reference here
23:07
resnets we chose 20,000 images from the imag net validation set calculated the
23:13
effective dimensionality for all the convolutional layers then for language models they did
23:19
something appropriate for that smoothed it they did this unstructured pruning
23:25
this down sampling they estimated the s AC ity which is where you have these layer-wise
23:31
features in response to stimuli and calculating it using a standard method for these
23:41
representations and so then the results are topon nets achieve High model performance with comparable spatial
23:51
topography and here is a graph here figure two Topo Nets achieve higher
23:57
model performance with comparable topography so let
24:11
me so this is figure
24:18
two okay so this is figure two this is uh
24:25
where we have the on the xaxis for both graphs we have the degree of topography we go from no topography to high
24:33
topography so [Music] smoothness uh decreases over space I
24:40
guess or I guess no smoothness increases because you're smoothing out the repr or
24:46
smoothing out the topography and then image net validation
24:52
accuracy in the Y AIS basically your validating your image
24:59
net data set higher is better so the higher we go on the graph the better so we have these two results for
25:06
vision and language models Vision on the left language on the right we see that we have these previous best topographic
25:13
models on the bottom and in in pink or in
25:21
fuchsia and then blue Baseline models on the far left which uh includes resonet
25:28
18 bit b32 and resonet 50 so you can see for these different to
25:34
values they plot them out on this functions you have uh these two that
25:40
kind of uh these two functions that can of go out and so you have topon net
25:48
resonet 18 topon Net v b32 then topel net resonant 50 at the
25:56
far right and so those are th those are the results
26:01
there um and then for language models we have the same thing with Baseline models
26:07
we have topon net GPT Neo 125m then topon net Nano gpts so you can
26:14
see that there's basically this trend towards High
26:19
topography and see let's see if we can figure out more about this from
26:25
Legend So the dash black lines indicate the parto curves for Resident 18 and resonant 50
26:32
models um and so the parto curves are these dotted lines
26:41
um the y axis here denotes the language model evaluation SP glimp the dash Gray
26:46
Line indicates the reported
26:53
topography so uh figure three is to tography
26:58
explains reductions and model dimensionality so this is where they have Vision models and language models
27:04
the vision models on top and then the language models down here so the vision models have they have this graph of
27:11
model performance which is imminent image net validation accuracy the percent correct on the left and then the
27:18
degree of topography or smoothness on the right and this is both in terms of the effective dimensionality or the IG
27:26
values so for vision you have these models over here model
27:32
performance uh increases youve resinet 50 on the far right resonet 18 a little
27:38
bit less uh accurate bb32 a little bit less
27:43
accurate than that um and then you have these towel values plotted out um at
27:50
different points um on the right we have these to values we have so that's model
27:56
performance we have the percent CR degre topography your bb32 resonet 18
28:03
and resonet 50 are all kind of on the far left with a low degree of
28:08
smoothness and that's a sort of a higher degree of Randomness uh so this is topography not
28:15
model performance explains reductions in dimensionality so the effective
28:21
dimensionality is high for bb32 and lower for the resonant models
28:27
in the case of language we have the same sort of thing we have model performance in terms of blimp accuracy and then
28:34
degree of or uh yeah degree of topography or smoothness on the right so again Neo 125m is uh very low
28:44
model performance nanog GPT very high model performance nanog GPT has a higher
28:50
effective dimensionality than the neo5
28:58
then for four measuring the efficiency of Topo Nets against Baseline models you
29:03
have these Vision Vision vision and language example you have resonet 18 resonet 50 and GPT new and
29:10
25m you have these fraction masked weights so this unstructured pruning
29:15
versus drop an image at accuracy from Baseline and then Topo Nets on the uh in
29:22
B on the very right that's where we have these language models and so
29:28
basically on the left we have the vision vision and language models in their point a in this figure is that topet
29:36
show greater resilience to L1 unstructured pruning exhibiting sparer weights and then this B example topon
29:43
Nets are more parameter efficient showing the example for down sampling
29:49
here where you're down sampling the toal layers um so yeah that's I'm not going
29:55
to go into any more data there but I think you get the point that the um that
30:02
there's they did a lot of work that they developed this technique for looking at
30:08
these structured networks and that they can test it for both the vision and the language models
30:13
and they get pretty good results with respect to uh their mathematical modeling
30:20
basically uh so one more thing
30:26
here so one more thing here there is a uh actually
30:33
is oh yes it is okay so one more thing here there's a a GitHub
30:38
repository uh tet's High performing vision and language models with brainl
30:44
topography I don't have the uh GitHub repository up as a active link but you
30:52
know if you go to their gith repository search for topon Nets you'll find it this isn't was an i CLR Spotlight for
31:00
2025 this is um the where they have the code they
31:06
have the implementation in pytorch they have some of the graphs here and uh yeah so check that
31:18
repositorio so I don't know if we had any questions or comments on that I probably has something to say about that
31:34
so why are we using GNN in this in this
31:40
project what was that uh I was asking that we had a
31:46
project for using GNN this year for G so can you explain that in more details why
31:54
are weor yeah oh well in our group we uh are
32:01
focused on graph representations of Developmental biology so we're
32:07
interested in the graph structure of different uh embryos or different
32:12
processes and move moving from that you can use graph Neal networks to analyze
32:18
data that have this graph structure so that's where we've been using this before we've also been
32:25
looking at this issue of uh topology in development because developmental
32:32
structures have a high degree of geometric complexity and so graph neural networks
32:38
were interested in sort of how to you know analyze that so yeah I don't
32:45
I don't know how else to uh you know I mean we have a number of different interests in the area um we
32:53
haven't really published anything specific to you know implementing graph Norm networks we have the hypergraphs
32:59
paper the hypergraphs preprint which is on the archive and I'm going to share that with you in slack I think it's
33:06
already been shared in the slide but um and we're preparing another version of
33:11
this from last year's uh work uh where we did uh
33:17
celegans analysis of the celegans embryo using hyper looking at hyper graphs and
33:24
graph Networks so could you please share the research
33:31
paper link of the toet that you just read out yeah explain us yeah I
33:40
can yeah thank you did we have any other comments or
33:47
questions on that paper on the tokon nuts
33:53
paper sir uh I have a small doubt regarding the not the paper but the
34:00
paper is really interesting so I uh do we have are these you know the the
34:07
project that was you know assigned or like whatever that's posted so do we
34:12
really need to stick with that no or we can know add some new things into that
34:17
space so do we have the Bound for that or are
34:24
we open to do something something uh new which can add up to that Oh you mean
34:30
like in the gso project yes sir oh well yeah the way I approach that as usually
34:36
you can sort of come up with your own like you work within the bounds of the
34:41
project description and you can do kind of whatever you want if you think it's relevant to the overall goal so you know
34:48
I don't know what you're thinking in terms of you know how to contribute to it I mean contribution is just making it
34:55
useful or making it better so I don't care if it's like involving different methods and how to get there you know we
35:03
had different approaches to it and they kind of coexist so um yeah I mean think
35:11
about like maybe how would I kind of
35:16
work you know work in the GNN space but like how would I make this useful to myself how could I make this useful the
35:23
girls of the group and you could you propose any sort of you know sort of project you want so I'm pretty
35:30
flexible sure
35:37
thanks all right so I don't I don't know if we have any other comments or questions so if not well let's move
35:45
on so this is a interesting paper we've had some discussions on synchrotron
35:52
x-ray tomography in the past and doing things with embryos
35:58
um we did some things I think with um axle autles and talking about different types
36:05
of approaches uh to this where you a synchron is a large um source of
36:13
electrons and they usually have them at some sort of super collider
36:18
facility and the idea is that you get these uh electrons they're highly
36:24
charged and they allow you to like image things at a very high
36:29
resolution and so that's why they're attractive to microscopist so this is a
36:36
microscopy oriented paper this is from pnas this is not uh Imaging embryos but
36:43
mesin brains so moleskin are of course are Marine vertebrates and they're doing this
36:50
functional mapping of the mesin brain and this is Guided by synchrotron x-ray
36:55
tomography so uh not familiar with this group but uh let's let's go through the
37:02
significance and the abstract and then look at what they're kind of uh maybe I
37:08
don't know if they have results in uh down further in the paper uh so the significance here is thanks to their
37:15
accessible nervous systems simple model organisms have provided some of the most
37:20
fundamental insights into how neural circuits generating control behavior however in many cases
37:27
understand standing is constrained by the absence of detailed brain M here we address the CH this challenge
37:34
using a synchrotron source to en enable rapid highresolution x-ray Imaging of
37:40
whole multi millimeter scale brains applying this classic mesin Model
37:46
uh Lia we construct a detailed 3D an atlas of its feeding circuit and use
37:52
this to guide identification and functional characteristic characterization of pivotal cell types
37:58
in the network our approach readily generalizes the CNS Atlas building and other model organisms enabling the
38:05
interrogation and sharing of brain structures across research groups or comparative and functional studies so
38:13
what they're doing is they're constructing this Atlas from these high resolution x-ray
38:18
images uh they're looking at this mesin model which is fairly well
38:24
characterized they build this 3D Atlas of a feeding circuit so it's not the
38:29
whole uh you know they get information for the structure of the brain but they're reconstructing this feeding
38:36
circuit and then putting it into a detailed 3D Atlas and then um yeah so
38:44
then their their idea is that their hope is that you can use this as a methodology for other types of neural
38:52
structures so uh see the abstract mesin brains are composed of
38:58
morphologically consistent and functionally uh in interrog neurons
39:04
offering Rich opportunities for understanding how neural circuits Drive Behavior nonetheless detailed component
39:11
level CNS maps are often lacking total neuron numbers are unknown and
39:16
organizational principles remain poorly defined limiting a full and systematic characterization of circuit
39:22
op so you know this is a you know we just talked about how you can use
39:28
topographic structure in artificial or you can use that principle in artificial
39:33
neural networks to give you maybe boost in performance or to give you something that you're missing in these kind of
39:41
stationary uh random networks and what they're saying here is that we we don't
39:46
really understand that that well in biological neuron networks we have to sort of kind of uh do a lot of Imaging
39:54
and figure out kind of from the basic structure what the information processing value of
40:00
that structure is so this is where we're trying to get at this here we establish
40:07
an accessible generalizable approach harnessing synchron x-ray tomography to
40:12
rapidly determine the three-dimensional structure of the multi- millimeter scale CNS of lemonade focusing on the feeding
40:20
gangola we generate a full neuron level reconstruction revealing key design
40:25
principles and revising C con estimates upward three-fold so with this high
40:30
resolution imaging they're not only able to get like more information about the
40:35
structure they're able to get a reestimate of the cell counts that in the brain so actually they've been able
40:42
to revise their cell count estimate three upward threefold um so that's interesting our
40:49
Atlas uncovers is superficial but also non-superficial ganglionic architecture
40:55
reveals the cell organization normally hidden regions so they have these you know when you have something in
41:02
microscop you usually acquire a stack of images you go through focal planes
41:07
sometimes things are included in those images because you can only get to the
41:12
IM the uh the things that you can you know basically access through some sort
41:20
of visual signal so sometimes we use fluoresence sometimes we use uh
41:27
different lighting techniques and so they had these ganglionic dark sides as
41:32
they say which are these hidden regions or cluded regions which are now visible from the synchrotron
41:39
approach this detail these detailed features of single neuron morphology
41:44
together guiding targeted followup functional investigation based on intracellular recordings so they were
41:50
able to reconstruct the circuit using this sort of visual approach and and
41:57
then they were able to confirm these cells and their cell types using
42:03
intracellular recordings of electrical activity using this approach we identifi
42:08
three pivotal neuron classes the first is a command like food signaling cell type the second is a feeding Central
42:16
pattern generator inter neuron and three is a unique Behavior specific
42:21
motor these together significantly Advance our understanding of the function of the classic control
42:28
circuit uh so they combin morphological and electrophysiological data they built
42:34
the CNS Atlas in lemon and this is so they have this I
42:40
don't know where their Atlas is we see they give it a link to it so let's see what if we had any
42:48
comments in the chat I think we had three okay uh dick says is there any CL
42:57
lineage Tre that shows Cal cells um I'm not sure if they have that
43:06
specifically uh I know that they have like the transformation from these developmental cells into
43:13
muscle cells but I don't know if they treat them in a way that shows like the sential aspect of um yeah I don't I
43:22
don't think that they have that specifically I think it's just people just have the standard lineage
43:29
tree but I don't know I have I haven't found a paper on maybe someone has written a paper on but not familiar
43:38
with so um so these are these synchrotron x-ray
43:46
Imaging uh so basically this is what they're acquiring so these are the images here
43:57
we have these different ganglia so in the mesin brain you have things organized around ganglia it's not like
44:05
the Mamon brain where you have these regions of Cortex and so they're they
44:10
look a little bit different so this is the pedo ganglia and you can see that they have these clusters here
44:28
let me get back
44:36
in so this is these are the images here it's a little unstable when you
44:42
have it zoomed this you have the pedo ganglion you have the cerebral gangion
44:47
here this is the 200 Micron bar so we can see the see it to scale you have the
44:55
parietal plus the VIS Al ganglia here we have the parietal plural
45:01
ganglia the poal ganglia and they have these annotations
45:09
and color coded so you can see where different substructures are so you can
45:14
see there's a lot of substructure in each ganglia and uh so the local ganglia
45:21
the raw images here on the left the annotated images here on the right then
45:26
you have these selecting annotated session sections that are sort of uh
45:32
exploded beneath that so you can see that there's a lot of structure in these
45:38
ganglia and they have these pretty nice images that are you know pretty clear
45:43
and you can see if you know what you're looking for you can see what's going on you can still see that like it's hard to
45:49
determine the cells if you zoom in and you do some uh image processing you can
45:55
get good cell from but you can see where you can have these uded regions in the brain of M so
46:04
that's uh that then we have this uh figure here that shows the
46:09
CNS of the mesk this is the meskan brain here the CNS the bual masses here you
46:16
have the nerves here you have neurons in these bual ganglia so these gangli are these large masses where they have
46:24
neurons and there these information processing centers they get very different from the melean brain and then
46:30
they connected the buom mass which is a muscle up here then you have the synchrotron
46:37
method so you have the synchrotron which is of course one of these uh usually
46:44
find them in Associated association with a particle accelerator so they're these large structures that generate these
46:52
electrons they turn them into x-rays they hit the s with x-rays they they put
46:58
it in a resin medium and they get the image so
47:04
they acquire the image here uh then this is an overview of the CNS reconstruction where you have these
47:11
ganglia and you have these uh neurop pill or nerve tracks I think we talked about neurop pill in a previous meeting
47:18
basically they're these tracks of nerves that connect the ganglia in insect
47:24
grains and in in vertebral so you get this sort of uh con connectivity map
47:31
here where you see all these ganglia this kind of the internal structure and
47:36
then these neural and then you have these
47:41
recordings from the different uh I guess they different neurons and they show you
47:48
know the electrical activity here so what they did was they basically got these high resolution images and then
47:55
they confirmed some of their data with these electrophysiological reords we
48:01
have figure two here which is nice because it shows these threedimensional models of lemonia feeding ganglia so
48:09
this is uh showing like this sort of orientation in space we have going from
48:16
the left side to the right side this is the dorsal face and you have this structure here um you have these
48:25
different orientations of Cl interior face the ventral face and the anterior face so you can see the kind of
48:32
resolution you're dealing with you also have the lateral face um and so this is
48:40
where they have this three-dimensional model with the feeding ganglia so this is this region of the brain where they
48:46
have this it performs this feeding function and so they've done a lot of
48:51
analysis on this this is a reconstruction model I'm
48:57
not going to play this but basically it's a movie that shows you that they rotate this
49:03
threedimensional model that they've reconstructed from the IM from the syncr image
49:11
data okay um so this is figure three this is a
49:17
class of command like feeding neurons identified with the S xrt Atlas this is
49:23
their Atlas so they're able or actually this is a different at and they're able to analyze their structures
49:33
here curious if they have a link to the atlas here that they've this is figure six which is this
49:42
x s xrt based Atlas with key cell types in the bual
49:47
ganglia um and so they kind of go through these different cell types where
49:52
they have a location in the feeding gangly in red and they have a name for the cell and so L8 or lb1 which is at
50:03
that red uh region at the top uh left of this image is the dorsal face motor
50:10
neuron they have a location activity the action potential looks like they have
50:18
these fictive feeding cycles and then they have this connectivity map here or
50:23
they have this connectivity map which shows the excitatory in in inhibitory
50:30
cells so this is where you have B1 and you have the uh it looks like they're
50:37
all excitatory connections coming in from OC cgc and n1m and then B1 the dorsal
50:46
face motor neuron has uh I think an excitatory connection to salv
50:52
salivary so you can see that kind of connection map in place
51:01
okay so um so they talk about their map our map
51:07
also provides a detailed view of the neuronal organization of ganglionic regions that have so far remain largely
51:14
unchartered thanks to the way the nerve that nerve bundles except the ganglia andless typically hide some faces from
51:21
view we refer to these as gang lanic dark sides which was mentioned in the abstract our Atlas allows us to examine
51:28
the detailed organization of neurons in these regions and thus inform functional targeting leading to the
51:34
characterization of a type of feeding motor neuron B12 notably this is an example of a
51:40
behavior specific motor neuron in the lemona feeding cpg or Central pattern
51:46
generator in this case recruited into the ingestion Cycles but not the egestion cycle so one function but not
51:53
another likely behavioral consequence of this differential activation is that swallowing actions or enhanced an
52:00
ingestion but suppressed an egestion preventing the animal from ingesting
52:05
potentially harmful objects and they talk about this xsr s
52:10
xrt which is an emerging as a powerful tool for mapping components of M Alan
52:16
networks but has not been exploited widely for interrogating the brands of simple model organisms we anticipate
52:23
that the approach we outline here will re readily generalize to various model systems with comparable brain sizes so
52:30
other mests crustacea anals etc for other animal classes such as many
52:36
insects where neurons are often too small for S xrt to are liable resolve
52:42
detail it can provide a rapid means to gain overview maps for example neurop pill organization fiber tracts Soma
52:50
clusters and other structural features so these are all features that you find in these uh invertebrate BRS um and
52:59
so these are kind of they're very hard to resolve using conventional microscopy
53:05
techniques since the data sets that it yields relatively small especially when compared to their nanoscale maps that
53:11
emerg from the extraordinary the laboring tens of elector microscopy based connectomics approaches the
53:18
generation of an accurate overview Atlas is rapid and easy to interrogate ating
53:24
comparative studies and detailed functional character ization so this is where you know we
53:30
might want to look at inter inter individual variability within a species so we might want to collect multiple
53:37
connectum for different organisms and look at the variation there so we might
53:42
have a hypothesis about whether something is always generated the same
53:48
way as as it is in C elegans and we've confirmed that with data uh that
53:54
basically the lineage train development lays out more or less the same morphology in every single organism if
54:02
they have a a defined mutation usually it has some systematic effect on that
54:08
phenotype and you know using this kind of Imaging you can find out in organisms
54:14
like Marin and vertebrates or insects you know if that also holds true in those organisms or if it doesn't to what
54:21
extent it doesn't so you know we can share Maps
54:26
across the scientific Community we can use GPU accelerated image sharing
54:32
platforms which allow people to manipulate the braw image Stacks so a lot of times we want to share the raw
54:39
image stacks and not just like summary statistics with their collaborator so that they can like you know look at
54:46
different uh different parts of the uh the image or of the structure and so
54:54
forth so this is a bit more powerful the pipeline that they're developing for
55:00
this um let's see if I can find these references up here they mentioned that
55:07
um x s xrt is an emerging is emerging as a powerful tool for mapping components
55:13
in a melean networks and they point to these references 15 16 and 68 so 15 is this uh uh reference by BOS
55:24
at all functional and multi M skill 3D structural investigation of brain tissue
55:29
through correlative invivo physiology synchrotron microtomography and volume electron
55:36
microscopy that's a nature of communications 16 is sample preparation
55:42
warping accuracy orative multimodal Imaging in the Muscle Factory bulb using
55:48
two Photon synchrotron x-ray volume electron microscopy that is by Jang all Frontier
55:55
cell and development biology so two papers where they're doing this uh synchrotron Imaging and then
56:03
68 is here uh this is also from bosal non-destructive x-ray tomography of the
56:10
brain tissue alra structure and that's a bioarchive preil so there's uh I think
56:18
sort of a momentum here for this type of approach and like I said again we've discussed this in previous meetings
56:25
where you know we we talked about doing this in different embryos and they're
56:31
not doing this in embryos they're doing this in brain structures but you could easily see how this could be uh modified
56:38
for looking at embryogenesis um you wouldn't have these functional Maps you wouldn't well you
56:44
might have some uh electrophysiology but you would definitely
56:50
have least you could probably work out when age trees from these from these data um so yeah that's that's in MKS so
56:59
it's not celegans it's a far different organism and it has like actually quite a bit more complexity than C
57:07
Elegance can someone generate the C Elegance on each tree sential cells I'm
57:12
not really sure if they can from the existing data sets that U that are out
57:19
there but I'll look into it I don't know
57:24
maybe considered we have any questions about the second paper hello hello Bradley am I audible
57:32
yeah oh yeah so yeah it's an so I was
57:37
going through the uh graph neural network stuff and from last year which
57:43
Baki implemented and I had a question which was uh did we uh did she train on
57:51
just one sample as in like one uh cell uh
57:56
uh or was there like a because I believe she said she did uh well she trained in one cagans I
58:04
think it's just kind of like you know the seans embryo is you have this data set where you go
58:11
from a single cell to I think she trained it up to like 60 or 70 cells so
58:17
it was like it wasn't the full data set it wasn't all development but she did train it upwards like you know something
58:24
where you had a pretty robust Network so I don't know and I don't know where like where her
58:31
training data is like I'm sure she has it on I I don't I don't remember I haven't looked through the get up
58:37
repository on that but uh we have training data so it's it's you know it's really kind of training it on these cell
58:44
centroids and then getting the algorithm to work but I i''d have to get more
58:49
information from her I think on this okay yeah so she particularly worked on
58:56
the uh spatial temporal aspects of the uh of like creating the network itself
59:03
of the graph itself so I was wondering whether like you could suggest some things related to like other learnable
59:11
parameters so uh I was thinking like whether like we could kind of consider
59:17
Gene expressions and uh cell cell interactions for like training like a different network yeah
59:24
yeah yeah we could look at I mean we've we've talked about that um
59:29
gen the thing about gene expression is of course you need to have a data set
59:35
that gives you a good gene expression for development it's kind of iffy I mean there are atlases out there but it's
59:41
kind of um it's hard to make that mapping but we could do it I mean we could get like
59:47
some select data and put it in yeah but I mean that's that's a
59:54
different data set and you need to train it um oh okay okay okay
1:00:00
yeah so I was like thinking that it was part of the same data set there just something like another part or like
1:00:07
another parameter that we could like try and model after like get better results
1:00:13
well you know often times you have to combine data sets or you have to like have one data set where you have cells
1:00:21
and then another where you have you know often times they'll have a gene expression for a single cell
1:00:26
or a gene expression for a single region so you just attach that onto that data set and then use that as like the
1:00:32
training data set and in the testing data set you have to make that split and then go forward you know you have to you
1:00:39
have to make sure you you're consistent in how you've uh integrated those data sets so that's the only cavey out
1:00:48
there oh okay yeah um
1:00:54
okay okay okay yeah thank you you're
1:01:00
welcome uh okay uh Jad draa said I am
1:01:05
not a slack group um yeah I think I remember mul saying something about this I can add
1:01:12
you yeah I'll add you into the slack Group after eting I'll send you an email I I have
1:01:20
administrative permissions for the slack so we'll see if I can do it all right thanks
1:01:29
all right oh yes this is uh this is yeah this is okay this is the paper what is this
1:01:36
paper this is the I think this is the paper
1:01:43
that yeah this is the Naro reesi paper this is the one that yeah what's
1:01:50
her name or U this is the name that we drew from for the gck project last year
1:01:56
so that's that's a good paper to go through if you're interested that's the neural developmental programs approach
1:02:02
so building the networks each cell is a neural developmental program and you're
1:02:09
building these networks that grow so that the cells are all I guess these ndps and they're generating this network
1:02:18
so you'll have to go back and read the documentation not only for last year's
1:02:24
project but back to their documentation for the forked stuff because this is a fork of that
1:02:34
project okay yeah
1:02:42
thanks uh there is not nothing else thanks for attending have a good week
