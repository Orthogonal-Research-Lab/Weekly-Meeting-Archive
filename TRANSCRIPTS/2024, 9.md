     
Transcript     
0:00     
yeah it's um APS physics um the March meeting um starting this     
0:07     
morning oh okay you going yeah I've got a virtual um BL     
0:17     
virtually okay so I've got at 11:30 biologies of cells and     
0:25     
tissues live streamed there's more I don't get to go to all of them like     
0:33     
there's right little full Sate all right yeah welcome uh yeah hi     
0:40     
Ved how are you hi thanks I'm     
0:46     
fine here's Muhammad mamed oh hello hello     
0:53     
everyone hello U would you like to introduce     
0:58     
yourself Muhammad yes uh I do research in swarm     
1:04     
intelligence and evolutionary computation and I've done some work on imag     
1:10     
construction all right great welcome he's also a master of bles he has a whole flock of back here oh     
1:20     
yeah it helps it helps with the students yeah posi students that for yeah you     
1:25     
need something to feed them yeah     
1:33     
all right uh so yeah heed wanted to give a talk here uh so this is Ved aumi and     
1:40     
he's been involved in openworm for quite a few years he's been doing a lot of things with uh ion channels and and     
1:47     
modeling there and um let's see what he has to     
1:54     
say okay thank you and hello everyone again uh we're going     
2:01     
to have it maybe somehow a weekly talk     
2:06     
on some uh progress on uh major     
2:12     
reconstruction work that uh represented by Dr Gordon some time ago so firstly I     
2:18     
asked him because uh actually he's an expert now in in in this project and the     
2:24     
book is where I think I asked him earlier that if he could start with the introduction     
2:30     
because I guess we had some introduction in the past but because since Muhammad     
2:35     
and maybe some nearly could you please have one or two     
2:42     
minutes introduction to the project because yeah these day actually since     
2:49     
I'm newly added the project I'm going to start with the IM     
2:54     
construction but before that we' be happy to have your okay okay here's the basic can you     
3:01     
guys see the b word yes yes okay it's the basic     
3:07     
prop okay Square I I     
3:14     
don't okay X okay now how do I get a square there     
3:20     
we go Square okay take a square     
3:27     
image you shoot x-rays into it let's see I go to get a line here you shoot x-rays     
3:34     
into it that's supposed to be a straight line the x-rays detector over     
3:42     
here okay this is what we're     
3:47     
after different in see in ordinary computed     
3:53     
tomography x-ray computer tomography you either use a fan beam in     
3:59     
two Dimensions or a cone beam in three dimensions in other words the     
4:05     
x-rays spread out throughout the patient and you hope to image some problem in     
4:11     
the patient now this means that ordinarily in CT you're irradiating a lot of normal     
4:21     
tissue okay what we're trying to do is minimize that so what we do is instead     
4:27     
of a two-dimensional f beam which would go like     
4:33     
this or a threedimensional cone beam which look like     
4:39     
that we're just going to send one line in at a time and the reason we can do that is     
4:45     
this is that we're using poly what are called polycapillary x-ray sources which     
4:51     
have been pretty well perfected over the last 20 years     
4:56     
they but it can you'll show the principal of the poly     
5:02     
capary okay yeah actually I'm uh trying to open your     
5:08     
presentation the the last one but yeah could you could you have the slides as     
5:16     
well because uh your slides is not very well in my computer no I don't have it     
5:21     
on this computer that would take that would waste a lot of time trying to get it up so if you want to show fine     
5:30     
okay I S I sent you the Powerpoint okay so basically what we     
5:35     
have is a line across the     
5:41     
Bas and then we'll aim at another angle and we got another     
5:48     
one okay now now we have an interesting problem here and that is can we     
5:54     
reconstruct the whole Square from a couple of lines okay or what is the minimum number     
6:01     
of lines that will allow us to do that oh there we go wow okay     
6:07     
uh what I call for next slide okay actually I'm sorry because     
6:12     
it's not very it's said PowerPoint Windows based one on on Mac it's not     
6:18     
very well well just just start at the beginning and I'll you for the next slide just a second I'm going to close     
6:24     
the door so I don't do the dog marking so     
6:40     
[Music]     
6:49     
okay go to slide number two okay so what we're trying to do is a variation on breast compression mamography now the     
6:56     
geometry of this is quite simple there's an array of detectors at the bottom and     
7:02     
a transparent plate at the top and you send x-rays to the top and onto the detector and you reconstruct the breast     
7:09     
in between now 25 cm is chosen as an industry     
7:15     
standard uh for this uh uh for these compression paddles so the breast is     
7:21     
compressed typically to about 5 cmet uh in this mammographic     
7:28     
apparatus okay okay uh the detectors typically have one1 of a millimeter size     
7:36     
so it's it's a very large array and the total size is to take the B biggest     
7:41     
breasts that are known okay uh so this machine will take any     
7:49     
size including when that fills the whole Space okay next slide     
7:57     
please okay so this is the new apparatus that is used it uh it it's an     
8:05     
x-ray Source the Box on the right hand side is the X-ray source with a fan to     
8:11     
cool it uh it focuses it on to a set of glass     
8:18     
capillaries so it's called a polycapillary x-ray source and uh the idea is to aim it at     
8:25     
any angle to this 20 in any position this 25 by 25 cm squar okay next slide     
8:35     
please okay this is the general principle the blue lines represent the poly capillaries the individual ones the     
8:42     
X-ray Source hits them they are heated so they can be curved and in this case     
8:49     
they come out parallel the x-rays come out parallel to each other and it's a very very small uh uh     
8:58     
angle angle of spread in this thing I think they get 0.004 uh uh degrees something like that     
9:07     
so they're effectively very parallel next slide please this shows the real polyc     
9:15     
capillaries if you look at it on the left you can't see them they're very     
9:20     
tiny they're about 10 Micron inside and if you magnify them then you can see them the individual     
9:27     
ones so it's a huge array of poly of polyc capillaries and the x-rays can be     
9:33     
bent because they do what's called total uh reflection on the surface of the     
9:38     
glass so these these capill just have air in them and the x-rays come out then     
9:44     
uh through these 10 m 10 Micron uh capillaries so that's the     
9:49     
basis of the polycapillary source next slide     
9:55     
please okay so what we're going to do is test this idea here we take a black and     
10:01     
white picture of a butterfly next slide     
10:07     
please and what we're going to do is we're going to extract Alias lines of     
10:12     
pixels across the picture now this is a preliminary to doing x-rays so here we     
10:18     
use the Alias version of of lines rather than the more common anti-alias straight     
10:25     
line which has great scales along it the anti-alias straight lines look better     
10:32     
the alas straight lines uh are either black and white or white so we use them     
10:39     
to select the pixels from the butterfly picture picture Okay along the line uh     
10:46     
you'll notice that the steps are not equal in size some are three here and some are two this is a good example okay     
10:53     
next please okay so here's a bunch of     
10:59     
parallel lines going from top to bottom representing the top of the top pedal to     
11:05     
uh top rest pedal at the top and the uh detectors at the bottom here we're just doing two     
11:11     
dimensions and uh I don't know if you can imagine it but you can barely see     
11:16     
the butterfly next slide     
11:22     
please uh okay if we do a very simple form of Vin painting uh which is just     
11:30     
interpolation between nonzero pixels in a row you can start to see the butterfly     
11:35     
even uh even with this kind of lines next     
11:42     
please okay now most in painting is done to remove lines for example this birden     
11:50     
is out gets out of the cage if you remove the cage and so in painting is     
11:55     
usually used to remove lines you don't want where what we're doing is the opposite we only have lines and we're     
12:01     
trying to fill in between the lines okay next slide     
12:07     
please so this shows uh let's see s is the Power of Two for the number of lines     
12:13     
we used going top the bottom and uh you can see that the idian distance to the butterfly is much     
12:20     
reduced if we use even our naive simple in painting Al next     
12:27     
please okay now here we try random lines again you can barely see the     
12:34     
butterfly next slide please uh to okay so here's a number of     
12:42     
line we we always add two lines which are the basically the columns on the     
12:47     
left and right side so it's 2 6 plus two so it's     
12:53     
faintly visible next SED if we do the in painting you can see     
12:59     
the butterfly okay next please okay and uh even with random     
13:07     
lines going from top to bottom you can see the ukian distance to the butterfly is much lower than just the lines by     
13:16     
themselves next please okay     
13:23     
so uh this is okay what we're faced with here is a     
13:31     
challenge if you if you try to search on CT algorithms you'll find thousands of papers for both fan beam and Cone beam     
13:39     
but you'll find absolutely no papers at all on what we call Ray byray computer     
13:45     
tomography no one has done it so we have a new A new challenge here uh to do     
13:51     
computer tomography one rate at a time okay next slide     
13:57     
please okay so this is just showing what the fan beam and the con beam look like     
14:02     
and uh and on the right is again our uh aiming a single Ray next     
14:10     
please okay now the problem well go go back one     
14:17     
slide go go back to the previous Slide the problem with fan beams and Cone     
14:23     
beams is that they irradiate everything in their path and so whether it's normal     
14:30     
tissue or the tumor it all gets radiated uh now the problem with breast     
14:36     
Imaging is that breast is very sensitive to X-rays and you can actually cause     
14:42     
tumors with it uh so what we're trying to do is hit basically we're trying to     
14:50     
image the tumors with minimum Imaging of normal tissue uh now I can make an analogy     
14:59     
do all is there anyone who doesn't know what a shotgun is no answers okay now shotgun shoots a     
15:08     
whole bunch of small pellets let's say at a flock of birds and you hope to hit a bird and uh and get get at least one     
15:16     
out of the flock and basically that's what uh fan beams and Cone beams do they radiate     
15:23     
they blast x-rays out in a whole uh sector or volume     
15:29     
and you hope to find the tumor in in there uh okay uh you all know what a     
15:35     
rifle is a rifle shoots just one bullet at time but you have to have it precisely     
15:44     
a okay so the uh the analogy here is that we're trying to turn x-rays     
15:51     
into a rifle Source instead of a a shotgun Source uh for the x-rays next     
15:59     
slide please okay     
16:06     
now uh before you go reading that it shows what a     
16:12     
focusing uh polycapillary Source looks like uh and it has a bunch of polyc     
16:19     
capillaries again but they've been bent so they come to focus now the advantage of this is that it puts a lot of X-rays     
16:26     
at the focal point and very few at the exit of the PO     
16:33     
capillaries so it will it can actually uh there's one     
16:39     
report of using a focus Source poly capillary source to kill a tumor uh in     
16:46     
in about one minute so it's a practical way of destroying or a blading     
16:53     
tumors uh uh by by focusing on them with     
16:58     
minimum damage to the normal tissue okay now obviously we need better     
17:05     
in painting algorithms uh possibly including machine learning algorithms which to be honest     
17:11     
are over my head uh we are what V has been working on     
17:18     
is what we call Image depend image process can you can you see me yes yes     
17:26     
now we see it okay okay uh so during the past two or three months actually I was     
17:33     
working uh uh maybe 10 to 15 hours per week uh on this project for uh seeing     
17:42     
how we can help with the image construction uh beside the past week     
17:49     
that's uh I was encountering a BG in in a python version     
17:55     
of shapely that's actually uh to a week of me to to figure out what was wrong     
18:01     
with with the shapely famous library in Python that was actually I was dealing     
18:07     
with lines and points uh but in general uh I I started     
18:14     
with a new in painting algorithm the P the previous one that Gordon mentioned     
18:20     
especially with with the method for generating a line uh based on uh previous uh     
18:28     
previously generated lines and their Pizza values there was uh     
18:34     
some actually the problem was that U these type of uh line generation usually     
18:42     
uh tries to find uh some new points or pixels which are which have actually the     
18:49     
risk of being sted in some uh uh     
18:54     
actually some points as you see here in the left and right most parts of the picture it could not find     
19:01     
any lines so uh the picture is not imp painted fully so I was trying to     
19:08     
work on firstly on on an painted a better maybe in painted algorithm uh I I     
19:15     
used open CV a very famous in painting algorithm uh in Python which which is     
19:22     
using uh nav STS algorithm uh the problem with uh     
19:28     
General general purpose in painting algorithm is that they usually     
19:34     
as Dr Gord mentioned they generally try to find something which is missed in a     
19:42     
picture so to imp paint it and uh recover the image so they usually have     
19:48     
the full image and some for example lines that try to remove those lines or     
19:54     
something like that but here we have uh a problem which which is wiers we have some lines and we are trying to uh     
20:03     
complete uh the lines between them uh so I I use the tweak somehow for using the     
20:10     
open CV uh I uh I converted a line     
20:15     
actually an image with some lines into uh a background image which is     
20:22     
everything within and the lines are now being considered as uh some new uh     
20:29     
actually uh image actually here the lines are image so here the black     
20:37     
background is going to be disappeared and replaced by uh some pixel values in     
20:43     
between uh as you can see here I could uh imp paint it using this algorithm     
20:48     
using uh actually the open CV and N it's some default uh the first try was that I     
20:57     
I actually I believe it could be possible maybe that we could have some masks from left to right as well because     
21:04     
here we we our devices only allow us to have some lines from top to bottom     
21:10     
but maybe in the future we could have uh this as well so I tried at the first time uh some lines from left to right as     
21:18     
well and got a better result with the Imp with the same algorithm in the past     
21:25     
some better results uh then as you can see the the image is much better than the previous     
21:32     
previously generated image uh but considering the available     
21:38     
um applian in devices I tried in the next tries I tried to have only     
21:45     
the bottom or top to bottom uh lines uh so here again I uh I tried to     
21:54     
use a new algorithm which is trying to find every     
21:59     
possible polygons within     
22:11     
the you cut out can't hear     
22:26     
you yeah I think we'll have to wait for him to get back online might have     
22:32     
dropped yeah let me explain something the uh we're sticking to the     
22:38     
standard rest compression geometry which is why we only consider lines from top     
22:44     
to bottom and the uh difference between     
22:49     
reconstructing this butterfly and reconstructing uh the X-ray image is     
22:56     
that with the butterfly we're extracting all the pixels along the line so we know     
23:02     
their individual values with x-rays to a first     
23:07     
approximation we won't know those values well we only know the average value along the line and so we need to both     
23:15     
reconstruct along the line and uh and do the in painting to get the rest of the     
23:23     
image so uh the goal which uh by he is     
23:28     
working towards is to try to find an algorithm for choosing the next line     
23:35     
that you aim across the patient and this the fewer the lines we     
23:41     
can use the lower the X-ray dose okay so that's what he's after now     
23:50     
when we get him back I don't     
23:56     
know okay so Hamed you can you can see that perhaps a buy approach would do both     
24:04     
reconstruction along the lines and reconstruction between the lines which would be the Imp     
24:10     
painting yeah this is interesting I'm trying to understand the kind of imp painting that B has used in in in his     
24:19     
work and um and if there is a way to use a fitness     
24:24     
function uh to see the the quality of the in painting     
24:30     
and we've been using just standard ukian distance between the inpainted picture     
24:38     
and the uh original um okay but but this is     
24:44     
something that you would do only towards the end to see the quality of the Reconstruction not through the process     
24:50     
you can calculate it time but but that wouldn't be that's uh that wouldn't be     
24:57     
realistic it would be a it would be an issue because obviously we don't have access to the to the original image in     
25:04     
reality with when doing x-ray but we do have access to the receptor's value so     
25:11     
that that is that is something that we have access to Yeah well yeah the approach we're using is uh you're right     
25:19     
you're right we don't know the original with the x-rays uh but if we can't reconstruct it     
25:26     
we do know it know don't have good ALG yeah yeah no exactly exactly so so     
25:33     
trying to see what the fitness function is based uh that that would be helpful so so when you're so basically when     
25:40     
you're projecting one line and uh you have part of the image     
25:47     
reconstructed is there is there another way of is there another way of comparing     
25:54     
so for example we can use the uh if if we if we take I don't know if you're familiar with I'm not very familiar but     
26:01     
if you're familiar with MRI we have the image space which translates to k     
26:07     
space and then then we can compare the the distance between the K     
26:13     
spaces space transer yeah yeah okay yeah     
26:19     
so so in a way we have we have a measurement where we can see what the     
26:24     
image similarity is but not by looking the image by looking at the case base     
26:29     
which is here the inverse trans I got you hav tried yeah we haven't tried that     
26:36     
so so if we have U if we use U if we use single Ray projection and uh we have an     
26:44     
image reconstructed instead of comparing the images if we have a way of saying     
26:50     
what is the quality of the Reconstruction without having to compare it against the original one then um it     
26:57     
it it would be it could be useful could and that and that would be     
27:03     
our fitness function and if we have that then uh we can use form intelligence and     
27:09     
evolutionary computation to to basically optimize the process okay okay except what do you     
27:17     
what do you use for your uh data set for     
27:22     
machine so I mean at the moment I'm using the     
27:28     
uh the sign noids which is uh basically what uh what we have let me see if I can     
27:36     
I don't know if I can you mean mean yeah the okay so yeah so you you     
27:42     
know yeah of course you know the the the the sinogram is the is the measure for     
27:47     
comparison um at the time being which is again exactly what we have access to and     
27:54     
we try to reconstruct from the sign okay so if we can have a measure along that     
28:00     
line then uh then we can basically yeah apply some of these methods with with not so much difficulty     
28:08     
and also without having to use the image itself which would not be accurate     
28:13     
because if because if we use the images then it would be very easy to reconstruct the image in a way because     
28:19     
you'll have um in a way your optimization would become unimodal     
28:25     
instead of multimodal which makes the problem much easier u in principle yeah     
28:30     
by the way the uh you're familiar with the mark algorithm     
28:35     
Mt uh I haven't come across it but I've I've seen it in your okay okay the mar     
28:42     
algorithm could do the reconstructions along the     
28:47     
lines okay because it's line it is based on lines so you could just confine it to     
28:54     
the lines that we know okay now there there     
29:00     
uh many ramifications of this simplest approach is just uh replace the pixels     
29:08     
along long line with the with their their average a line yeah yeah so so this is so this is     
29:16     
an imp painting technique but would we be able to use the cogram     
29:22     
uh style for the Imp painting because obviously you're projecting array and     
29:27     
then you're reconstructing along the line by putting the aage value or any other way then that could be that could     
29:32     
be a fitness value that could be a way of seeing how similar are we to the original I'm not used to that I've     
29:39     
always work in real space Sorry say again I'm not I've never     
29:47     
I've always worked in real space I've never attempted to use the sinograms okay so so so that's one thing     
29:54     
we need to find out how compare the images yeah I didn't say we know everything here uh what Ved has been     
30:02     
concentrating on is how do we choose is there an optimal way to choose the next     
30:09     
line yeah and this becomes at every step it becomes a picture dependent image     
30:15     
processing problem yeah okay yeah I yeah I don't     
30:20     
think I don't think it's a difficult problem but um I was wondering if um if there has been a choice on that because     
30:27     
once that he done then well I'm curious to we haven't unless he's succeeded     
30:33     
lately I'm not familiar with I tried I tried a very simple algorithm and it did     
30:39     
not produce better results okay so he's trying to find something better now one thing that we     
30:46     
have going for is that we're looking for small tumors which means they're around     
30:51     
2 to four millimeters in size and remember we're dealing with 1 0.1 1 mm     
30:58     
uh uh detector uh uh     
31:04     
pixels okay uh so a small tumor is likely to be more circular and we could     
31:13     
use a template approach also where we look for tumors that may     
31:21     
be showing up uh uh well enough so that they could be     
31:28     
located by a template yeah which is uh which is great because uh I think these uh fine tuning     
31:36     
Could Happen rather nicely towards the end once there is a solution created     
31:41     
having those uh uh prior knowledge yeah absolutely yeah okay are you ready to go     
31:51     
again uh do you have my screen or it     
31:57     
I got I got disconnected for a while [Music]     
32:03     
yeah must be Sandstorm in your     
32:08     
case that's somehow usual now over here we have lots of snow     
32:19     
outside okay did you ask anything from my side     
32:25     
because I I didn't have you for for one or two minutes oh you know we're     
32:30     
discussing uh Muhammad was suggesting uh other approaches to     
32:35     
reconstructing the images you don't know the original okay yeah there will be some     
32:42     
question I'm I'm curious to see what uh what what what you've got in your in your work and uh it's it seems like a     
32:49     
like an interesting work so we were just discussing some some uh uh components     
32:56     
around the work either uh pre or post and but we'll discuss that hopefully after your yeah for sure     
33:03     
because my next actually my next tries are going to be based on uh some de learning based algorithms for finding     
33:10     
for considering uh uh a range of uh pixels that every new line is going to be uh to     
33:18     
add to the to the previously generated uh or uh regenerated uh pixels so uh     
33:26     
that would be a a more complex one but uh the unique thing about about this     
33:32     
project is that uh actually in compared with other in painting algorithms we     
33:37     
usually in other in painting algorithms we have much more data from a pictures     
33:43     
we have usually we have all the pictures available and we are trying to do something else but here usually work     
33:50     
with sparse matrices matrices and so it's a little different we we have we     
33:55     
are trying to uh have as less maybe as possible number     
34:02     
of lines to find out find the best or the optimum image that could be     
34:10     
reconstructed uh I have one more point that I     
34:15     
mentioned somehow but I would need some answer from Gordon to for other words I     
34:22     
I will ask him later maybe but the point is how much because that was really     
34:27     
different for my Works how much a a WID of of a ray or of a beam uh effective     
34:35     
for for for for actually this one do you have any uh exact measurement of the WID     
34:44     
of a beam is that for example one pixel with wide or oh oh yeah for     
34:51     
now yeah for now we're assuming the beam is just one pixel wide but the real beam     
34:57     
can be millimeters or so wide yeah but the Diversion the what do you call it     
35:05     
the the spread of the beam is very     
35:11     
low in the worst case it's about two pixels okay so that would be really     
35:18     
different because because array with two pixels actually if we are considering     
35:24     
for example 64 lines and array with two pixels that is two multiplied by 64     
35:32     
lines one yeah but and that could be really make a lot of difference in uh     
35:40     
our measurements as well let me open the chat here I have     
35:46     
I I I will discuss it in detail later maybe you know because I I guess it's a     
35:52     
little detailed more and uh maybe it's not so much interested for others uh     
35:59     
could we have actually Muhammad what what he was uh what what the point was     
36:06     
was making and finish that to say I think I no no that's okay I think     
36:14     
V if it's U it would be great if you finish your presentation and then we'll get back to that because I'm uh it's not     
36:20     
it's not something so I don't know if you finished your presentation because yeah yeah yeah actually it was finished     
36:26     
and I I had some minor points that are something like what I mentioned right     
36:31     
now but the others are are more detailed words that we could uh discuss later as     
36:37     
well okay uh well let me just point out I wrote a paper many years ago on what I     
36:45     
called foxal and foxal are where you use a parallel set     
36:52     
of of beams that are grouped together and reconstruct from those instead of a     
36:59     
single line so we like having a bunch of parallel lines and it worked very     
37:06     
well that's all I can tell you it's okay and one more thing uh I I     
37:14     
guess very important I discussed it once before but uh I know that maybe Muhammad     
37:21     
has been deal with something like that in the past uh a very important     
37:27     
parameter here is or method is to be considered is the evaluation metrics we     
37:33     
we are we are here we are using ukan distances but when you have for example     
37:40     
something like 25 maybe thousand pixels or uh two 250,000 pixels we can say the     
37:51     
distance between them uh and especially the distance between the most prom     
37:56     
points is much more uh actually important for us to be evaluated     
38:03     
correctly because our matrices are usually sparse matrices and when we are     
38:10     
comparing an image with uh something that is simulated we we are actually     
38:17     
purposefully simulated because we are looking for something in the future like T or something like that so some     
38:24     
sections of them are much more important for us than the backgrounds something like that so uh the evaluation mat is     
38:33     
really important I guess uh I had some experiments in this one that only one     
38:39     
line could have something like one% uh difference in the uh in the     
38:46     
actually uh finally the final result that was really high so I guess uh we     
38:53     
should first of all uh agree on the distance measurements and the evaluation     
39:00     
criterias and then try to uh find the best uh actually the optimum algorithm     
39:07     
that we are looking for and this is a this is something I I I actually had a     
39:15     
lot of problems with during the past uh weeks of War so I guess that would be a     
39:21     
really important uh point to be uh agreed with other first     
39:27     
yeah happy to to to take that or dick would you like to say something to that or shall I shall I     
39:34     
uh this is a very strange form of computer tomography as I said no one has there's     
39:40     
no literature whatsoever on it that I can find okay so we have a chance here of     
39:47     
developing a whole new set of computer tomography algorithms the aim of which is lowest     
39:54     
possible dose to detect tumors okay now one thing that about the     
40:00     
lines is kind of curious suppose you detect a possible     
40:07     
tumor if you aim more radiation through it the impr dep picture of that tumor     
40:13     
should improve okay now it can improve in two different ways if there's a real tumor     
40:19     
there you'll start to see it if there's no tumor there it will faint to     
40:25     
backr okay so that's one that's one of the things about the another thing is     
40:30     
that since we're going top to bottom it means we the distribution of angles is     
40:35     
an nrow compared to Full Circle okay so we're going to have some     
40:42     
vertical distortions which can possibly be removed by     
40:49     
deconvolution okay uh we we've shown back in 1984 that for standard Compu     
40:56     
tomography if you deconvolute the image you get a much improved     
41:02     
image if it's the Ang range is narrow so that's a possible there are other things     
41:08     
that we need to consider that is that the breast can be characterized by     
41:13     
what's called plin noise let take that in     
41:21     
noise burlin noise is kind of strange but it seems to be character istic of     
41:26     
the texture of normal breasts okay so it provides the background against which we need to     
41:33     
detect the tumors and so we need we need to introduce peum noise and see IFH what     
41:40     
it takes to inject the tumors uh there are other things we can do such as increasing contrast of tumors     
41:47     
this can be done by using chemicals that concentrate in tumors or by using What's     
41:52     
called the K edge of iodine the K Edge means a sudden jump in     
41:59     
the absorption coefficient of x-rays between one one wavelength and another     
42:06     
so if we take two images with with our lines one below     
42:13     
this K Edge and one above it and take the difference between them that can show up between     
42:19     
the okay so there's there's a lot of interesting things to do here and as I     
42:24     
said also the foxal approach we be more realistic because we're dealing with a beam rather than a narrow line of     
42:32     
x-rays okay so we're trying to work up to the realistic     
42:38     
case yeah okay so uh back to your question in terms of the in terms of the     
42:45     
comparison and the ukian distance or or other measures there are other measures I think it's not that important at this     
42:52     
stage you can stick still to the to the ukian distance as a measure uh what what     
42:58     
I was uh uh what we were discussing when you were uh uh when your line dropped we     
43:04     
were talking about the the actual image similarity that you're trying to access     
43:11     
during the process so during the image reconstruction obviously you start with     
43:17     
projecting um one Ray and then uh you would start the image reconstruction and     
43:25     
you have obviously with one Ray you have an infinite or or near infinite possibility for a reconstruction yeah     
43:32     
and then you you uh you add in Project the second gr and then you try to limit     
43:38     
that possibility of the Reconstruction so you'll continue with that but in terms of these infinite or near infinite     
43:46     
possible reconstruction what would be your measure in terms of choosing which one is a     
43:52     
better so what what is how do you how do you pick the the high or better     
43:58     
reconstruction in terms of all these     
44:03     
possibilities uh actually the the only metrics     
44:08     
currently we are working with is uh is the cian distance and uh I'm I'm trying     
44:15     
to to add some pre-processing uh premade as well like     
44:22     
let me see if I can share my uh window uh I I've been using some thresholds for     
44:29     
for the because we are working with gray scale image as well I'm some this is is     
44:35     
somehow making some thresholds on the image uh after actually this is this is     
44:43     
something for for for the next level because in the next level we are working     
44:50     
with um with the supervis I'm trying to find out if it would be possible to work     
44:55     
on some supervised algorithm supervised algorithm we could find something like this if we consider these spots on the     
45:02     
wings of the butf uh similar to some tumors uh I could actually uh make some     
45:12     
pressing on on an image like this to find out how we could better deal with     
45:18     
those spots on the wings of the butterfly so similarly we could find the tumors as well in this way and uh more     
45:27     
specifically to how to concentrate on on on some tumors like these because when     
45:33     
we are generating regenerating an image or in painting an image we are more interested in in uh discovering and as     
45:41     
much as possible uh some some uh points like that some images some parts of     
45:47     
image like like this so uh I'm hoping in the future if we could have a good data     
45:54     
set we might be be able to uh learn from uh some uh real some realistic image and     
46:03     
then we could have some better supervised algorithms for finding for not only imp painting algorithms or imp     
46:11     
painting but also a data oriented imp algorithm in in a way that we could uh     
46:18     
actually generate uh the best possible uh image of of that sections of     
46:25     
the image that we are interest especially so yeah so so so on that on that what I was going to say there's an     
46:33     
Eco um what I was going to say is that uh when you are uh so the reason why I     
46:40     
ask you how you pick the best Reconstruction from an infinite number of possible     
46:45     
reconstructions is to find out what is your measure obviously you have the ground truth which is your original     
46:52     
image so so the original image you should not be uh you should not be using     
46:58     
During the Reconstruction process so there should be a different measure that would tell you how far you     
47:06     
are from an act from the uh from your target reconstruction if you use the original image it means you you have the     
47:14     
original image which means that there is no point in uh in the Reconstruction process so the intermediate process for     
47:22     
evaluating which reconstruction is better should be a different I can give you an     
47:28     
example I know we are going with this uh topic but here because we when we are if     
47:35     
I'm right Mr Jord Mr Gord please correct me if I'm right if I'm wrong when we are     
47:41     
actually when we having a ray or a beam the beam is going to be detect what is     
47:46     
in the way from the source to the detector so we have for example five if     
47:52     
we are uh if we have a uh     
47:57     
a vertical line for example from top to the bottom we have 512 pixels detected     
48:03     
by the beam so you have actually only one value so you don't have no we have     
48:09     
we have 512 values new values because the beam is detected on the bottom on     
48:14     
the detector part and it can approximate the value of the uh of the pixel that     
48:21     
the line or the beam has been detected so we have from a range of     
48:26     
500 multiplied by 500 possible pixels we     
48:32     
have with the vertical line we have 512 pixels detected now so every time we are     
48:39     
having a new line we we we have some uh values in the past that we we could     
48:46     
actually decide based on them mam maybe I should make something clearer the reason we're dealing with     
48:53     
the butterfly picture is to see can we reconstruct the butterfly from lines now     
48:59     
in this case we know all of the pixel values along each line and we regard     
49:05     
that as a subset of the problem of X-ray computer tomography because in x-rays we     
49:11     
only know the average of on each line at each pixel so in this case uh when you     
49:17     
project one Ray do you have access to the values of the uh of the     
49:23     
entire uh pixel sets that that Ray has visited yes yes we do in the butterfly     
49:32     
example in the real X-Ray example we only have access to the sum of the     
49:38     
pixels right so this is what I was trying yeah exactly so if if we have if we have a toy problem then obviously we     
49:45     
try to uh try to get as close to the um yeah to the actual uh scanning situation     
49:53     
which wouldn't be uh which wouldn't be a waste of time actually we would be on on target but uh so basically on a scanner     
50:01     
when you project one Ray would we be having one value at the end at the     
50:07     
receptor end okay yes so yeah yeah 12     
50:12     
yeah so so this is what I was trying to get to V that we don't have access to those pixel values in the in the real     
50:19     
world so if we assume that we have access to the 500 uh 12 pixel values     
50:26     
then we would be dealing with a different problem not the Reconstruction problem well we we regarded the     
50:33     
butterfly problem as a subset of the real problem yeah no the butterfly     
50:38     
problem is fine uh it it can okay let me can I use the Whiteboard just to go     
50:43     
ahead just to explain um okay so I I hope I can draw something and so     
50:52     
basically I I hope you can see the Whiteboard yes     
50:58     
okay so this is the so if we imagine that this is the U this is what we're this is what we're     
51:05     
trying to reconstruct so we have this object inside the square that we're trying to reconstruct and if we project a a ray at     
51:14     
the end here we have one value so basically if we assume that uh we have     
51:20     
this uh this is a butterfly basically and we have a ray that is projected from     
51:25     
from the top and then at the end we'll get a value here let's call that X so     
51:32     
this is the only value that we have access to rather than the uh rather than the individual pixels right in problem     
51:40     
in the real problem yeah so one way of seeing the Reconstruction that we are     
51:47     
close to the original reconstruction is to give these pixels certain values yes     
51:53     
so so we give these pixels C value so that the assuming that this tray is     
51:58     
giving us the sum of all the pixels along the way then we try to put these     
52:04     
pixel Val values here and then we compare it against the against this uh     
52:11     
the detector value at the end and then we will that's what the mark algorithm     
52:16     
will do yeah okay okay so so that's uh so so this would this could be the     
52:22     
measure right if you're if if you're if you're with as well uh so this would be the measure in term in terms of seeing     
52:29     
how similar uh the Reconstruction that we have is from with the original image     
52:36     
so basically we compare We compare this X and if we have uh so if we assume that     
52:43     
this is the original image and then here we have the reconstructed image right here and then     
52:48     
with the reconstructed image again we do the same thing and then we collect and we calculate the all the pixel values     
52:56     
and then we'll have let's say x Prime the way that we we can tell how     
53:03     
different the reconstructed images from the the original image is by taking the     
53:09     
distance between x and x Prime     
53:16     
yesam there's one one pro one problem here yeah and that is that the Mart     
53:23     
algorithm is iterative it drives x - x Prime to     
53:29     
zero yes yeah okay yeah so it's trying     
53:34     
to reduce that uh that distance is that what you're saying yes it reduces that distance to zero if     
53:41     
possible yeah which is which is what we want in a way for reconstruction I with     
53:48     
the with the caveat that we know that the system is under deterministic oh yes correct yeah but but this is is what     
53:55     
this is the measure that we have oh this just that's thean distance     
54:02     
between the two yeah and then we have and we have the distance between the two     
54:08     
uh the two single values rather than uh the pixel values now uh let's see can I     
54:14     
oh let's see stroke F stroke with oh boy oh hey here we go okay we have two     
54:23     
two algorithms I would recommend that's March and what's called     
54:41     
Power uh that's what I'd recommend the uh of course you haven't     
54:48     
done the infilling yet so the question is how do you do the infilling in an optimal     
54:54     
way when uh perhaps using Mark to get values along the lines then what do you     
55:01     
do in between the lines yeah what we are going what we     
55:07     
have been done so far is actually somehow interpolating or imp painting between lines that have been estimated     
55:13     
in the past using something like Mark or power Mark so we have something like 500     
55:19     
12 values estimated values for each which is actually estimated by by     
55:26     
an algorithm like Mar or power and after that we are going to decide what could     
55:31     
be yeah in between of those lines you see for for Mark suppose we     
55:38     
take another line like this where these two lines     
55:44     
cross pixel values should eventually be identical after iteration this is a     
55:52     
certain point yes absolutely yeah okay uh so that's what the M algorithm     
55:59     
will attempt to do that yes so so you have a number of     
56:05     
certain points so this one it would be a point I would call them well near     
56:11     
certain qu I I better estimated yeah and and then the all the pixel values along     
56:19     
the line where there is no intersection would be the estimation uh to with to     
56:24     
the best of our knowledge but again the the the only values that we have access     
56:30     
to are the values at the detector's end correct so so when you're doing the     
56:36     
interpolation what uh what I'm saying is that you should not access the pixel     
56:42     
value you should only access the detector value oh yes of course we haven't got actually yeah we haven't     
56:49     
gotten my assumption was we are doing uh after the the of the line of every line     
56:56     
so I wasn't going one step earlier which is which Mark is doing and is detecting     
57:04     
or estimating those pixels up on line I assume that we have some estimation for     
57:11     
every pixel of the of a line some estimation so after that what we are     
57:16     
going to do with in between those lines if if we are going to actually uh deal     
57:23     
with the with with the line estimation as well that that would be another problem again yeah so maybe here Dr     
57:31     
Garden could better uh make a point that if we are     
57:36     
going to to do the the first part as well or not or we are just doing the the     
57:42     
part after the mark estimation uh we have to keep making it     
57:49     
more more realistic so soon we give up the butterfly     
57:58     
and and what remember it's not you you can carry on using I'm sure that is     
58:05     
making a joke with the uh giving up the butterfly butterfly is a is a is an     
58:10     
example the only thing that I have uh uh concerned with is the fact that you do     
58:17     
not have access to the pixel value of the butterfly so so all you have access to     
58:24     
is the value at the detector's end so you'll have you'll have the detector's end and then you'll have the X the     
58:31     
depending on the the places where the projections being done you have access     
58:36     
to only this particular single value and based on that you should continue the     
58:41     
you should do the Reconstruction rather than so this would be your image similarity measure rather than comparing     
58:48     
the pixel value of the original image and the reconstructed image yeah by the     
58:53     
way let let me let me finish this here it that is correct but the real     
59:00     
problem is going to be threedimensional     
59:07     
yeah and uh this gets more more difficult but I think the principles     
59:13     
will work yeah it would apply it would get it would be more time consuming but     
59:18     
the principle is the same so yeah so in a way um and um so     
59:27     
you can have and and this is what what we've in what we've talked about in um     
59:32     
in our paper because we didn't want to lose the image um the image similarity     
59:39     
so we have two error measures which you can keep in your analysis right when you're doing the analysis that would     
59:44     
give you a very good Insight so one is the image similarity and the other one     
59:50     
is the similarity of the detector end value so the one that you use working on is the detector end value so let's call     
59:58     
this E1 error because this is the error that you you basically uh sorry E1 error     
1:00:06     
this is the error that your system has access to and on the side you can have     
1:00:12     
the image similarity uh distance you can call that uh ukian distance difference     
1:00:18     
between the reconstructed image and the uh and the original image E2 and then     
1:00:24     
you can plot them individually but not to use E2 During the Reconstruction     
1:00:30     
process only use E1 we can say from a actually machine learning point of view     
1:00:37     
the first one could be a supervis but the second one usually is an unsupervised algorithm that we are     
1:00:43     
dealing with uh well it it is supervised but uh the the supervised uh version     
1:00:50     
comes from the E1 uh value if you have which if you have a large number of     
1:00:55     
projections then you can you are guaranteed to find uh you're guaranteed to have an E1 and E2 going down together     
1:01:03     
more or less but if you have smaller number of projections then E1 and E2 will be very far apart you can have an     
1:01:09     
E1 equal to zero that is not representing of the original reconstruction maybe a real a real data     
1:01:17     
set could help could be really helpful because uh one thing I'm I'm not sure of     
1:01:23     
one problem I I mentioned earlier is the the width of every beam that that is in     
1:01:29     
detail we are usually dealing with some problems like this the the other problem is actually how many uh as you mentioned     
1:01:37     
how many uh beams in the real world how many usually beams are usually uh going     
1:01:43     
to be used so how many how much data we usually have at the end at the detector Point these are maybe some uh uh     
1:01:53     
questions that we realistic data set could be answered very well and I I     
1:01:59     
asked some many times from Dr G that if could be possible I could have access to     
1:02:04     
a realistic data set but uh maybe in the future we we we should have     
1:02:12     
something yeah the the only way to get real data is to take a dead body and cut     
1:02:18     
it up then you cut it you cut up this has been done actually there's a uh there     
1:02:27     
was a project oh gee a couple decades ago on reconstructing two prisoners who     
1:02:34     
died uh one man and one woman and they sliced them up and they     
1:02:39     
x-rayed all the slices okay so that's the only data that     
1:02:46     
I know that's going to be realistic yeah but from the from the technological from the actual research     
1:02:54     
pers perspective it would not be uh it would not be as important at this stage     
1:02:59     
having access to these data set if you if you assume each uh Ray to uh have the     
1:03:05     
width of one pixel that's more than enough because you can always change that line of code to uh cover two pixel     
1:03:12     
and and make some tweaks and then that would update the program so that's not important at all at this actually I'm     
1:03:18     
saying uh because I we need I need to actually have a have an ey     
1:03:24     
the real real data in the world so I could for example consider how many how     
1:03:30     
much data usually I have at the at the end point uh sector level how for     
1:03:37     
example as I said how wide could be align and something like this just     
1:03:43     
having an idea of the real world problem so maybe as you said it's not much     
1:03:50     
important for now having uh a complete data said but only for having an     
1:03:58     
idea yeah I if I can if I can interject here I think one easy way to go about it     
1:04:07     
is to assume that you are trying to project the smallest number of rays and     
1:04:13     
get the most accurate reconstruction yes so that is that is the goal so um if if     
1:04:19     
even if the real world data was would tell you you can you can project um I know a thousand rate and get the data     
1:04:26     
that is not the holy grave so you you'd like to reduce that to 500 if you can     
1:04:31     
reduce that to 100 then even better so try to reduce the raise to as uh little     
1:04:37     
as possible and get as good reconstruction as possible I think that's a that's that's a goal and um     
1:04:44     
yeah it's a it's going to be a research let me give you some context for this     
1:04:50     
the if you consider Woman as a whole only one out of eight has breast cancer     
1:04:57     
during their life okay so that means that if normal     
1:05:03     
women are coming in you want to give them at least the least amount of radiation which means the smallest     
1:05:09     
number of lines in in our case uh so that uh they get a very low     
1:05:17     
dose before the sent home okay okay all right yeah     
1:05:24     
I think we'll end it here um I want to move on to Hussein he has some things he wants to report on here all right     
1:05:32     
Hussein why don't you share your screen okay Muhammad perhaps you can if     
1:05:40     
we use if we use the Mart algorithm for reconstructing the lines perhaps you you     
1:05:46     
can help work with a heat or something or use voids to try to fill in between the lines yeah well uh back     
1:05:54     
uh swarm intelligence Evolution computation can be used even with this structure so for uh oh really for     
1:06:02     
imprinting because so this is what I was trying to get to when I was talking about creating an error that represents     
1:06:08     
the problem in the real world rather than the image similarity because uh the     
1:06:13     
Reconstruction can happen um in this form so it wouldn't be um the process     
1:06:19     
would not be uh would not be difficult the result will be unknown but uh     
1:06:24     
certainly they can be used and it would be a it would be a high dimensional problem but um I think um it's     
1:06:31     
absolutely worth um worth exploring and that's why I I I I talked about the E1     
1:06:37     
error and E2 error because this is where they would become quite relevant in terms of quantifying the Reconstruction     
1:06:44     
okay now we have I have a problem which I haven't been able to solve uh are any of you familiar with     
1:06:51     
pein noise has anyone worked with pearlin rise I     
1:06:57     
haven't worked with it I know a little bit about it okay the problem is there's one paper which claims there are three     
1:07:04     
types of normal tissues in breast and each is characterized by C noise with     
1:07:10     
different parameters and I cannot figure out from the paper which parameters go with which     
1:07:17     
tissue type right okay so have you tried right to the     
1:07:24     
authors they don't answer yeah that's it that gives us the     
1:07:29     
answer so it's it's not a well-written     
1:07:37     
paper are you ready excuse me yeah thank you thank you so much um right yeah hi     
1:07:43     
nice to meet you all my name is Hussein my name is s Hussein Aur uh I previously worked under Bradley through oral I um I     
1:07:50     
currently work um I I finished my Graduate Studies or I I used to I was doing my PhD at the University of     
1:07:56     
Toronto in medical Sciences particularly in computational neurosciences where I would analyze FM and EG to understand     
1:08:01     
the brain connectivity structure underlying schizophrenia as well as other disorders emphasizing BCI brain computer interface technology as well as     
1:08:09     
um more General uh excuse me more General applications of math statistics computer science to understanding neural     
1:08:15     
and neurological data during my undergraded years at a Bloomington I had the honor of taking classes under many     
1:08:22     
renowned my undergrad of physics so I did study under great professors like Robert deer is um one perfect example     
1:08:29     
and uh I at least had a few conversations with people like John begs Olaf sporns if those names ring bells um     
1:08:36     
so I'm coming to you right now to present with you a project that I'm working on um kind of semi under Bradley     
1:08:42     
but more so my my own independent research where I'd like to start making contributions to openworm and dorm um to     
1:08:49     
open worm into Divo worm this is part of a lot of remote contractual projects that working on here and there I can     
1:08:55     
also just let you know this fall I plan on actually going back to school to get my masters in data science part-time through Georgia Tech to really really     
1:09:02     
refin a lot of those mathematical and computational skills more so as they can apply to like very very highend careers     
1:09:09     
in data science AI machine learning uh etc etc so um right um as I understand     
1:09:16     
or excuse me as I understand there are many there and and once again I do have     
1:09:21     
a meeting in about 15 minutes that's why I am trying to speak with but I do want to you know I do want to leave the room     
1:09:27     
the room open if people want to CH into um excuse me husin how about putting     
1:09:33     
your email address in the chat sure nice to meet you all nice to meet you all     
1:09:40     
uh and uh you guys should follow me on Twitch because I I stream my research on     
1:09:47     
Twitch and it's really cool I'm trying to get twitch off the ground for streaming science and programming for those of you who don't know twitch is a     
1:09:53     
platform mostly known for people streaming video games but I got featured     
1:09:59     
a several years ago when I said hey why don't we use this for streaming science and it was such a cool idea that they featured me in Science magazine and this     
1:10:06     
is where I'm showcasing my journey into public source and open source and I've even met a few other scientists here and there who are starting to use twitch too     
1:10:12     
so if you want to see what I'm doing in real time um feel free to stop by whenever I'm live or um like if you make     
1:10:18     
a twitch accounts uh and you follow me it'll notify you whenever I'm live but yeah thank you so much right now so I     
1:10:24     
had pitched a few ideas to Bradley uh some potential ideas for projects to work on um and ultimately let me excuse     
1:10:31     
me I need to bring this up all right my computer always gets so slow when I'm streaming video let     
1:10:36     
me uh okay um excuse me all right uh thanks okay so     
1:10:44     
I'm just turning my video off because it makes my computer easy you know my computer gets a bit slow sometimes but there were two potential ideas I'll I'll     
1:10:51     
paste uh paste the summary into the chats really quickly two potential ideas that Bradley had given some good vibes     
1:10:58     
towards like some uh like I I pitched some potential ideas of what it would be best for me to focus     
1:11:04     
on right now regarding the projects and two two excuse me two of them stood out     
1:11:09     
in particular um uh these two I'll post in the chats     
1:11:17     
okay data fusion and integration yeah I was posting in the chat thanks guys no um video video is just off because does     
1:11:23     
make my computer a bit slower when I'm when I'm streaming too um yeah data fusion and integration and adaptive     
1:11:30     
simulation control with a few uh broad broad ideas and general General ideas on     
1:11:35     
how I could contribute uh in those ways I also do have access to VR AR and VR     
1:11:42     
technology too I've tra I have some practice with that if we wanted to really go virtual with the virtual worm     
1:11:48     
and push it to VR environments um but anyways any sort of feedback of which of these these ideas would be best for me     
1:11:55     
to focus on given the time given the time and energy and efforts and resources I can offer any sort of feedback would be greatly appreciated so     
1:12:02     
thank you nice to meet you all oh yeah so yeah that that's good I think like I     
1:12:08     
said I think the data Fusion in integration project would be good because we don't do I mean we've done     
1:12:14     
some integration of the data that we have but we have a lot of data sitting out there so we have uh a lot of     
1:12:20     
microscopy data we've done machine learning on that we've done we have electrophysiological     
1:12:26     
data through openworm and I know that Ved is interested in getting some of that for     
1:12:31     
developmental uh stuff so you know being able to integrate that is always a good way to you know find find a new paper or     
1:12:39     
find a new sort of spin on things because there's so much bioinformatics     
1:12:44     
out there it's just putting it together it's really the challenge so I think that's probably the best way to do this     
1:12:50     
and of course we have people who are doing like machine learning and and they're always looking for data sets so     
1:12:57     
I mean you know the but there is one challenge to data fusion and that is getting the right context so if you know     
1:13:04     
you have like you know comparable things not apples and oranges so like for     
1:13:10     
example if I had microscopy data on embryogenesis and I had this process of     
1:13:17     
embryogenesis I would want to find data say like uh electrophysiological data that I could map to different stages of     
1:13:25     
the of development so I can't just throw them together I have to kind of make uh     
1:13:31     
I have to kind of make a framework for that right and especially solving that problem is a difficult problem Central     
1:13:37     
from mathematics to areas of neuroimaging Neuroscience and bi formatics too like how can we um how can     
1:13:42     
we uh how can we produce the images that we want to produce from the data itself but     
1:13:48     
more specifically um like like a like FF fft is convolution under RS um in what     
1:13:55     
way can we use them in the most practicable manner to yeah yeah and then adaptive simulation control so that's     
1:14:03     
interesting uh I don't know you know reinforcement learning algorithms would probably be a lot better than like     
1:14:09     
standard machine learning for that I mean at least in my opinion um and then of course there also like different     
1:14:16     
control algorithms that I don't mean that if if you want to get into that that would take a lot of sort     
1:14:22     
of training and yeah so I mean we might try with you know uh reinforcement     
1:14:28     
learning algorithms and I think we've been talking about that a lot in my other in the other group that we do um     
1:14:35     
and you know it's like there's a lot it's very hot actually as a topic people     
1:14:41     
are really kind of trying to drive reinforcement learning forward so that might be interesting with respect to     
1:14:48     
like some of the openworm simulations that we have now so like um you know we have the 3D worm     
1:14:55     
which is a browser and we have cybernetic which is like a biophysics simulator so you can see the worm like     
1:15:03     
crawl through different media and you know you're just simulating the physics of the media and     
1:15:09     
the physics of the worm so that's that's all that is but you don't have like a brain so you know that's that's where     
1:15:15     
maybe reinforcement learning comes in they're they're pretty smart despite not     
1:15:20     
having a brain yeah well they have a brain but it's like you know the simulations don't have any brain they're     
1:15:27     
just kind of uh you know physics interactions I mean right I I me more so     
1:15:33     
the the source of a cognition by the through of their networks versus a you know yeah yeah yeah yeah but I mean like     
1:15:42     
I think yeah I think the you know and then like with reinforcement learning and I wouldn't jump into that like try     
1:15:49     
to be as sophisticated as possible right away I mean you'd want to have like maybe started with a very simple Pro     
1:15:56     
problem like for example you know like with brenberg vehicles which is something they use in cognitive     
1:16:03     
science uh they model photo taxes which is where they have an agent and there's     
1:16:09     
a light source and then the agent has to navigate towards the light source it's     
1:16:14     
just kind of the the way that the internal neural network is set up in the bright andberg vehicle it can recognize     
1:16:22     
light gradients and so it just moves towards and so it's not really intelligent but I mean it has like this     
1:16:29     
you know has this preference for a certain uh part of the gradient and so it moves there it's kind of like a     
1:16:34     
single cell moving towards like a chemical gradient uh one part of a chemical gradient so this is some it     
1:16:41     
would be a very simple problem to work on you could like Kind of Perfect the algorithm or see what kind of algorithms     
1:16:47     
people used for something like that and then move from there so I think you know breaking the problem down into like     
1:16:53     
these very simple toy problems absolutely I'd also like to strongly emphasize my experience I've gained     
1:16:59     
through blender unity and Maya um we Jesse py and I were kind of very briefly last night talking to slack actually on     
1:17:06     
how AI researchers are embracing 3D they're all talking about how they're taking advant of 3D software for their     
1:17:12     
simulations for understanding say like reinforcement learning agents and um in 3D spatial environments and um you know     
1:17:19     
moving to platforms like that and being able to being able to take advantage of those software would be ideal as well so     
1:17:28     
yeah um once again I I have a few minutes guys any any feedback is     
1:17:33     
appreciated feel free to email me with anything I'm on the dev the openworm slack Channel too so yeah yeah yeah well     
1:17:41     
thank you very much husin I just wanted to get that in um yeah and so yeah any questions for us any final questions     
1:17:49     
thoughts criti critiques yeah so my my thesis supervisor at     
1:17:55     
University of Toronto was John gitz he's an expert in neural Imaging and cognitive science and we would work on     
1:18:01     
the human brain so um a couple summaries ago I got involved through oral through     
1:18:06     
Google summer of code and um have since been you know uh looking for ways to get involved more more thoroughly so thank     
1:18:12     
you all yeah thanks nice meeting you     
1:18:18     
yeah so the final thing I'd like to talk about today is uh there's been some     
1:18:23     
progress in the past week or so on Foundation models for biology and so Foundation models I I've     
1:18:32     
really had trouble trying to get my head around what exactly a foundation model is but I'm going to go through the     
1:18:38     
Wikipedia page on Foundation models and then go through some of the New Foundation models that have been     
1:18:44     
proposed and then an Institute that's working on this problem so the Wikipedia page for foundation model defines a     
1:18:52     
foundation model is a machine learning model that is trained on Broad data that it can be applied across the wide range     
1:18:58     
of use cases so this is uh I think we had a proposal a couple years ago on a     
1:19:05     
general model a general biological model in the group and the idea was to train     
1:19:11     
it on a lot of different types of data and be able to use a model to go across     
1:19:17     
different types of microscopy data from uh C elegans to human to     
1:19:23     
whatever so that's kind of you know this sort of generalized model usually     
1:19:29     
machine learning models require very specific training on one data set or another data set but a foundation model     
1:19:36     
the way it's being proposed should be applicable to a wide range of use cases     
1:19:42     
Stanford actually proposed one a while back uh their human centered Artificial     
1:19:47     
Intelligence Center uh they they were the ones that popularized this term and whether it's just a matter of creating a     
1:19:54     
term for people to use and then you know overselling something I'm not really     
1:19:59     
sure but this is the way it's kind of coming together so Foundation model is very broad models this broad class of     
1:20:06     
models so early examples of foundation models were language models like Google's Bert and open ai's GPT n Series     
1:20:15     
so gpt3 gp4 and so forth Beyond text Foundation models have been developed     
1:20:20     
across the range of modalities in including Di and Flamingo and music gen for music and rt2 for robotic control so     
1:20:28     
there are a whole host of different types of foundation models that we might be able to use uh in different domains     
1:20:34     
there's actually a part on the Wikipedia page on definitions of foundation models     
1:20:40     
we're looking at the Stanford Institute for human centered artificial intelligence they have their Center for     
1:20:46     
research on Foundation models they coined the term Foundation model in August 2021 to mean any model that is trained     
1:20:53     
on Broad data that can be adapted to a wide range of Downstream tasks where     
1:21:00     
they synthesize this from pre-existing terms large language models were too narrow given the focus is not only     
1:21:06     
language so basically it's a large language model but it's including other types of data and other types of     
1:21:13     
function self-supervised model was too specific for the training objective and     
1:21:18     
pre-train model suggested that noteworthy action all happened after pre-training they wanted to emphasize     
1:21:24     
the intended function of the model which is where you know we want to look at the     
1:21:29     
function over something like the modality or the architecture or the implementation well all of those three     
1:21:36     
things are important the function is the primary goal in a foundation model so     
1:21:41     
the US government has a definition they've defined a foundation model this is the executive order on safe secure     
1:21:49     
and twor the ai ai model that is trained on Broad data generally using     
1:21:55     
self-supervision contains at least tens of billions of parameters and is applicable across a wide range of     
1:22:04     
contexts uh the AI Foundation model transparency act which was proposed in     
1:22:10     
2023 defines a foundation model as an artificial intelligence model trained on Broad data generally uses     
1:22:18     
self-supervision generally contains at least 1 billion parameters and is applicable across a wide range of     
1:22:25     
contexts exhibits or could easily be modified to exhibit high levels of     
1:22:30     
performance at tasks that could pose a serious risk to Security National Economic Security National Public Health     
1:22:37     
or safety or any combination of those matters and then of course the European Union has their own     
1:22:44     
definition which is an AI model that is trained on a broad scale or on Broad     
1:22:50     
data at scale is designed for generality of output and can be adapted to a wide range of distinctive     
1:22:56     
tasks and the UK has the even another definition which is a type of AI     
1:23:02     
technology that is trained on vast amounts of data that can be adapted to a wide range of tasks or operations so I     
1:23:09     
think you can see the commonalities between the definitions Foundation models constitute a broad shift in AI     
1:23:14     
development foundation models are being built for astronomy Radiology genomics     
1:23:20     
music coding and mathem itics so the genomics one is this reference 10 and     
1:23:27     
that's this uh gen slm's genome scale language models reveal sar's kv2     
1:23:33     
evolutionary Dynamics this is a bioarchive article that talks more about     
1:23:38     
how they're applying foundation models to I guess rology and     
1:23:44     
genomics uh so they their aim here is we seek to transform how new and emergent     
1:23:50     
variants of a pandemic causing viruses specifically SARS K2 are identified and     
1:23:57     
classified by adapting large language models or llms for genomic data so you     
1:24:03     
know you can build a large language model for something like English we know that that's what GPT uh 3 and gp4 do you     
1:24:10     
can build large language models for languages like Swahili other smaller     
1:24:16     
languages that you can use to build large language models for so a lot of     
1:24:21     
Native American languages for example where they're trying to uh preserve the languages you could use a large language     
1:24:28     
model to uh characterize those languages and generate text uh you'd have to train     
1:24:34     
it on a specific data on a specific Corpus and you can do this in parallel with something like an English olm that     
1:24:41     
would be able to translate terms and things like that but you can also do this with genomics data you can do this     
1:24:48     
with proteomic data um and you can do it with VNA so we have sequence data and we can look     
1:24:55     
at the we can look at RNA sequence data as well and look at the structure of the data and we can generate sequences which     
1:25:02     
is the crucial part of this de generative model that we can use so uh     
1:25:08     
by adapting llms for genomic data we build genome scale language models or gen slms which are genomic slms uh which     
1:25:18     
can learn The evolutionary landscape of SARS K2 genomics so they're actually proposing that this     
1:25:24     
evolutionary landscape which is something that in evolutionary biology we're very interested in because we're     
1:25:31     
interested in all combinations of a genotype so if we want to look at the uh     
1:25:37     
evolutionary landscape say of seans we might take the seans genome and     
1:25:44     
we might put it in the context of all combinations of that genome now needless to say that's a very     
1:25:51     
highly intensive computational task but um you know that's that's     
1:25:57     
something we can then explore and find not only different you know where different variants are placed in that     
1:26:03     
landscape but also latent variants that maybe don't exist in the world where we     
1:26:08     
can't observe them directly so we can simulate them and simulate paths from     
1:26:13     
one genotype to another so in this case we have SARS K2 genomes which aren't very big so we can     
1:26:21     
actually simulate this probably quite effectively in an llm and then the idea would be you'd be     
1:26:26     
able to generate with a prompt you'd be able to generate a certain genotype by pre-training on over 110     
1:26:34     
million procario Gene sequences and fine-tuning as SAR as CO2 specific model     
1:26:39     
on 1.5 million genomes we show that gen slms can accurately and rapidly identify     
1:26:46     
variant of concern uh so you know we can actually prop this with different things we have     
1:26:51     
this basically this database of all these sequences that we can look through     
1:26:57     
thus to our knowledge uh jlms represent one of the first whole genome scale Foundation models which can generalize     
1:27:03     
to other prediction tasks we demonstrate scaling of gen SL slms on GPU based     
1:27:08     
super computers and AI Hardware accelerators utilizing 1.63 Zeta flops     
1:27:14     
in training runs with a sustained performance of one 121 pedop flops in     
1:27:20     
mixed precision and Peak 850 pedop flops uh we present initial scientific     
1:27:26     
insights for examining gen gen slms and tracking evolutionary dynamics of sarscov2 having the path to realizing     
1:27:34     
this un llarge biological data so these are very big data type things um so     
1:27:41     
you're able to scale them up so you know you can work on a a very large space of     
1:27:47     
uh viral phenotypes or viral genotypes I guess they'd also be phenotypes in the this case but you'd also have um the     
1:27:55     
ability to build larger models for maybe something like seans but the memory     
1:28:01     
requirements are going to be pretty substantial so that's one example of a genomic Foundation model and so This Is     
1:28:07     
How They Define pH Foundation models after a foundation model is built it can be released in one of many ways there     
1:28:13     
are many facets to a release the asset itself who has access how access changes     
1:28:20     
over time and the on use so these are all things that we worry about in open source models and and releasing them and     
1:28:28     
managing how they're used and managing sort of the the different parts of the     
1:28:33     
uh model so we don't want to just uh open it up you know to anyone we want to be     
1:28:39     
able to control some of the aspects of it we also want to control the training data so that's a very important part of     
1:28:45     
it um especially with like medical data but also with just biological data um     
1:28:51     
you know it's hard to uh keep the model relevant if uh you just kind of train it     
1:28:58     
on anything so we need to have sort of this auditable system of saying these     
1:29:03     
are the data sets and the model this is what we have uh that we currently training it on you know if we specialize     
1:29:10     
it in any way we need to be able to record what those data sets look like and I think in the meeting today we saw     
1:29:16     
that we have needs for sort of understanding what our training data looks like much effects on results so     
1:29:23     
when a model is released via an API users can query the model and receive responses so there's that querying     
1:29:29     
aspect it's some sort of text query and then we end up with something coming out as a result uh but we cannot direct     
1:29:38     
directly access the model itself so we can't see the weights necessarily if we're using an open source version of it     
1:29:44     
but we can do you know use a query it's kind of like the way a lot of larger language models work uh online if we we     
1:29:52     
run it online you know we might enter a query into a website we might get a a a     
1:29:57     
result maybe like if you're using chat GPT you might get a a block of text you     
1:30:03     
might get some uh images from something like stable diffusion but you can't see what how the way it was assembled and     
1:30:10     
what the weights were in the model and so forth so this is you know of course the sort of the limitations of one of     
1:30:16     
these Foundation models is you can't go in and under the hood and inspect what's going on or even you know modify it as     
1:30:23     
you need to now you can run it on your own machine or your own Hardware but the problem is of course is that you need to     
1:30:30     
have the right you know you need to have sufficient computational power and so a lot of times these models are hosted in     
1:30:36     
the cloud so that's a limiting factor for a lot of research     
1:30:42     
groups uh so some open foundation models are gp4 Palm 2 llama 2 and     
1:30:48     
mistra uh so these are just different General Foundation models large language     
1:30:54     
models um and so you know the thing is is that when uh open foundation models     
1:31:02     
uh well okay so actually Foundation models the pro one of the problems we have is     
1:31:09     
that because they're so General people can misuse them a lot and so with biological models it's an interesting     
1:31:16     
problem um on the one hand you have access to these kind of you know highend     
1:31:22     
models you can put in images and you can get out a result but on the other hand     
1:31:27     
if it's not transparent we don't know exactly what the appropriate conditions for use are so with a general model I     
1:31:35     
can train it on say C Elegance data for a Sean's embryo and I might try to     
1:31:40     
analyze a drop embryo and of course my results on the drop embryo are not going to be very     
1:31:46     
good because it's a different uh you know the the the developmental process     
1:31:52     
is a bit different so it might recognize some aspects and not others now you know     
1:31:58     
if you it's kind of like misapplying statistical models so statistical models can be misapplied if you violate the     
1:32:05     
assumptions of that model and so it is with machine learning models and especially with Foundation models if     
1:32:12     
there are certain underlying assumptions depending on how they're trained they're trained very generally but at the same     
1:32:18     
time they have a lot of limitations that can rise from the choice of things that are put into the     
1:32:25     
training set so we might end up making mistakes or violating assumptions that we weren't     
1:32:30     
aware of so this is a problem and and you know so we want to be able to     
1:32:35     
balance sort of the transparency with the uh ability to under to interpret the result and so     
1:32:42     
forth so this is you know what's going on with Foundation models in general and     
1:32:47     
I just gave you an example of a genomic Foundation model     
1:32:52     
so with that being said uh there's this Foundation model that came out recently um so this is a blog post from     
1:33:01     
the Stanford human centered artificial intelligence group and this is Reflections on Foundation models so they     
1:33:08     
launched a center for research on Foundation models and in this art in this blog post they discuss why these     
1:33:13     
models are so important and reflect on the community response so they define IT stand for     
1:33:21     
they Define Foundation models as models trained on Broad data generally using self-supervision at scale which means     
1:33:28     
that they don't have people you don't have to train the model for your specific instance you can just use the     
1:33:34     
model and you train it on enough data on the back end and on the front end you     
1:33:39     
can put in different types of input data to try to get a uh you know to try to get an analysis out and it's basically     
1:33:47     
self-supervision uh you don't need to have like specific data sets um so you     
1:33:53     
know uh these models which are based on standard ideas in transfer learning and recent advances in deep learning and     
1:33:59     
computer systems applied at a very large scale demonstrate surprising emerging capabilities and substantially improved     
1:34:06     
performance and a wide range of Downstream tasks given this potential we see Foundation models as the subject of     
1:34:12     
a growing Paradigm Shift where many AI systems across domains will directly build upon or heavily integrate     
1:34:19     
Foundation models uh this incentivizes homogeneization the same few models are     
1:34:25     
repeatedly reused as the basis for many applications some consolidate such     
1:34:31     
consolidation is a double-edged sort centralization allows us to concentrate and amortize our efforts so we have     
1:34:38     
single models that we're working to Perfect On A small collection of models that can be repeated and applied across     
1:34:44     
the applications to reap these benefits so this is like building the infrastructure for like the standards in     
1:34:51     
a field but of course centralization also pinpoints these models as singular points of failure that can radiate harm     
1:34:58     
so we might have problems uh you know in the case of biological models uh there     
1:35:04     
isn't really a social risk but there is this risk of misapplying the model violating the assumptions of the model     
1:35:10     
and so forth so there are a lot of ways in which we can screw up using     
1:35:15     
Foundation models even though it seems like this is sort of the Holy Grail for a lot of our modeling efforts so you     
1:35:22     
know we have a lot of tools out in the world especially hugging face and some     
1:35:28     
of these other large Corp some of these large corporations that are trying to     
1:35:33     
develop Foundation models so Foundation models aren't something that small groups can generally do but um you know     
1:35:41     
I'm going to talk about an Institute that's doing this though so     
1:35:50     
um okay so they have another interesting point here and this is with respect to     
1:35:56     
grounding models and uh so they say that founding Mo Foundation models can and     
1:36:02     
increasingly should be so they have the section here on grounding models and it's interesting     
1:36:08     
how they connect this to um embodiment and some of the things that we talk about with embodied cognition um so     
1:36:17     
Foundation models can and increasingly should be grounded perception interaction acting in a physical 4D     
1:36:23     
world and acquiring models of Common Sense physics theories of mind and acquiring language grounded in this     
1:36:29     
world are important components of AI this is from Mal 2021 we can get to the     
1:36:35     
bottom and look at that that all in valve will require grounding given the importance of grounding we draw     
1:36:41     
attention to existing efforts that successfully build Foundation models with various forms of grounding such as     
1:36:47     
clip which is trained on image text pairs on the internet and Mero which is trained on YouTube videos with     
1:36:53     
transcribe speech so there are a lot of you know kind of models that are multimodal you have text speech and then     
1:37:01     
images and video further grounding can also be achieved by an integrating Foundation     
1:37:07     
models with other approaches in AI so piglet is it's an acronym cute acronym     
1:37:13     
is a recent example that GRS Linguistics form encoded using a pre-trained language model with a neurom model     
1:37:20     
physical dynamic so you're now connecting language and language learning with physical Dynamics     
1:37:26     
Foundation models are not just large language models Foundation models can also be trained using images video and     
1:37:32     
other sensory and knowledge based data and some already are so they in their     
1:37:38     
they have a separate report from this we underscore grounding is critical to how Foundation models will evolve in     
1:37:44     
computer vision and Robotics so they use this aspect of grounding a model or     
1:37:50     
there's this idea of ground grounded cognition where you're grounding Concepts in other things so there's a     
1:37:56     
multimodal aspect to this so in a biological model we might have a list of     
1:38:02     
terms or like a corpus of linguistic terms jargon it's very famous in in a     
1:38:09     
lot of scientific fields that you have your own set of jargon and then you have your own set of data which could be     
1:38:16     
image data it could be electrophysiological data it could be sequence data and the like and so all     
1:38:23     
those things are put together into a model and so we already do this kind of with labeling but this is a more     
1:38:28     
sophisticated way of doing     
1:38:40     
it so now I'm going to talk about two papers here this is the first paper sequence modeling and design for     
1:38:46     
molecular and genome scale with EVO and so Evo is this New Foundation model that     
1:38:52     
I'm going to talk about um these people are associated with Stanford University what they call the arkans     
1:38:58     
Institute which I'll talk about in a little bit something called together AI the Chan Zuckerberg biohub and the     
1:39:07     
University of California Berkeley so a Bay Area group of people very Bay Area     
1:39:12     
flavored because they have a number of startups involved so these are the people behind this Evo     
1:39:19     
model so they're interested in going from the molecular to the genome scale so they're not working on phenotypes     
1:39:25     
they're working on processes necessarily so let's see what how they do on this so     
1:39:31     
the abstract reads the genome is a sequence that completely encodes the DNA     
1:39:36     
RNA proteins that orchestrate the function of whole organism so this is     
1:39:41     
what they're trying to bring together some of these molecular and genomic aspects of the phenotype and of the     
1:39:49     
organism advances in machine learning combined with massive data setss of whole genomes could enable a biological     
1:39:56     
Foundation model that accelerates the mechanistic understanding and generative design of complex molecular     
1:40:03     
interactions we report Evo a genomic Foundation model that enables prediction and generation     
1:40:09     
tasks from the molecular to genomic scale using an architecture based on     
1:40:15     
advances in deep signal processing so they're working on Signal processing uh techniques we scale EVO to 7 billion     
1:40:22     
parameters with a context length of 131 kilobases at single nucleotide by     
1:40:29     
resolution trained on whole procaryotic genomes Evo can generalize across three     
1:40:35     
fun fundamental modalities of the central dogma of molecular biology so these are described in the     
1:40:42     
paper to perform zero shot function prediction that is competitive with or outperforms leading domain specific     
1:40:49     
language models so again in Evo they're using language models uh but they're applying it to these problems with in     
1:40:56     
this case procaryotic genomes Evo also excels at multi-element     
1:41:02     
generation tasks which demonstrate by generating synthetic crisper cast molecular complexes and entire     
1:41:09     
transposable systems for the first time so they're actually generating uh new     
1:41:14     
not only new they're not only training it on these procaryotic genomes and     
1:41:20     
generating SE quences but they're actually able to generate crisper cast molecular complexes and transposable     
1:41:27     
systems so this is very important for synthetic biology and for bioengineering if you need to create a construct for     
1:41:33     
example you want usually people Design This with keeping the promoters and the     
1:41:39     
the sequence the target sequence in mind and they have to put it together and it's a pretty painstaking task instead     
1:41:45     
with a foundation model you could just simply design them by generating what you need the needs of of what you want     
1:41:51     
to do with some sequence that's generated by the model and of course you     
1:41:56     
can you know specify function in the model you can train the model with functional annotations and make it even     
1:42:02     
better or at least that's the Hope using information learned over whole genomes Evo can also predict Gene     
1:42:09     
essentiality at nucleotide resolution and can generate coding R sequences up to 650 kilobytes in length so we get     
1:42:18     
these fairly large sequences 650 kilobytes it's actually a pretty big SE     
1:42:23     
chunk of sequence um you know it's something that we can definitely use and building     
1:42:29     
constructs or identifying conts or whatever we want to do it's definitely enough to really kind of make it     
1:42:35     
biologic or bioinformatically useful and so this is where coding rid     
1:42:42     
sequence is meaning that it can identify coding regions and generate parts of coding regions and so forth so this is     
1:42:49     
important in terms of like sequence quencing data because a lot of sequencing data is can you know you're     
1:42:54     
trying to build uh consensus sequences and try to build uh different types of     
1:43:01     
annotated genomes and this can help with that process uh for individual     
1:43:06     
organisms advances in multimodal and multiscale learning with EVO provides a promising path towards improving your     
1:43:13     
understanding and control of biology across multiple levels of complexity so what they're doing here is     
1:43:19     
they're building a foundation model for genomes we're identifying coding regions     
1:43:25     
identifying promoters identifying things that could be used for bioengineering or     
1:43:32     
for bioinformatics and the like so we haven't built up to the phenotype at     
1:43:37     
least for procaryotes or for viruses you know we have we can do this     
1:43:43     
for rnas we can get phenotypes but for things like SE allans we cannot get a phenotype from this but it's very much     
1:43:50     
bioinformatically useful as I said so they kind of go through this um     
1:43:56     
so I'm not sure if this is something that's available to the public I think they're just reporting on this pre-train     
1:44:02     
model that they're using so they're actually getting you know they're building these language models     
1:44:08     
so in this figure here they show that they what they call Long context genome     
1:44:14     
Foundation model so they have this uh this sequence they have a regulatory DNA     
1:44:20     
model where they have a promoter they have a coding sequence they have a promoter and a non-coding or an aging     
1:44:27     
and so they can take all these data and they can build a number of language models they can build a protein language     
1:44:33     
model they can build an RNA language model they can build a code and language model from the coding sequence you can     
1:44:40     
build a Coden language model which basically takes the coding sequence and builds a series of codons and of course     
1:44:46     
we have these uh open alternative open reading frames and alternative sequences that result from where the open reading     
1:44:53     
frame starts on the coding sequence so this is a generative problem that we can actually solve pretty well hopefully     
1:45:01     
using the codin language model it takes all the possible codin and it puts them     
1:45:06     
together in a generative fashion and given the sort of the grammar of what     
1:45:11     
this Gene produces in terms of or what this coding sequence produces in terms of transcripts we could you know predict     
1:45:19     
what this is going to look like from the codin language model then we can build a protein language model that predicts the     
1:45:26     
protein sequences or the possible protein sequences and again this is a matter of taking codons and turning them     
1:45:34     
into amino acid sequences and then predicting some protein sequence so you     
1:45:39     
have another language model that involves the rules of proteomics or uh amino acid sequences and so we can put     
1:45:46     
those together as well and you know this is not perfect but it gives you a starting point it's it's basically     
1:45:53     
solving this generative problem that we've had in biological systems where uh     
1:45:59     
whereby we can translate a gene in many different ways and we want to be able to predict which ones are most     
1:46:06     
likely so then the non-coding RNA Gene then can produce an RNA language model that RNA language model then is just     
1:46:13     
basically predicting the RNA sequence and the secondary structure that     
1:46:18     
results so they they have they they build a model using likelihoods so again this is using a lot of bioinformatics in     
1:46:26     
in prediction um yeah so this is uh This Is How They pre-train these genomic     
1:46:33     
Foundation models and this is on a procaryotic Model system so we're using procaryotic genomes where we have a lot     
1:46:39     
of single genomes and we can do this um very well and then we can of course uh     
1:46:45     
have a compute optimal scaling here and they're comparing it to other approaches uh in this graph     
1:46:53     
G so this is the other this is actually I think a blog post from uh their     
1:46:59     
Institute where they're from so this is the ark Institute and this is their blog post on this so this is Evo long context     
1:47:07     
modeling from molecular to genomic scale this accompanies the paper that I just talked     
1:47:12     
about so they kind of talk about how this is trained uh Evo is a long context     
1:47:19     
biological Foundation models so they use long they they use context from uh     
1:47:24     
sequences based on the striped hyena architecture so we talked about hyena or     
1:47:29     
in another meeting like last year and hyena is a way that like they're using the language model to predict DNA     
1:47:36     
sequences so there's this whole hyena architecture and this is the stripe hyena architecture and this generalizes     
1:47:43     
across the fundamental languages of biology so in this case what they're doing is they're building models for DNA     
1:47:50     
RNA and proteins as we saw in the paper in figure one Evo is capable of both     
1:47:56     
prediction tasks and generative design from molecular to whole genome scale over     
1:48:02     
650,000 tokens in length so you know they're using individual nucleotides as     
1:48:08     
the tokens and they're building the model from that Evo is trained at a nucleotide bite resolution on a large     
1:48:15     
Corpus of procaryotic genome sequences covering 2.7 million whole genomes so they have to use 2.7 million whole     
1:48:22     
genomes as the foundation for this Foundation model so that's a lot of sequences and of course in procaryotes     
1:48:29     
you can have those kind of genomes in other organisms probably not yet but we     
1:48:34     
may get there um but the the basic Point here is that it takes a huge amount of     
1:48:41     
training data to do     
1:48:46     
this so the St pen architecture is a deep signal processing architecture     
1:48:51     
which is designed to improve an inic an efficiency and quality over the prevailing Transformer so they're     
1:48:57     
building upon they're they're trying to get Beyond like the transformer type model and they're using this sort of     
1:49:04     
deep signal processing architecture so Evo one was collabora     
1:49:10     
collaboratively developed together by the together AI in AR Institute so we'll talk about the artitute in a minute uh     
1:49:18     
but I want to highlight that this is the arch texture that they're using so this is a model of nucleotides here we have     
1:49:25     
our a our C or G and our T So adenine cytosine guanine and     
1:49:31     
thyine and then we have probabilities for each so this is like in bioinformatics where you try to predict     
1:49:37     
the next nucleotide in the sequence and they occur at a certain probability and     
1:49:43     
so we can actually put probabilities on those and the striped hyena model means that it's building all these different     
1:49:50     
sequences so the input sequence is down here you have an input sequence     
1:49:56     
sometimes it has gaps sometimes you want to generate alternative sequences and from that input sequence You can predict     
1:50:03     
where there are going to be a lot of input sequences but you can predict what the transition probabilities will be at     
1:50:10     
any one position along that sequence so when reconstructing a new sequence you     
1:50:15     
know a might be likely maybe at 0.5 likelihood C might be likely at 0.1 and     
1:50:22     
so forth and so of course the next choice in this uh example would be a most likely     
1:50:29     
but it could be CG or T and so we can do this you know uh sight by sight until we     
1:50:35     
get our sequence uh we have these different     
1:50:41     
aspects of hyena and what they call rotary attention so I'm not sure rotary attention is it should be in the paper     
1:50:47     
if you're interested but it Maps it to this hyena operator and it's Glu which     
1:50:52     
is uh the uh General language unit I guess and then it goes out uh this is     
1:50:59     
where you have the sequ uh signal processing aspect so you have uh where     
1:51:04     
you're mixing sequences and channels so you have this convolution step long     
1:51:10     
convolution step and then there's an output so this is a pretty uh sophisticated     
1:51:16     
architecture um the model actually is available on hugging face and the link is here I don't have a the link but uh     
1:51:25     
you can go there and and probably search for uh this uh architecture here the Evo     
1:51:32     
architecture and you should be able to pull it up and and also in the GitHub repository that they have in this link     
1:51:37     
so I'll put this in the description of the video this this blog post and you can go to those resources if you want it     
1:51:44     
will be made available via the together API and playground soon in addition a model w we are also excited to share     
1:51:51     
intermediate checkpoints we will be releasing the training data set open genome so they're going to release the     
1:51:58     
data set in coming days and I got I captured this earlier this week so it should be available now so there's this     
1:52:04     
data set open genome and this has 2.7 million publicly available genomes for     
1:52:10     
procaryotes and that's the training set so they're releasing their training set they're releasing their code and their     
1:52:16     
architecture so by the standards of foundation models this is actually pretty transparent um so this is and then they     
1:52:23     
ask the question and we always ask the question in machine learning probably erroneously but is blank all you need so     
1:52:31     
in this case is DNA all you need and so that's the the sort of cheeky question     
1:52:36     
they're asking in biology everything starts with DNA genomes carry an entire set of DNA to make a complete organism     
1:52:44     
and so you know I don't think that's going to solve your all your problems uh     
1:52:49     
DNA I mean if you want to eat a cheeseburger maybe but if you uh maybe     
1:52:55     
want to build a phenotype no so you know this is again you know     
1:53:01     
Foundation models are interesting because they try to kind of be very inclusive in terms of all the data     
1:53:07     
they're bringing in and they try to make these very general predictions across data but again you know if we're trying     
1:53:14     
to build a complex organism this is maybe just a very very early starting point     
1:53:20     
so that brings me to the ark Institute so the ark Institute is this institute     
1:53:25     
uh where the it's a new Institution for curiosity driven biomedical science and     
1:53:31     
technology and so this is they're headquartered in uh pal Alto which is     
1:53:37     
where Stanford is near silan Valley uh Arc is a nonprofit research organization founded on the belief that many     
1:53:43     
important scientific programs can be enabled by new organizational models Arc     
1:53:49     
of science SST no strings attached multi-year funding so they don't have to apply for external grants and they're     
1:53:55     
interested in like complex diseases and biology so that's where they're coming     
1:54:01     
from their tools they have a series of tools here um so software and tools they     
1:54:08     
have Evo DNA Foundation models they have genome engineering     
1:54:13     
tools like cast 13d they have this design tool and then Screen Pro Tool     
1:54:18     
which is a tool for Flex pulled crisper screen analysis so they have these genome engineering tools and these     
1:54:24     
machine learning tools and so that's all I have to say about that um hope you learned something and uh especially if     
1:54:30     
you're interested in gso maybe we can have more discussions about this with respect to your projects
