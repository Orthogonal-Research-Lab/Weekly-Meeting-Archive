## TRANSCRIPT

0:07     
hello oh you can okay um well anyways uh     
0:15     
welcome um not sure if anyone wanted to mention anything before we start     
0:24     
or there he is hello Hi how are you yeah there you go yeah I guess it is the     
0:35     
browser um yeah I hear you and uh Hussein are making progress on the     
0:42     
embryo or the uh breast cancer physics yeah yeah I he he seems to be     
0:50     
coding but I haven't seen the results from the code yet oh okay yeah okay yeah     
0:56     
okay uh I sent you that problem with the texture right uh I think there's a general     
1:03     
problem there okay you know that uh if you take a pear if you take a pearlin     
1:09     
noise in 3D and project it is the projection pearlin noise I don't know     
1:15     
yeah or if you take a if you take a projection and it has pearing noise does     
1:21     
it come from 3D pein noise right okay and uh I have a sneaky     
1:29     
suspicion that this reversibility is only true for gaussian noise okay okay because gaussian seem to     
1:37     
transform between Dimensions     
1:42     
fine so I I leave you with that puzzle yeah I'll have to look into it I haven't     
1:48     
had time lately     
1:55     
but okay yeah what we I think we'll make deadline for the book The uh the Russian     
2:01     
group is also working now and uh they've come up with a clever way of spreading     
2:09     
the lines so that you don't miss any tumors of a given     
2:14     
size okay okay so uh you know they they have     
2:21     
produced a little bit so far but not a full algorithm     
2:27     
right right because you could you could tumors within the spaces when you sample     
2:33     
yeah right right and they they've come up as I said with the way of Genera lines so you the way they put it so you     
2:41     
hit each tumor at least with one line okay okay now uh you know the the     
2:50     
method that hin are working on on the other hand is a greedy algorithm if you hit it to where you go after it and you     
2:57     
sort of ignore the rest yeah okay uh so uh we'll see which does best     
3:07     
it's not obvious     
3:12     
yeah okay but it's sort of as I it's sort of as I figured that there will be     
3:17     
alternative algorithms right okay and the other     
3:23     
thing is they seem to be taking an image independent view whereas hin and I are doing hit     
3:31     
very image dependent uh     
3:37     
algorithm yeah okay so that's that that's the     
3:42     
current state of things uh the say seems be getting more and     
3:48     
more involved in this and uh so I'm sending him some general C algorithm of     
3:55     
papers as we go yeah yeah he seems to be pretty excited about     
4:01     
it so yeah yeah yeah you may be rich someday if he     
4:07     
gets his startup going I doubt I'll be alive so I'm not     
4:13     
worried about     
4:19     
it anyway besides there's nothing nothing to spend it on here I'm     
4:24     
getting an echo yeah     
4:30     
this town is the town I'm in is down to uh only     
4:36     
one business and that's the post office     
4:42     
oh yeah we had to uh but uh that was a     
4:47     
veterinarian's office but he died in July     
4:53     
oh so uh that's finished yeah     
5:02     
uh but so wealth doesn't matter     
5:09     
here yeah yeah well thanks for the update     
5:15     
that's good okay yeah strange so I don't know if Morgan has     
5:22     
got anything uh I have some things actually from his posting um we'll go     
5:28     
over that a little little bit uh yeah so why don't we just     
5:34     
actually I'll go over uh another thing here um so let me share my     
5:40     
screen I'll go over another thing first uh so this week uh Pocky Balia who was     
5:49     
one of our Google summer of code students she published a blog post on     
5:55     
her project and this is the result so this is developing adaptive neural networks     
6:02     
growing grass for dynamic data representation and I'll post it in the     
6:10     
chat hello Jesse so this is the blog post uh she     
6:17     
prepared for her project it basically goes over the project so you know again     
6:23     
this was a a a project for Google summer of code there were two parts to this     
6:30     
there is the topological data analysis using K mapper uh this was a way to sort of     
6:36     
explore the data for complex data structures things that you might expect in a network topology and then the     
6:44     
second one was growing graph neural networks where you know that was the main part of the     
6:51     
project so uh actually I think uh hamano shogul uh I think it was last year     
6:57     
worked on the TDA using K mapper he kind of played around with it a little bit uh     
7:03     
at the end of his project but didn't really get very far and uh Pocky also     
7:11     
did this uh at the beginning of her project but there's a lot more to do     
7:16     
here I don't really know um if we could find like I don't     
7:21     
know if that could be like the focus of a project uh just that or what but I'm     
7:27     
really intrigued by some of the work that's been produced you just need to kind of go in a more formal direction     
7:33     
for that uh thinking about topological data analysis and what we can get out of it uh people have done this with celan's     
7:41     
uh connectome already so uh you know there is some precedence for it but     
7:48     
otherwise you know I'm I'm I'm kind of interested in that aspect of the project and although she did work on that it     
7:55     
wasn't uh you know to any sort of completion so that's still open but the     
8:00     
more important part is is growing the graph neural network part which I think is going to be really interesting so this K mapper algorithm     
8:09     
so this is uh basically a way to sort of explore the data so this is uh you know     
8:16     
using topological data analysis um using the K mapper     
8:21     
implementation which is uh where we can visualize High dimensional data and     
8:27     
identify key topological features so this is a Mathematica applet that     
8:33     
allows you to do k mapper um and so it looks kind of like maybe like a umap but     
8:40     
it's not a umap um and so you know what it does is basically plots the data out     
8:46     
in certain ways and with topological data analysis allows you to do is find like shapes in the data so the shape     
8:53     
will have a c the data will have a certain shape especially if you you know reduce the dimension fun ality and you     
9:01     
can plot it in different ways and see the shape and you can actually analyze it using more Advanced Techniques so     
9:08     
this is the visualization step where you see it in the in the applet here um but     
9:14     
there is like a a mathematics underneath and that's something that's I think     
9:19     
something to follow up on um now the shape of the data is interesting because it gives you you know if you think about     
9:25     
like a trend line a linear trend line a linear regression it's like something that follows a line     
9:32     
so it's like a point of clouds that goes along a line or you know if you think     
9:37     
about like in classic topology you have these uh you know annulus uh like shapes     
9:44     
that look like a donut and sometimes data look like that sometimes it's because it's distributed in space     
9:51     
sometimes it's because it's distributed in time sometimes there other features that give it that shape but the idea     
9:58     
here is we want be able to find some shape to the data that isn't like the     
10:03     
anatomy so this is uh sort of the summary of this um and you know again     
10:10     
like there's probably more to do here I think Pocky did work on this for a while during the summer but I you know again     
10:16     
I'd like to to explore this more but this anyways this allowed us to go to     
10:22     
sort of The Next Step which was growing the graph neural network so you know     
10:27     
that um uh H did some work on uh hyper     
10:32     
hyper graphs uh that were graph neural networks and so uh Pocky was just doing     
10:38     
a standard graph neural network where we had this growing Network so the idea is     
10:44     
that you know in a in an embryo you have a lineage tree the lineage tree unfolds     
10:49     
by having a single cell that divides and does this repeatedly until you get to uh     
10:56     
differentiated cells so that produced is this sort of growing graph if you if you     
11:01     
plot the lineage tree out as a complex graph or a complex Network you get these     
11:08     
uh you know the number of nodes increases uh basically there's a doubling of nodes and so you can you     
11:16     
have to sort of readjust the network uh as that happens but you also have to     
11:21     
visualize that and so that's where this kind of project is useful so we had a     
11:27     
couple things in the growing graphic Network we had temporal node Edition which is where the graph reads     
11:34     
from a data set so every time a new cell is born it's added to the graph every     
11:39     
time a cell dies it's removed from the graph and so you but you end up basically with this doubling of nodes     
11:45     
every so often and then you have this change in connectivity um the second part is nodes     
11:51     
are dynamically added to the graph at their specified birth time so the timing of this process is such that it Mir     
11:59     
miror like what happens in an embryo and then the simulate cell division um and oh you     
12:07     
were gonna say something uh yeah Bradley one minor     
12:16     
correction okay rle minor correction in the differentiation tree we refer to     
12:25     
each intermediate cell type as if it is     
12:31     
differentiated okay okay so that it's not just there's a terminal     
12:39     
differentiation okay yeah yeah I mean this was just simplifying it so yeah you     
12:44     
you would have like these different cell types that kind of are intermediate stages yeah Mo most people count those     
12:51     
intermediate cell types as differentiated okay okay     
13:02     
all right um then okay so that was terminal node     
13:08     
Edition where we have this process of adding and removing nodes then we have this spatial learning process which is     
13:15     
where we just don't want to just add nodes we want to learn spatial relationships between nodes so basically     
13:21     
the daughter cells are in some spatial relationship with each other and this is     
13:27     
going to be important for like things like looking at polarity and looking at other structures like we get these     
13:34     
intermediate cell types that start to form some sort of structure within the embryo so in the celegans you have this     
13:41     
comma phase of development where the embryo basically forms a comma and you     
13:47     
know where there's like an uh an indent in the middle and so we can look at those sorts of things uh those spatial     
13:55     
relationships the data not only have the division information but also this some     
14:00     
spatial location information as well so that's good uh the network computes and     
14:05     
optimizes the distances between every pair of nodes and this was done using the cmaes algorithm so this is an     
14:12     
evolutionary algorithm so this was forked from a project uh where they were     
14:18     
doing this with neural networks where they were growing neural networks and so they have this evolutionary algorithm     
14:24     
that's associated with that um that source of the fork that kind of goes it     
14:30     
walks through this sort of optimization process so you can see that you're     
14:35     
basically um initializing an you're using an evolution strategy to initialize the optimizer you're     
14:43     
performing this optimization Loop so it looks for um has like a loss function     
14:48     
and it looks for the uh best answer from that and then it's retrieving the     
14:54     
optimized parameters so it's basically optimizing this whole process process of     
15:00     
spatial learning using this algorithm uh the third part is biological analogy so this can be liken     
15:07     
into embryonic development although it's not quite the same thing we're just kind of abstracting it in in According to     
15:14     
some sort of complex Network structure and then the graph structure evolves both both temporally through these time     
15:21     
node editions and spatially through learning the intern note distances and kind of making sure that they're um you     
15:28     
know in a in a reasonable sort of arrangement so this is a gift that shows     
15:35     
kind of how one of these grow this network grows um it you know kind of it's expanding     
15:42     
and the way it's kind of plotted out the uh it's weird because you know every     
15:48     
time you plot new cells or every time the network grows it's visualized in a     
15:53     
different way and this is one of the things about uh complex networks and working with them is that these layouts     
16:00     
are kind of hard to um optimize and get exactly the way you     
16:06     
want them so like if you want to see for example you see that you can't see a lot of the labels are all smooshed up some     
16:13     
of the cells are all or the the nodes are all smooshed up in blue it's because it's just visualized in a way that is     
16:19     
hard to see so you know you could visualize these in a circle like a lot of people use uh you know circular     
16:27     
layouts but that doesn't really give you you know that spatial information if we     
16:32     
use something like this with spatial information you know we end up with things that are bunched up in certain     
16:39     
places and you can't really see the fine structure so you know if we were doing     
16:44     
this in a live app we could zoom in on a certain region or rotate the three-dimensional graph and maybe see     
16:51     
one perspective on this but this is you know this is a problem in complex     
16:56     
networks and trying to interpret sort of their structure by you know kind     
17:03     
of visualizing the nodes and their connections so you know we have like things like force directed layouts     
17:09     
circular layouts these kind of layouts where you keep Shifting the sort of the     
17:14     
orientation and the topology but it's basically the same degree of connectivity and that's what I mean     
17:21     
that's the best we have to work with so that's one of the problems with these kind of um these kind of structures is     
17:28     
it's kind of hard to get any insight out of just visualizing them a certain way     
17:33     
so you know or maybe the best way you have the easiest way you can visualize     
17:39     
them so this is um you know that's maybe why we use the or where we're interested     
17:46     
in the topological data analysis so we can understand if there are any structures in there that are not easy to     
17:52     
see from something like this the other thing too are of course we can now calculate graph statistics so we can     
17:59     
calculate things like centrality or you know uh graph distance uh the span of     
18:05     
the graph across the the the diameter of the graph and other types of things so     
18:11     
we have you know a number of ways we can describe the structure these growing structures that we can't do with say     
18:18     
just a normal embryo or you know some sort of uh visualization procedure or     
18:24     
segmenting the cells out and analyzing each cell or even with a lineage tree     
18:30     
where we're just kind of looking down uh sort of a directed uh branching structure so that's what we have with     
18:38     
that so again challenges and learnings growing a a GNN came with its unique set     
18:45     
of challenges we wanted to design the training Loop so you know determining an     
18:51     
effective training Loop for the network so in other words there was some Source data that was embryo data and that was     
18:59     
plugged into this algorithm and then the optimization procedure happened then you get this visualization so you know     
19:06     
getting more data of course I need to send Taki more data um we also need to     
19:12     
work on you know some of these aspects of optimizing the training so you know     
19:19     
maybe training the data set or training the model with other data sets maybe     
19:24     
they are not celegans it may help it may gener it may not     
19:37     
help okay     
19:43     
um so then there's the limits of back propagation so standard back propagation     
19:50     
a key component of neural network training proved inadequate for the groin GNN so you know we usually use back     
19:57     
propagation in a lot of neural network applications but in this case we couldn't do that uh the increasing     
20:03     
number of parameters as the network grew made traditional back propagation ineffective the nonlinear growth pattern     
20:10     
of the network further complicated the use of gradient-based optimization methods so this is again the solution is     
20:17     
this algorithm cmaes or covariance Matrix adaptation Evolution strategy so     
20:24     
this is what comes in the thing that was forked this is uh although it's a     
20:29     
powerful optimization algorithm it doesn't rely on gradient information it's uh but it's also     
20:36     
particularly well suited for nonlinear non-convex optimization so this is a     
20:42     
non-convex problem nonlinear problem and we can use this algorithm to kind of overcome those limitations of say     
20:49     
something like back propagation which relies on a convex linear     
20:56     
regime implementing cmaes allowed me to optimize the Network's parameters despite its Dynamic structure then there     
21:03     
was this aspect of spatio temporal consistency where you know we wanted to     
21:08     
maintain consistency between the temporal growth of the network and its spatial learning presented there were     
21:14     
some ongoing challenges to this so the first was that uh newly added nodes     
21:20     
which where the daughter cells were position correctly relative to existing nodes we can optimize this process and     
21:27     
train the data you know we can train the model on the data but it's you know     
21:32     
unclear as to actually whether we're actually getting the right positions so this was a challenge of course uh the     
21:39     
spatial relationships needed to remain meaningful as the network expanded over time so we don't want cells that are     
21:46     
clearly say in the anterior posterior end of the embryo being flipped or     
21:52     
otherwise delocated um and then of course we want to maintain those spatial relationships     
21:59     
between the anterior and posterior poles between the left and the right sides between the dorsal and the vental aspect     
22:06     
of the embryo and then finally balancing local and Global optimization node positions     
22:13     
was a constant consideration so this is where again we were visualizing this     
22:18     
network and the network was constantly changing sort of its orientation so it's     
22:23     
kind of hard to see how well it's doing and kind of hard to see the relationship ship between cells or nodes in this case     
22:32     
and so that's that's a major challenge as well so congratulations to Pocky for     
22:38     
that uh work that was very good and that was all committed to our uh dvo learn     
22:45     
repository and so yeah we this is a project that relied on two open source     
22:50     
forks for the TDA M or the kmapp or the TDA analysis and then for this uh     
22:57     
growing networks uh we talked about these these Source Forks or the for source of these     
23:03     
Forks in the past so I refer you to those uh     
23:09     
references and Bradley yeah in terms of growing networks why not make the new     
23:15     
nodes uh a different color and keep the previous diagram and then add the new     
23:24     
noes yeah I could do that right yeah it seems that that might be a     
23:33     
way to neutralize the growth yeah that might be an option for     
23:40     
I mean the only diff the only problem there is it gets really crowded but that's probably okay um you know yeah you might have to     
23:47     
change this you might have to change the scale as you go and then make it into a     
23:54     
movie yeah it's sort it's sort of like uh the old movies of hours of 10 when     
24:01     
you familiar with those oh yeah yeah I remember those yeah you had to keep changing     
24:07     
scale to visualize what was going on     
24:12     
but     
24:19     
yeah yeah yeah I think the problem is we can't visualize a continually changing     
24:25     
topology but if you rest red yourself with the previous topology when you add     
24:32     
the new nodes and make the made a distinction you might be able to catch it     
24:45     
yeah yeah so it's interesting to note that like the algorith or the the whole approach that was forked from was done     
24:53     
with Merl networks and as such you know you just basically what they were interested in and that     
24:59     
it's just adding nodes to a neural network and it wasn't so much in uh important to have like good     
25:05     
visualizations of that process because it was just like expanding that Network     
25:11     
and so yeah you're getting the two confusions one is continually changing     
25:19     
topology uh and the scale is changing and     
25:25     
uh so uh I I think in terms of human     
25:30     
visualization we need something to hook on to right     
25:36     
right even if it's a changing Target if the problem is to smooth out     
25:42     
the changes in [Music] topology yeah     
26:00     
digging some notes here yep okay uh any other questions or     
26:08     
comments about     
26:14     
that okay so the next thing I'll talk about is uh Morgan has been posting on     
26:21     
some videos YouTube videos so we were talking uh I think it was last week or     
26:27     
the week before about the Nobel Prize in physiology and medicine and this is of course was the     
26:33     
prize given for uh the discovery of uh I     
26:39     
think it's micro rnas and celegans so you know they were discovering the function of micro rnas in a very     
26:47     
specific context in the up or down regulation of a specific Gene and they were able to do these studies to show     
26:54     
this uh has brought to my attention after the meeting that this a you know the Nobel was given to to se elegans     
27:01     
people and there's been a lot of work done in Plants uh we're talked we had one paper in the group that I talked     
27:08     
about on Plants uh but the plant P they didn't give any award to the plant     
27:15     
people who are working on this problem so it's kind of an oversight maybe but uh anyway was     
27:22     
that it's typical yeah so you know just     
27:27     
so that plant work yeah plant work in Mor for Genesis is generally ahead of     
27:33     
animal work but it doesn't get recognized yeah     
27:39     
yeah so that being what it is um you know we we got into this uh further     
27:45     
discussion about like rnas and and the different types of rnas people have discovered and sort of the functional     
27:52     
role of you know all these rnas some are functional as you know uh a very with     
27:59     
very tight targets others are expressed but it's questionable whether they're     
28:04     
functional others are you know just present and so you have a lot of     
28:10     
different types of rnas you have you know that they do all sorts of different things in the cell sometimes they are     
28:17     
involved in gene regulation sometimes they're not sometimes they do other things that are very     
28:24     
transient and so forth so this is a video from Gary this is uh UC Bly events     
28:31     
is the YouTube channel and I guess he won the Breakthrough prize in 2016 so     
28:37     
this was on the T this is his talk uh for the Breakthrough prize the tiny RNA     
28:43     
World which is just basically talking about the world of different RNA     
28:48     
molecules that we know of at this point and you know to Brad there's an     
28:55     
unfortunate there's an unfortunate ambiguity here because RNA world has for     
29:02     
the last 50 years or so meant the uh the world when uh be before life started     
29:11     
right yeah I know that's not what he means     
29:17     
right but he's introduced he he's introduced an ambiguity by using that     
29:22     
RNA world uh expression right it's so it's the early     
29:28     
life clashes with molecular the the fa the favorite model     
29:34     
for origin of life now is the RNA world but that means a world in which RNA came     
29:41     
first right okay     
29:49     
yeah well anyways yeah that so that I mean you know he just kind of goes through kind of our state of knowledge     
29:56     
there I don't think he talked we may talk talk about the original life but I doubt it     
30:01     
[Music] yeah uh this next video was of course um     
30:07     
Anton Petro who's a science Communicator it talks a little bit about the the     
30:13     
Nobel Prize homeor mutants led to the 2024 Nobel Prize so if you go back and     
30:19     
read the you know the well we did it in the meeting two weeks ago where you have     
30:25     
two Landmark papers and seans where they did they looked at a specific Gene     
30:30     
looked at a specific micro RNA and they they were able to kind of get this uh     
30:36     
nail down this circuit where they could see what was going on um so that's like at a more     
30:43     
accessible level than like maybe reading the papers actually uh because the papers you know can be pretty um obtuse     
30:51     
if you're not used to the language they're using and you know all the C     
30:56     
elegans jargon as well so this is actually like Morgan has     
31:02     
brought up the builda cell Summoner a lot so this is from the builda cell seminar uh Channel this is uh Anthony     
31:11     
Forester from RNA to Luca ideas data and utility now this is actually like kind     
31:17     
of more about kind of moving towards RNA World thinking about like the sort of     
31:23     
the origins of RNA or like we had RNA world and then we had had of course     
31:29     
complex life and and rnas of course are doing a lot of things so what is the connection there um I didn't watch the     
31:37     
video so I couldn't tell you much more about it I don't know if Morgan could tell us a little bit about it but it     
31:42     
sounds like a really interesting video yeah sorry I I only I only saw it last night um and so dropped     
31:52     
it in but uh but like you said you know it's it's it's always been a great     
31:57     
seminar Series so I'm sure it's going to be interesting yeah okay     
32:04     
yeah well thank you for dropping those in and and uh you know giving us some     
32:09     
things to I I I was gonna just say one thing     
32:16     
and and um um sort of I mean maybe maybe     
32:21     
related to your kind of initial um uh discussion but just may open it up to to any topic     
32:30     
but um the uh Celsius uh kind of uh you know     
32:37     
our our fear fearless leader at Celsius um was uh visited future house this this     
32:46     
this weekend yeah and future house     
32:52     
um is working on you know it's a I'm not sure where     
32:58     
they got their funding from but they're they're funded to create a     
33:03     
scientific uh assistant and they're doing you know so     
33:10     
they've got I mean I think they've got     
33:16     
some some various projects but um couple     
33:21     
of their current software outputs or this um this kind of Q&A assistant that     
33:29     
does digestion of kind of a particular if you feed it     
33:35     
a um a collection of PDFs that you know have have some sort     
33:41     
of relation or some sort of topic then it digests those and can     
33:49     
produce a kind of uh dnaa from them or you know     
33:55     
obviously summaries and things like that and I was wondering if that was um you     
34:02     
know if there was a already a good kind of curated set of of papers on a     
34:09     
particular topic that you could think of that might be might be useful to use as     
34:14     
an example um we're going to try and put together a um I mean they're they're     
34:21     
basically asking for an event and um that that we can use their space which     
34:27     
is it's a really cool space down in down where the kind of new UCSF campus is     
34:36     
um so was just thinking about that neurot X oh sorry     
34:43     
daughter is asking me to eat the top of her banana     
34:48     
um at NCH X um one of the co-founders has a a paper on the     
34:58     
um kind of like a review paper of the literature review     
35:03     
paper on deep learning and EG okay and so like he has this great     
35:12     
collection of curated papers in a database from you know I forget like 2012     
35:20     
to 2022 or something like that you know     
35:26     
yeah um and just just wanted to put it out there     
35:31     
you know would um do do we have anything you know I was     
35:37     
kind of also you know not that it's Diva Worm but thinking about Jess's uh paper     
35:43     
and and kind of review of cybernetics things like that     
35:49     
yeah of course the seans community always has their they're very uh you     
35:55     
know organized in terms of their resources so I don't know if you go to something like arm base and they have     
36:02     
like they have their sort of Source papers because they have all the Sens     
36:08     
different genes and so that might be somewhere to go I don't know if they have an AP okay yeah yeah this this is     
36:15     
more like like for people to think about and but yeah that's that's an awesome     
36:20     
one yeah um and topical yeah well I mean     
36:26     
it's around a certain top IPC so like you have like yeah so you could like     
36:31     
discover connections between papers that people wouldn't have thought of because a lot of the genetic work is like on a     
36:38     
specific Gene and they're T doing a test you know uh they're trying to figure out     
36:43     
the function of it and everything but like people would look across genes and say in a genomics paper you wouldn't see     
36:50     
this where you look across genes and you say oh these two genes are doing similar things or you know the usual way you     
36:57     
have have to do is go in the literature and like pick out basically the genes     
37:02     
implicated in function acts it's like if I want to see all the genes that are implicated in like the morphogenesis of     
37:11     
the intestine I have to go into the literature and kind of look for intestine and see all these papers and     
37:18     
you know I don't know if I have all of the right genes or what you know so     
37:24     
that's that would be one really helpful thing I think yeah no no I mean that's     
37:31     
that's interesting I'm gon I'm gonna look it up um I'm still still working on     
37:36     
school run here but um but that sounds like it would be a really interesting uh really interesting     
37:45     
um so is that a database or yeah it's worm base and so you know     
37:51     
they have um a lot of Community Resources um     
37:58     
so got it yeah base is here you have basically this version of The genome C     
38:05     
elans genome and you have you know the nomenclature for the genes and then you     
38:10     
know you might search for a gene and it tells you kind of what it does so let me see I can find one here this is     
38:18     
aak1 and it has like the different information about it and tells you where     
38:24     
it is in the genome and I think they are papers that are associated with this and I'm not     
38:30     
sure yeah I'm not sure if they have them because they have to get their data     
38:36     
from somewhere so I'm not really sure uh how you would access that but um but they're     
38:44     
usually papers associated with each of these genes like where they actually found them or you know studies that talk     
38:51     
about them so it may not be worm based it would be the best source for the data but it would be like maybe Pub met and     
38:58     
then you could validate it with wormbase well yeah I mean I see I see this genetics paper     
39:06     
2022 um wormbase in 2022 data processes and tools     
39:16     
um yeah yeah well still really interesting     
39:23     
um you you know like like it would be obviously a very very good topic given     
39:31     
this meeting yeah um but again like um     
39:37     
perhaps a good topic in terms of what future houses goals are you know right um because again like like I think that     
39:46     
is their goal is to you know create the kind of     
39:53     
topical a topical article mean that to Ryme     
40:02     
but yeah that that would be interesting yeah so why don't we yeah I think Jesse     
40:09     
put the link to future house so don't we take a look at that if yeah yeah yeah you can check out     
40:15     
their the the tools um section um I don't know if they've got     
40:21     
an about because uh honestly I don't know their history oh the is uh we're     
40:28     
building an AI scientist yeah I was hoping for a bit     
40:34     
more from new so yeah well I mean somebody     
40:40     
somebody gave them somebody gave them a lot of money just to set up     
40:45     
um and and they the other thing that came up in our meeting was that they had     
40:50     
bought their building because there's a bunch of buildings in San Francisco that are going really cheap you know again     
40:56     
Rel to their you know their last sale yeah right because especially commercial     
41:02     
real estate is kind of crater here yeah um uh um and you know but     
41:11     
that's still still a lot of     
41:17     
money if you have a human that's the quest then AI scientists you have a     
41:23     
world model hypothesis generation then experimentation     
41:28     
you have this AI science assistant agents for specific biological workflows     
41:34     
so you could have like for next gen sequencing or some literature search and then they have ai tools which is the     
41:41     
predictive type model which is like Alpha fold apis which way you to get your data set and then laboratory     
41:48     
experiments which are of course you know um drawing from the empirical stuff so     
41:55     
this is kind of like you know like the community tool I showed you wormbase has     
42:01     
like the API and the laboratory experiments they just need the predictive model aspect plugged on to     
42:07     
that to like you know find relationships between genes or something that you know     
42:13     
because we do a lot of genomics next gen genomics we get a bunch of data but you know the whole point of that is to     
42:20     
synthesize it and find insights but often times we just you know see like uh     
42:26     
some sort of appap and then people make it and that's it yeah sure sure well     
42:34     
Bradley I've got two questions yeah has the AI scientists published any papers     
42:40     
yet uh I no what was that oh you know like again they're so     
42:47     
far they've only got these these few tools it's like UA summary tool and you     
42:56     
know I think that's just their their hypothesized     
43:01     
design well okay so so this is very far from an independent scientist yeah yeah     
43:10     
it's like I read about this where a lot of people are trying to build like what they call an AI scientist basically     
43:17     
using an AI to do like this type of synthesis that a scientist would do     
43:24     
after they run a bunch of experiments how would they find the ins what they're doing and yeah but has it has it sub     
43:31     
submitted any papers and P peer rreview no no it's all a vision and in fact     
43:38     
there was a paper that I had in one of my classes that we went over that was     
43:43     
like basically the argument they made the argument against the viability of an     
43:49     
AI scientist because they don't have the sort of like when people are trained as     
43:54     
a scientist it takes a certain amount of time and it's not that you're you're acquiring skills but you're also     
43:59     
acquiring this intuition for science so that's a thing that an AI scientist is missing specifically so they were     
44:08     
arguing that that's you know that's not going to be an easy thing to overcome um but yeah it's a pretty     
44:16     
Vision Ben blazic um at at argon has like much more     
44:24     
practical work you know systems that um     
44:29     
that they've been developing for I yeah um that they've been developing for     
44:35     
Material Science yeah um so this is you know and and um it'll be interesting to     
44:42     
see their the output of their last kind of meeting SL hackathon you know     
44:52     
so I mean again it's like like one of the reasons you know know it's not just     
44:58     
the the agent it's the the agent's access to information that'll make it     
45:05     
you know in in any way practical right and and the what you know Ben's trying     
45:13     
to do is definitely like you know it's     
45:18     
like yeah that's like automating automating design of     
45:25     
experiments I think be a good way to put it yeah like it's not so much as scientist it's just like hey we've got     
45:32     
to cover this this multi-dimensional space     
45:38     
right and yeah yeah it's like kind of some basic planning um as opposed to any     
45:46     
kind of like you know AI scientists     
45:52     
right I mean like on Saturday we talked about the     
45:58     
1970s and how like they had they thought they had it in the bag with respect to     
46:03     
like planning algorithms and other things and then there was this huge like     
46:09     
yeah winter where all of that kind of went away and they started on the path toward statistical AI we using     
46:16     
statistics more than like rules so it very interesting because it was like they were talking about some books that     
46:22     
were written right at that time and people were like really excited about things like having an AI scientist where     
46:31     
you could uh you know use rules to do things that say a scientist would do but     
46:37     
that never came to fruition so what what salary does an AI scientist     
46:45     
demand initially nothing but who knows okay there limited compute resources     
46:52     
yeah yeah except except for that you see all support for scientists can go away     
46:58     
and so all the grant agencies can disappear     
47:03     
yeah putting his whole in     
47:10     
poverty the other thing is it seems to be confined already to a certain     
47:15     
database and therefore getting AI to make a transition to a different level uh of     
47:23     
thinking uh would seem difficult yeah yeah no for yeah the     
47:28     
trial one that comes up is the one we work on which is H the transition from genes to differentiation     
47:35     
trees could AI ever make that transition I don't     
47:42     
know yeah I mean like it I mean basically it you know it does rely on     
47:48     
the training of the model so they they talk about a world model too which is of course this idea that you can build this     
47:55     
model of the world that gives you enough information to sort of represent any like if I train a model it     
48:03     
can take that data and spit it back out to me but like um you know it doesn't     
48:08     
have this sort of conceptual framework that if I give it any data set in     
48:14     
biology uh well let's say gen gen genomics or genetics but any organism     
48:21     
like it could be bacteria or plants or animals could it gave me a reasonable answer given that has a a general sort     
48:28     
of framework for Life what to expect and you know what the rules of Life are     
48:33     
which humans don't even know but let's suppose that we could sort of get it to learn this world model of biology where     
48:41     
at least you know genomic biology or genetics so that's what you would need you wouldn't just it wouldn't just rely     
48:48     
on the training data sets it would rely on this world model and that's that's going to be you     
48:54     
know probably the thing that does not make it happen um but who knows yeah maybe they     
49:02     
should call it AI librarian because uh uh it basically you     
49:08     
know in the good old days before uh before AI if you had a problem you could     
49:13     
bring it to your librarian who do the the uh lookup service for you yeah okay     
49:21     
so it seems to me they're replacing Librarians more than they're replacing uh science scientists yeah     
49:29     
well well the the the tools that they have are are very modest yeah I mean if     
49:36     
you look at them and and a AI librarian might be actually the the best um you     
49:43     
know a AI research Assistance or something like that which is kind of the     
49:49     
the librarian function you're talking about uh um is uh is kind of what their     
49:57     
QA tool is is you know not even like     
50:02     
reporting to do it's just yeah what what they're trying to do     
50:11     
yeah yeah I have it up here so there's paper QA which is one tool this is a     
50:18     
minimal package for doing question and answering from PDFs or text files so this is where you train it on PDFs and     
50:24     
you get like this answer it strives to give very good answers no hallucinations     
50:31     
by grounding responses with in teex citations in other     
50:36     
words preserving the hallucinations that scientists have well yeah it could be     
50:42     
that um and then yeah it could be an internet Rando you know like but uh yeah     
50:48     
and then uh hasan1 which is h an1.com a minimalist AI tool to search if anyone     
50:55     
has ever researched a given topic which is you know I mean that's within the bounds of the published literature but     
51:02     
trying to find like holes in the literature basically so um yeah I mean     
51:10     
you know this is an area where again it's going to depend on what it's trained     
51:15     
on what you know um and then but you know the tool itself is actually a nice     
51:22     
uh sort of summary and I think that's where the power of this is is sort of in this librarianship or being able to help     
51:30     
research the literature yeah I'm not arguing against it I'm just saying that uh it's rather     
51:36     
presumptuous that it's a scientist     
51:41     
yeah so yeah Morgan is this like uh what is this based is this are these large language models or are these like other     
51:48     
types different types of models that oh yeah no no no these are large language models okay the the only thing I     
51:58     
was gonna maybe say is that you know when they say World     
52:05     
model you know I think they're just I think     
52:11     
they're more you know I'm just thinking about those papers that that saffron had     
52:17     
in his RFP um those kind of early     
52:23     
like I want to say like 2014 like 2016 kind of     
52:30     
papers um I'm trying to remember the group but it's really just about like     
52:40     
trying you know trying to have some sense of planning     
52:46     
yeah yeah like and and     
52:54     
yeah my daughter is telling me what I need to pack     
53:00     
in so but but yeah um yeah     
53:06     
anyway it's it's very it's very rudimentary or you know again it's it's     
53:13     
it's somebody trying to take large language models and and connect all the     
53:18     
different kind of you know specialized features or kind of functions that that     
53:26     
you know some somebody has done research on and then put them all into one graph     
53:32     
right yeah and then say like we need something like     
53:38     
this and and and certainly you know like     
53:44     
like you know archive sanity preserver came along because you know machine Le     
53:51     
machine learning researchers were being completely overwhelmed by the amount of     
53:57     
literature that was coming up right yeah and like you could almost say the same     
54:03     
right now just about bio or Med related     
54:08     
llms yeah right that just that specialized topic you know and and     
54:17     
um yeah yeah so it's     
54:23     
it's yeah yeah so it's definitely like     
54:29     
it are thinking about I mean I also think about that that project that just     
54:36     
um like knew the people it's like the MIT Latino computer science you know     
54:45     
that that made you know the connections between literature okay I can't I can't remember     
54:53     
now oh the state-ofthe-art stuffff yeah yeah yeah yeah yeah yeah you know it was     
55:01     
um you know there there are all these tools that have come and kind of kind of     
55:06     
come and gone unfortun uh um but that were that were     
55:12     
really helpful in terms of you know I mean I I routinely now use the um the     
55:19     
pub meds uh similar and connect you know site you know certainly the citations     
55:26     
right and that's the old you know web of science approach but um similarity     
55:34     
similarity measures can be really interesting and other you know other     
55:42     
um literature helpers I don't know what to     
55:47     
call them sorry I'm trying to multitask here     
55:52     
yeah yeah I think that's uh yeah there are all sorts of tools kind     
56:00     
of dick you were gonna say something I talk to my dog     
56:08     
sorry oh that's yeah it's all good stuff um yeah I'm glad we brought that up     
56:15     
because it's you know I think really interesting stuff kind of how these     
56:20     
different tools are evolving and you know in in celegans of course we have this very rich community of kind of data     
56:27     
and uh you know resources like that and sharing of things and so this is     
56:34     
actually maybe a next step in that and um I don't I don't know if open worm has     
56:40     
really experimented with this too much but it might be kind of an interesting     
56:46     
kind of openworm project because I know we've done like things in open worm with     
56:51     
the literature and some other tools but nothing like you know an l or anything     
56:56     
like that where you're actually trying to train a model to give you um you know     
57:04     
you know generating something uh that can be useful for research     
57:10     
so yeah if if it's useful maybe Elon mus incorporated as robots     
57:20     
yeah it's called a lab assistant Yeah well yeah     
57:28     
okay well uh do we have anything else we want to talk about before wrapping it up for today     
57:37     
or think we had pretty good discussion here um so yeah uh yeah well thanks for     
57:46     
attending and uh see you next week okay take care take care than bye     
57:54     
bye bye okay so our final discussion during the meeting was about using World models and     
58:03     
other types of training data to build as AI scientist and what's interesting here     
58:09     
is that you know openworm we have a lot of data sets more generally in the seans     
58:15     
community we have a lot of data sets that are shared that are available to     
58:21     
you know do things like data mining or to build models from one thing we haven't really addressed as this idea of     
58:27     
a world model so I'm going to go over quickly about maybe the outlines of an     
58:32     
openworm World model or maybe a world model for seans maybe you know if we talked about     
58:40     
sort of these bio General biological models or biological World models that you might need to do this sort of AI     
58:48     
science across species but you know focusing just on seans I think we can     
58:55     
see the pieces that need to be in place at least to get it to a starting point on this so the proposed open worm World     
59:02     
model would in consist of at least three major components the first is mining or at     
59:11     
least capturing all the seelan databases and using those as training data for     
59:17     
this world model so as we showed in the meeting you know we have genomic data we     
59:23     
have uh different experimental data we have morphological data we have data on     
59:30     
the connectome we have electrophysiological data these are all things that are available to us through     
59:37     
you know the community openworm has curated a number of these data sets as well as you know some of the     
59:46     
other things that exist in biological community at large so if we go to uh     
59:53     
genbank or you know ncbi we can find a lot of these resources as well basically     
1:00:00     
training on as much celegans related data as possible then we need General models of sort of biological     
1:00:08     
processes so these might include models of morph Genesis genetics metabolism     
1:00:17     
conomics electrophysiology we might also have a general model of development since we     
1:00:24     
have an entire lineage tree worked for celegans we might have a general model     
1:00:29     
of behavior as well um and so we have a lot of those types of data like we have     
1:00:35     
worm movement data that is sort of you know unclassified you have a lot of raw     
1:00:40     
data video data those all could be built into this General     
1:00:46     
model but you know thinking putting setting kind of behavior aside we can     
1:00:53     
then build functional models so in this case I say here that functional models     
1:00:58     
or things like movement behaviors or social behaviors functional models really would be anything that tells us     
1:01:05     
something about the specific function of cells of     
1:01:11     
organisms for different sort of domains so we have the general model of say     
1:01:18     
morphogenesis or metabolism or genetics which would just basically be you know     
1:01:23     
what kinds of circuits do we have movement circuits what kinds of uh Gene     
1:01:29     
regulatory networks do we have what kinds of U other types of networks do we     
1:01:34     
have we we basically want to describe these processes and celegans at a very     
1:01:40     
general level so for example with morphogenesis we have specific things in     
1:01:45     
celegans like movements in the embryo or patterns of division in the embryo or     
1:01:51     
patterns of differentiation in the embryo we can see this in the tree we     
1:01:57     
have patterns of connectomic so the seans connectome is unique from other connectomes it's also unique from a     
1:02:04     
random graph those General models give us these Tendencies or this sort of structure on which to hang uh any sort     
1:02:12     
of prediction from data and then the functional models would be more about sort of a time a     
1:02:20     
dynamical model or a Time dependent process and so you know it could be like     
1:02:26     
social behaviors like um you know they different social behaviors seans engages     
1:02:31     
in or movement behaviors that cgans engages in we know that the     
1:02:37     
movement uh celan movement repertoire is a somewhat low dimensional space     
1:02:43     
compared to something like uh the uh genomic space for celegans so we can     
1:02:50     
characterize all these functions uh aside from sort of these General tend encies and having models of     
1:02:57     
both of those would contribute to a world model that would then allow us to     
1:03:03     
you know basically train the model have this world model and then be able to query the model for predictions or     
1:03:11     
insights so you know that's something that I just kind of sketched out from our discussion hopefully you know we can     
1:03:18     
kind of clarify this a little bit more we might actually put together a pre-print on this so stay tuned
