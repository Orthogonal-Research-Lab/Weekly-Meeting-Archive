     
Transcript     
0:00     
all right welcome to this week's meeting uh this week's meeting I had some problems with encoding recording so I'm     
0:07     
going to give an update on what we did today and I want to give an update on some of the work that some of our     
0:13     
aspiring g-o students are up to as well uh in fact that's most of what our meeting was was people presenting and     
0:20     
that's why I wanted to give an update um and so we're going to talk a little bit     
0:25     
about hypergraphs and we're going to talk a little bit about topological data     
0:31     
analysis so we're going to we have two students interested in these two projects and uh so we won't be finding     
0:37     
out who is being selected for the project until beginning of May but we     
0:44     
are interested in people contributing to this project this is the DOR graph project throughout this summer and so     
0:51     
let's get started so we'll start with the hypergraphs project and I have some uh     
0:57     
screenshots from the recording the audio didn't work but I did get capture a lot     
1:03     
of the slides that people made so our presenter was mahul Aurora     
1:09     
and he's been joining our group for the last several weeks and he's interested     
1:14     
in this hyperg grass paper that I group put out at the end of last year and this is     
1:20     
hypergraphs specifically for developmental biology so he was very intrigued by that because he had written     
1:27     
himself a hypergraph paper not on embryos but on human brain data so this     
1:33     
is a nice um you know he's going to try to transfer these skills over to     
1:39     
development but this is also nice that people have expertise in these areas     
1:44     
that are synergistic with what we're trying to push forward so mahul wrote a paper called     
1:50     
hyper Gale and this is hyper graphs which and then the acronym Gale so Gil     
1:57     
is gated attention GA and learnable hyper edges Le so this is a     
2:03     
classification of these structures you're using two things you're using gated attention and learnable Hyper     
2:10     
edges so this paper that he did he was an author on it was you know an attempt     
2:16     
to sort of get your hands around Autism Spectrum Disorder so this is based on neuroimaging data and but they're going     
2:24     
to do is they're going to characterize brain activity based on different types of autism or try to classify different     
2:30     
types of autism and so this the standard boiler plate for a brain Imaging study the     
2:38     
important points here is that autism spectrum disorder is heterogeneous so there are a lot of different symptoms a     
2:43     
lot of different severity levels and it's very hard to just make the diagnosis by looking at the behavior or     
2:50     
by just looking at a set of images of of the brain and so we want to be able to     
2:56     
use machine learning specifically Graal Networks to come up with a good     
3:02     
diagnostic criteria uh so we also can link uh autism     
3:09     
spectrum disorder to distinct patterns of functional connectivity and so functional connectivity is a type of     
3:16     
connectivity they use in neuroimaging it involves uh making     
3:22     
assumptions about what what uh brain areas are connected to what brain areas based on function or what voxels are     
3:29     
connected to different other voxels and other brain enies based on function and so this is one of the things about this     
3:36     
model is that you know he's using hypergraphs as a way to sort of     
3:41     
characterize the regions of interest and so those regions of Interest are things that you might parcel out of whole brain     
3:49     
data such as like the hippocampus or certain regions in Neo     
3:55     
cortex or certain regions in serup element so you can actually do this uh     
4:00     
with postprocessing and it's this region of the brain but each region of the brain of course contains a number of     
4:06     
voxels and the voxels of course are the standard minimal unit in fmri for     
4:14     
example those voxal then serve as the nodes and then the ROI serve as the     
4:21     
hyper nodes so each hyper node has a bunch of nodes in it and each hyper node     
4:26     
has a hyper Edge to another hyper node or another Roi and in turn all of the nodes within the     
4:32     
hyper node are all connected to other nodes within other hyper nodes so you     
4:38     
have these hyper edges which are sort of average versions of different edges that     
4:43     
go from the edges to other edges and depending on their category or     
4:50     
classification you know they either are you know there's a lot of variation so this is a you know a thing     
4:57     
that we're also interested in with respect to Developmental hypergraphs and     
5:03     
in developmental hypergraphs we use actually a more formal method of category Theory to determine the     
5:08     
different categories so this is something that people have used novel machine learning     
5:14     
methods for classifying ASD subjects but also uh working with these types of     
5:20     
multi-level data and of course um I guess if you want to call hypergraph     
5:25     
metal networks uh that serves our needs very well and so they went through their     
5:30     
data set and their pre-processing steps they talked about the hyper Gale model so hyper Gale is a novel hypergraph     
5:37     
based model and it's being applied to as classification and of course Gil stands     
5:44     
again for gated attention GA and learnable hyperides     
5:49     
L then there's model evaluation so they're using different methods comparing it with graph based methods     
5:56     
which I like because I think we need to do a lot of benchmarking of RGN uh especially in terms of you know the     
6:02     
kinds of results we get with Evol learn and some of the deep learning methods there but also other methods of     
6:09     
classification on the other end so when you have we a model of hyper nodes and     
6:15     
regular nodes and Hyper edges regular edges can you get a better sort of result using something like a deep     
6:23     
learning technique or a Transformer just a a plain venola Transformer or something else so this is good this is     
6:30     
good that we have this sort of interest in doing these benchmarks and so you know they did a     
6:38     
pretty decent job uh in terms of accuracy and area under the curve so this is of course the standard model     
6:46     
evaluation and importantly they did focus on interoperability and looking at the     
6:51     
difference in biomarkers for different types of uh disorders so you know one of     
6:56     
the things about biological data specially we development modal data is we have this issue of     
7:02     
interpretability a lot of times we use uh models in especially for large uh     
7:08     
data sets or uh what we call Big Data and we don't really understand what     
7:14     
we're doing so in a sense what this hyper Network model is doing is it's     
7:20     
reducing the dimensionality of a network and so you have these hyper nodes that     
7:26     
sort of take in a bunch of nodes and their hyper edges that sort of average out the edges so basically is a way to     
7:32     
interpret say like a complex Network and complex networks indeed are you know a     
7:37     
little bit hard to to make interpretable for people people have their intuitions     
7:43     
about what's going on and they plot them out in a network and you get this hair ball and you know it's not really     
7:49     
something you can say very much about sometimes when we do things with genomic data or gene expression data we have     
7:57     
issues of you know using multi or diens ality reduction or multi-dimensional methods that give us a visualization     
8:04     
that looks very pretty but it doesn't do very much in terms of interoperability often times people will     
8:10     
see something and say it's a cluster and it's not really a cluster because you know the standards for or the support     
8:18     
for a cluster and not there so I think this is another very important uh Focus     
8:27     
interpret so this is the the figure that he kind of showed about their modified     
8:33     
hypergraph convolution so they're doing this series of convolution steps to     
8:39     
determine the connections between the Hy uh hyper noes and other hypernodes with hyperedges so there's the hyperg     
8:46     
algorithm down here and it's hard to see because of the way the the way jety displays it but     
8:54     
basically you you know you work from these parcellations of data you move     
8:59     
that you know you get a measure of functional connectivity and then you get all these different transforms you use a     
9:06     
railu function and you basically get these networks so basically what you're doing is you're growing these     
9:12     
networks um from the best fit to the data and so this is the results     
9:18     
comparing non-graph methods and traditional methods with the graph-based methods transform based methods and then     
9:25     
their approach which is our hyper graphs uh so this is a of course there other hypergraph models in the literature     
9:31     
hyper Gale then actually does a really good job actually higher performance than any of the other R so in terms of     
9:38     
accuracy uh think in terms of are under the curve as well and uh yeah so this is     
9:44     
sort of they they've done a pretty good job in terms of the hypergraphs making this a good um analytical procedure and     
9:52     
of course our Focus has been more on Theory but they really have shown that this could be used quite well for     
10:00     
at least for neuroimaging dep and so this shows some of the experimentation     
10:05     
so what they're doing uh for some of this in terms of uh looking for hyperedges or using this uh KNN or these     
10:13     
uh K means uh procedures to sort of get uh sort of cluster things and find like     
10:20     
edges and make hyper edges out of it so they're using this type of uh     
10:26     
classification technique and they're running this and getting results and so     
10:31     
suffice it to say that you have different you know issues with     
10:37     
hyperedges they're not like obvious things you have to search for them you have to classify them based on what the     
10:42     
rois are and so this is a problem too with you know with some of our developmental data so sometimes we have     
10:50     
these you know easy to understand labels so like in the paper that we did uh we     
10:58     
talked about some of the tissues that exist and of course in seans you have     
11:03     
individual cells that have a label and they divide and differentiate deterministically meaning that we know     
11:09     
what their fate will be we know what the lineage tree is we know what each cell is in the lineage tree and it can be     
11:14     
labeled and it's reproducible across individuals and a species now in the     
11:20     
case of other biological data perhaps if you're looking at human or Mouse     
11:25     
development and you're looking at different tissues being formed this cells are you know they respond to     
11:31     
signals we don't have like a label for every cell and often times they're these     
11:37     
processes where whatever cell get the signal first is the cell to     
11:42     
differentiate now those processes are predictable but my point being is that they don't really have labels that we     
11:48     
can use so even like functional labels aren't very useful when you have cells     
11:54     
that you know differentiate uh depending on certain signals or dedifferentiate     
11:59     
or they go through apotosis or whatever so it's it's very hard to get a good     
12:05     
sense of what a cell you know what a label of a cell is or what it even means     
12:11     
so this is one of the challenges of Developmental data in the Neuro Imaging world you know you know kind of what     
12:17     
your regions of Interest are but you don't necessarily know what what's inside each voxel so that's a little bit     
12:23     
of a limitation uh but you know that those voxels are in an Roi so it makes     
12:29     
sense sense that you know kind of what their function is in development you do have a similar thing but you often don't     
12:35     
have labels you have functional labels that we've gotten through annotations but those annotations can be very uh     
12:43     
high level and and sloppy in terms of you know giving you a good sense of what     
12:48     
kinds of transitions are happening what kinds of categories you can build and so on so if you go back to our paper you     
12:55     
know we use uh organisms uh uh you know we use cyanobacteria we use some sort of analog     
13:02     
of celegans and so it's clear that there's the spatial component that there's this     
13:07     
temporal component and that there you know you can label cells and tissues but     
13:12     
the thing is is that in other organisms or other systems you know especially when you're trying to get things out of     
13:17     
the data it's less clear as to what those labels are what those labels should be or what should be in these     
13:24     
hyper nodes because you're going to have a lot of different cell types in a single hyper so like for example the     
13:30     
liver the human liver or any mamalian liver has a lot of different cell types     
13:35     
in it and so we kind of know from estimates what the distribution of cell type should be in there but quite     
13:42     
frankly those labels are a little bit um idealized and so those are some of the     
13:47     
challenges of you know mapping data to these kind of methods and you know this is something that we won't counter in     
13:54     
applying hypergraphs to biological especially developmental data     
14:01     
now you know they they talked about interpretability specifically so one of the things they're trying to do is     
14:08     
they're looking at rois and they're noticing there's a huge overlap in rois so you can have rois for something like     
14:15     
decision making which is an important part of autism to understand that network but the thing is is that those     
14:21     
rois overl so that means that you're going to have uh nodes within maybe two     
14:28     
or more hyper noes because if your Roi represents a hypernode and those rois     
14:34     
overlap then those hyper nodes overlap and so you're going to have nodes or     
14:42     
voxul that are in more than one Roi so this is something that you know actually     
14:47     
you can identify through gated attention but is a problem for sort of our standard model and so this is     
14:55     
interesting because it provides both a challenge and an opportunity the challenge is of course understanding     
15:01     
that you have to have a model of overlapping hyper nodes but the other thing is is that you can use techniques     
15:07     
like gated attention or some other technique to actually identify trouble spots or places where you have this     
15:13     
overlap and so we can address it in a way that's you know makes sense from in the     
15:19     
model and so you know we can find individual differences and contivity and all that you have this here um so we     
15:26     
have like you know this comparison of typically developing brains versus autism and of course those different     
15:32     
conditions yield different network topologies as we might expect so we can observe that but we also can observe     
15:38     
that across subjects it's not surprising and of course you'll see this in development as well even in celegans we     
15:45     
have uh you know wild type development and then we have development that arises     
15:51     
from in specific GS so sometimes specific will be missing some cells or     
15:57     
you know the issues won't develop normally and so we have to account for     
16:03     
them so using their method they were able to identify key functional networks     
16:08     
and again this is based on neuroanatomy so we know from neuroanatomy what these networks should     
16:13     
look like although you know some of the regions of the brain are always kind of in dispute like the lyic system which is     
16:19     
hard to really Define in terms of what areas what rois are all involved in it     
16:25     
um you have executive function and intentional networks and again attentional networks can include a lot     
16:31     
of things or very few things it's one of these things that you know you can toss     
16:36     
things out and get a better result than if you include things but then you get a misleading result and then of course the     
16:43     
default mode Network which is also again another thing that is very hard to clearly Define because we don't really     
16:51     
know what it we kind of know what it does but there's been this debate in the literature about it so all I'm going to     
16:56     
say about this is that our Networks and we start puring it out by function can     
17:01     
be very um difficult to Define and sometimes the results can be misleading     
17:07     
but you know and of course they overlap as well so that's another job and so they were able to find you     
17:14     
know different differences between the autism brains and the de typically developing brains and that's I guess the     
17:21     
important     
17:27     
point now this is an illustrated of of the Gated attention module so two uh     
17:33     
images back uh hansu raised his hand and he asked about this he didn't see     
17:39     
specific mention of the Gated attention mod so they're using machine learning version of attention not the attention     
17:45     
network from the brain but the machine learning version of attention and they're using this gated attention     
17:51     
module and this is the diagram of it um so basically it learns this Alpha     
17:57     
parameter iterative and it learns it from an equation and then the alpha Vector is Multiplied to     
18:04     
get the final node features which is fed to the red layer so basically you have all these Alphas here that are estimated     
18:12     
and then those are combined into this uh Alpha Vector which is then multiplied     
18:19     
out here with the estimator and it gives you this readout layer which is Zed     
18:25     
equals this term so you get this um you know you get this gated attention where     
18:32     
I guess it's it's selecting things and then it's using this estimator to tell you what the answer is so this is a very     
18:39     
you know this is kind of a breakdown of the model I'm not going to get into it too much because I don't know if I really understand it enough to say exact     
18:46     
you know to give you the ins and outs of it but this is a transform that that's you know been used and there of course     
18:51     
there was the paper all Union is attention which is something that I think is both uh     
18:57     
very um very arrogant but also very maybe     
19:04     
true you know the attention has a a very important role in most machine learning     
19:11     
activities so here we were uh after this talk and it was very good talk uh Sher     
19:18     
Ral who was also in the meeting asked this question while researching did you choose the number of neurons later of     
19:24     
the MLP layers which we saw in the Gated detention module and I think that uh     
19:30     
meul answered it pretty well that sometimes you know we think in deep learning that more layers are better but     
19:36     
in fact in the case of gated attention War layers are not always better so that's a nice insight and it's actually     
19:42     
kind of a Divergence from the typical you know what we think is like the recipe for optimal deep learning um and     
19:51     
so that's good and then hansu of course asked about the Gated attention mod or uh diagram which was shown in that last     
19:58     
slide and then Susan apparent was having trouble with her bandwidth so thanks for all the fish and that was uh Susan for     
20:06     
today and Morgan of course is really interested in both uh neuroanatomy and     
20:12     
uh deep learning and machine learning so I I kind of hope that Morgan has some feedback I know he was kind of came in     
20:18     
in the middle of the uh talk but I'm hoping that he has some insights for us     
20:23     
as well and this is a special for me who will get selected for gonw so so this is     
20:28     
the hyper Gale paper this is in the archive it's archive 24031 14484 version 1 and this was just     
20:38     
put out like you know earlier this last month it's the 21st of March so this is     
20:44     
hyper Gale as classification by hypergraph gated attention wable hyper edges so again there is an gated     
20:51     
attention module to try to construct hyper graphs they're learning the hyper edges and we're not just kind of     
20:57     
assuming the hyper edges are you know connecting to hyper noes that we know are kind of associated and we're using     
21:05     
our functional connectivity to sort of give us these uh potential hyperedges     
21:16     
so one of the things they did in terms of their data set they have this a buy2 data set which is a data set of neur     
21:22     
Imaging data so again your data set is going to be important for these projects and we'll talk in a little bit about     
21:28     
some of the data sets for the D um project and so from the abstract typer     
21:35     
Gale not only improves interoperability but also demonstrates statistically significance enhancements in key     
21:42     
performance metrics compared to both previous baselines and the foundational hypergraph model so this is interesting     
21:49     
that you know there's this foundational hypergraph model but there are also other types of methods and we have these     
21:55     
baselines that we can use and labor outperform those so I'm hoping that you know maybe hypergraphs are sort of the     
22:02     
key to our uh efforts in in daph and especially in tackling some of these     
22:08     
developmental data sets the advancement hyper Gale brings to ASD research highlights the potential     
22:14     
of sophisticated graph based techniques and neurodevelopmental studies so this     
22:19     
was actually neurodevelopment in in the human brain in in um not you know not     
22:26     
infancy but in childhood so this is something that of course there are you know similarities because you have this     
22:33     
change this plasticity but it's it's a little bit more fundamental in our case     
22:38     
because you're actually getting cells being born and being connected together and you get a lot of features in our     
22:44     
data sets that really work wire these kind of models that expand outward and     
22:49     
we're going to talk about that in a bit too where they expand and they take in new nodes and they wire them in a     
22:55     
certain way and then they have these hierarchical organizational uh levels so you you     
23:01     
might have your nodes which are the cells and then you have tissues and then you have maybe have networks like     
23:08     
connectomes you might also have things that become centia like muscle which     
23:14     
means that they have multiple individual cells and they get fused and you have multiple     
23:20     
nuclei and other things like that so we have a lot of things that we can actually draw from that actually pre     
23:26     
unique challenges to these kind of methods and so this is their sort of pipeline here where they have the abide     
23:31     
to data set which is the autism brain Imaging data exchange uh then they have the schaer     
23:37     
parcelation which is a technique they use to get the rois so you parcel different parts of the brain and you you     
23:44     
know there's a whole map for this so you can align the data and it's very uh very     
23:49     
formalized compared to what we have to do in embryos although in SE alans people have done a lot of work on sort     
23:56     
of standardizing uh you know know segmentation for C elegan cells so this     
24:01     
shouldn't be too much too hard of a problem with C Elegance but another organisms it might be a little harder     
24:08     
and then finally your functional connectivity Matrix which is the standard measure of functional     
24:13     
connectivity and then you know for the hyper Gale pipeline you're creating hyper graphs in this functional     
24:19     
connectivity Matrix hyper nodes are these Parcels with these rois the nodes are you know individual     
24:27     
cells in this functional connectivity Matrix the edges are these connections     
24:32     
or where there's a correlation or covariance in the functional connectivity Matrix and then the     
24:38     
connections the hyper uh hyper edges are actually discovered through different     
24:43     
algorithms that are applied to that for that purpose and so you know you have you can imagine if you have two hyper     
24:50     
nodes with a bunch of nodes inside and they all have different connections and different weights uh it's going to be     
24:56     
hard to know what the true hyper edges so sometimes we have hyper edges that are like more likely or less likely     
25:03     
sometimes we'll have multiple hyper edges that kind of uh Converge on one     
25:09     
area and you know it tells us information that we wouldn't know just from building like this hairball which     
25:15     
is the typical model of the network this hairball where it's hard to interpret what's going on but you also have a lot     
25:22     
of just kind of uh you know places where there isn't a lot of structure on the     
25:27     
graph and there ways around that you can do things like Community detection and     
25:32     
looking for different motifs whereas we'll talk about later simplies uh so then there's hypergraph     
25:39     
convolution which is what we use for this convolution step and gated attention and then the MLP readout and     
25:46     
you get this these two different measures here the regular brains and the ASD and so hul brought up another paper     
25:54     
in our discussion this is called attention-based Deep multiple instance learning so this is actually uh     
26:00     
something that's you know kind of their attention methods were based on the Gated attention method in particular so     
26:07     
this is uh paper from 2018 this was also on the archive U and so the abstract     
26:13     
reads multiple instance learning which is M this is a variation of supervisor     
26:19     
where a single class label is assigned to a bag of instances so this is like a bag of words where you have your     
26:25     
instances in this sort of uh po is earn and you sample each instance like you     
26:32     
would sample each word in a bag of words and they're kind of nominally independent but you know you can replace     
26:39     
them you know sample with replacement or just sample them and it gives you a lot of uh Power to sort of randomize what     
26:45     
you're drawing from so you're dealing here with labels and class labels and so this is an an     
26:51     
important Point here because you know the way we treat our sort of classes you     
26:57     
know sometimes we have labeled data and sometimes we don't like I said sometimes the labels though kind of fall in this     
27:03     
in between area which is in between being very reliable and not reliable at     
27:08     
all so if I say something is muscle you know it's it's maybe muscle at some point in development it's not muscle     
27:16     
from the very beginning you might have a connecto when do we start to call it a connecto maybe when we get two neurons     
27:23     
connected for the first time to know where that boundary is to know where there's actually like you know some     
27:29     
structure that would resemble what we're trying to do which then translates to the glasses and the class labels is     
27:36     
going to be a hard problem and so you're going to have to do things where you know you do things like multiple instance or something else uh where it's     
27:44     
not the obvious choice so in this paper we State the mil     
27:49     
problem is learning the bruli distribution of the bag label so this is where you have this distribution within     
27:55     
the bag um where the bag label probability is fully parameterized by neural networks so you're building this     
28:02     
distribution of labels within a bag you're sampling blindly and you're getting you know they're nominally     
28:08     
independent so these are you know you're kind of choosing a label uh from this     
28:15     
distribution furthermore we propose a neural network based permutation and variant aggregation operator that     
28:22     
corresponds to the attention mechanism so this is like this attention like mechanism notably an application of a     
28:29     
proposed attention based operator provides insight into the contribution of each instance to the bag we show     
28:36     
empirically that our approach achieves comparable performance to the best milil methods on Benchmark mil data sets it as     
28:44     
multiple instance learning and it up performs other methods on mnist based     
28:49     
data set and two life real life histopathology data sets without sacrificing     
28:55     
interpretability so this is you know an interest Point here so basically um they     
29:01     
talk about this here where you have this scenario sometimes in real life data     
29:07     
where you have multiple instance learning where you need to learn from a weekly annotated data set so again this     
29:13     
is exactly what I'm talking about with the developmental data is weekly annotated it's not the annotations     
29:18     
aren't great but they're not totally worthless and so you know this is is something we have to work around so the     
29:25     
problem with weekly annotated data is especially apparent medical citation quc at all     
29:32     
2017 and where an image is typically described by a single label so you know     
29:37     
sometimes you have images that represent benign tumors or malignant tumors so you     
29:43     
classify them in that way but there is other information in there and in fact those labels can have false you can have     
29:49     
false positives you can have true negatives so the labels are based on like some you know Criterion that you     
29:56     
pick like some set of features and if the wble is weakly describes     
30:01     
what's going on so you know a lot of wble a lot of times you get a medical diagnosis where the symptoms point in     
30:08     
different directions and so the features maybe give a mixed signal they not all point in the same direction so that     
30:14     
label of benign or malignant is going to be harder to make or it's going to be harder to say what data fits into those     
30:23     
categories um uh multi-instance learning deals with a bag of inst es which a     
30:29     
single class label was assigned so this is again where we're choosing our labels     
30:34     
we're drawing from a probability distributions it's not blind but you know it's based on the the available     
30:40     
information uh hence the main goal of mil is to learn a model that predicts the bag     
30:46     
label which is a medical diagnosis so in the bag you have a bunch of labels or medical diagnos he's given some set of     
30:54     
conditions and you draw from that distribution and you know you're able to get maybe the bre     
31:00     
then um so then you know in additional challenge is to discover key instances     
31:06     
or for example the instances that trigger the bad La so there's certain features that may be very powerful in     
31:12     
making that diagnosis those are key instances and finding those is the important     
31:17     
thing in the medical domain the latter task is of great interest because of legal issues and its usefulness and     
31:23     
vable practice and so uh they talk about this bag classification in order to solve the     
31:30     
primary task of a bag classification different methods are proposed such as utilizing similarities among bags     
31:37     
embedding instances to a compact low dimensional representation that is further fed to a bag level     
31:43     
classified and combining responses of an instance level classifier so only this     
31:48     
last approach combining responses of an instance level classifier are capable of     
31:53     
providing interpretable results so you know we have these different Tech techniques that we can try so you know     
32:00     
I'm glad that n has put some effort into this finding what these different options     
32:09     
are okay now we're going to move on to the next presentation so py uh Balia is another     
32:19     
GSA Cal and she's actually working on a this problem of uh evog graph from a     
32:25     
different angle and that is topological data houses so hansu has uh signed up to     
32:32     
be a mentor for this year's project so we co-mentoring the project with me and     
32:37     
you know he's been involved with two other G projects that I've been a mentor for so in 2022 he was on this open     
32:45     
source sustainability project and he built a wonderful reinforcement learning model and then he worked on dvo learn     
32:53     
last year and he actually worked in daph in building topological data analysis     
32:59     
part of that Pipeline and some of the techniques that Pocky has been looking at and thinking about making going     
33:06     
further and making improvements upon     
33:14     
that so Pocky showed us her notebook and     
33:20     
so she's was actually looking through a lot of hu's code and looking at how it     
33:25     
can be improved she's also been looking at two other models uh neural     
33:31     
developmental programs and another Associated uh     
33:37     
topological data analysis method last time we talked about neural developmental programs this was two     
33:42     
weeks ago where we had this idea of the neural developmental program so     
33:47     
basically this is a technique where you can grow on neural networks we can each     
33:53     
one of these nodes in this graph that we see here is a neural developmental program so this is very much like um     
34:01     
sort of an evolutionary algorithm but it's not an evolutionary algorithm it's called a developmental program so it     
34:07     
does a lot of the same things that an evolutionary program would do but it does sort of this developmental aspect     
34:14     
where it's born and dies or it changes its wiring so it exhibits plasticity and     
34:20     
then of course you get this network of nodes that are wired in these what we'll call simplies which we'll talk about in     
34:27     
a little bit but these are you know these like things like triangles or you know other types of     
34:33     
basic shapes that you can identify within a network that are based on the connectivity so in a neural     
34:39     
developmental program you have graph convolution which is where you try different connections and you see where     
34:45     
which ones are the best fit so in this case we have a triangular a three Simplex which is you know a triangle and     
34:52     
that was a good fit for this data and then you know we have so we have this transformation from one time step to the     
34:59     
next and we do this convolution then we have this growth phase where we actually take the graph     
35:05     
state which is defined here and then we move to the next time step and we get     
35:11     
this s hat which is the estimator and that graph state is updated via local information aggregation so we're uh     
35:18     
aggregating information and we're growing the network so this is graph growth where we add two nodes and we     
35:23     
have to connect them in to the existing programs or nodes     
35:28     
and then we do this weight prediction step where we're predicting the edge weights updated as as we go along so     
35:34     
this is actually something that uh is Sebastian reesi and his collaborators     
35:39     
have worked on so this is a really there so that means that there's an open source repo for this uh but this is     
35:46     
something that we could use in a lot of different ways not just in dvo learn and dvo uh dvo graph but also in some of our     
35:53     
agent based stuff in the orthogonal was about so this is you know this good this     
35:58     
iterates to new development cycle and then of course this is based on a paper     
36:04     
towards self assembling artificial neural networks through neural developmental programs we talked about     
36:09     
this two weeks ago but I think it's worth talking about again because he's going to be using part of this as part     
36:16     
of the project but then the second part is this uh topological data analysis     
36:22     
mapper and so this mapper is actually an interesting um technique and so the best     
36:28     
way to is this is based on a paper actually back from 2007 called topological methods from the analysis a     
36:35     
high dimensional data sets and 3D object recognition this is the first mention of a TDA mapper so this is a standard tool     
36:42     
in topological data analysis and again this is the second part of this proposed     
36:47     
project and it's an open source repo so this is something we can fork and use     
36:53     
but uh Pocky actually created a demo in Mathematica which is kind of interesting now this is a very generic     
37:00     
demo so this just shows these different graphs As you move them around and they     
37:06     
kind of have this sort of phys this this uh physics here this Force layout where     
37:13     
it's kind of moving against you know you can't connect them like this you have to kind of so they're independent little uh     
37:21     
modules that don't connect but they kind of move against one another this is an interesting set of things going on here     
37:28     
in this demo and what's interesting about this is It's generic and you can create this     
37:35     
this is actually a umap projection where you have this these nodes are connected you can create graphs and so you can see     
37:42     
that there's a small demo here but it doesn't really look like anything biological maybe a little bit but um     
37:49     
it's it's actually giving us um you know something we can work from so if we want to think about this in terms of     
37:55     
developmentally specific data you know there are a lot of networks in the embryo that kind of emerge from maybe     
38:02     
different tissues forming or different uh parts of the embrio differentiating maybe basic polarity or maybe like the     
38:10     
animal pole you know where you have animal cells at one end of the of the     
38:16     
embryo and so you can actually model all these things with these kind of networks and this is of course the TDA mapper     
38:24     
technique and so this is you know a demo that you can work out here but it doesn't     
38:29     
really look like anything developmental but we can you know put the Flesh on this skeleton as a so that's a nice     
38:37     
demo this paper got back to this paper we actually have some interesting points     
38:42     
here so this is uh this is the method where they first uh propos this mapper method we present a computational method     
38:49     
for extracting simple descriptions of high-dimensional data sets in the form of simplical complexes so these are the     
38:56     
things that in this demo you know this is a simplical complex here these     
39:02     
triangles you have diamonds you have other shapes that form and so those shapes are like three simplies and four     
39:09     
simplees and things like that and it allows you to break down your network into components that you can identify as     
39:16     
like motifs so like you know if we know that there's some developmental process happening and there's you know things     
39:23     
are being connected together in the networks uh we can maybe when we see the first emergence of triangles then we     
39:29     
know that we've reached a threshold we can put a label on the data as opposed to before then when we couldn't so as     
39:37     
cells divide and and differentiate or come into contact with one another through vibration processes or you know     
39:43     
as the embryo expands the number of it cells you start to get these uh     
39:49     
connections that are you know that can be defined by these simplies and so that's a very important tool I think     
39:55     
especially in these growing networks where you're adding nodes into this network and you start to get features     
40:00     
that emerge and so at a certain time we can make a statement about like you know     
40:05     
when this thing actually has a label versus another a very interesting kind of     
40:13     
approach so this method is called mapper is based on the idea of partial     
40:18     
clustering of the data so in the last technique you know we had this uh K     
40:24     
means clustering uh and you know we had this search for different uh hyper edges     
40:31     
in this case we're doing partial clustering of the data Guided by a set of functions def finded on the data so     
40:37     
this is a different approach and now I'd like to see you know if say both of these students get selected for the     
40:43     
project I would actually like to see you know some sort of uh communication or     
40:49     
dialogue between what people you know what we're doing with partial clustering versus clustering on you know maybe some     
40:57     
like hyper edges and make comparisons and maybe develop maybe a new tool maybe     
41:03     
you know know which one is better for whatever uh whatever you know there's probably going to be different     
41:08     
conditions under which one method is better than another and getting that all worked     
41:13     
out the proposed method is not dependent on any particular clustering algorithm     
41:19     
so this is interesting that this mapper doesn't require a specific clustering algorithm in this demo actually we're     
41:25     
using umap is a visualization technique so you know this is something that we     
41:30     
don't have to use umap we can use another technique and it can give us like you know we can reduce the dimensionality and technically umap is     
41:38     
not a clustering technique but we can use other methods to visualize things as well U so this proposed methods this     
41:45     
proposed method is not dependent on any particular clustering algorithm uh but any clustering     
41:51     
algorithm may be used with mapper so that's an important Point uh we implement this method and     
41:57     
present a few sample applications which simple descriptions of the data present important information about its     
42:04     
structure so this is this is sort of the way they approach this problem um they     
42:10     
actually say that it's useful to label the nodes of a simplical complex by color and size so you can see that they     
42:16     
have this line and this actually is reminiscent of one of the hyper uh     
42:21     
Network bottles we made for CYO bacteria where you have this chain and the chain expand     
42:27     
and along the chain you have these breaks where you get differentiation versus things that just kind of get     
42:33     
pushed in either direction and that might actually be an interesting model system for this as well where we have     
42:39     
very simple morphogenesis that can be modeled with a very simplical set of simplical     
42:44     
complexes so we're labeling the nodes of the complex by color and size the color of a node indicates the value of the     
42:51     
function f red being high here and blue being low as a represent ative point in the     
42:57     
corresponding set of cover ubar or perhaps by a suitable average taken over the set so we have this you know we have     
43:06     
the simple gold complex that forms this uh you know line or I     
43:11     
guess you know positive half line and negative half line but this is something     
43:18     
that you know we can use in in Morag Genesis we can identify simplical complexes associated with certain modes     
43:25     
of morphogenesis in different organisms model organisms and so forth so they talk about the     
43:31     
implementation of a statistical version of mapper and which we've developed uh from point cloud data so we actually we     
43:38     
have a lot of Point cloud data in evoor worm we do you know there's cell tracking data which I'll talk about in a     
43:44     
minute but this is also something that you know if we wanted to apply a data set we're going to have basically Point     
43:51     
clouds so if we segment a bunch of cells the cell uh centroids are going to be     
43:56     
Point cloud we might have features that are Point clouds or you know maybe the edges of     
44:01     
cells are going to be series of points and they'll form this point Cloud but we have to then find the topological     
44:09     
structure the main idea in passing from the topological version to the statistical version is that clustering     
44:15     
should be regarded as the statistical version of the geometric notion of partitioning a space into its connected     
44:21     
components so this is kind of like in uh Network science where we try to find     
44:28     
like the giant component or we try to find like unities we try to find the structure within this hairball of     
44:35     
connected elements and so they talk about you know we assume that a point Cloud contains     
44:41     
end points that are contained in this set X and then we have a function from X     
44:47     
to R which Maps it to R whose value is known for end data points we call this function filter also we assume that it's     
44:54     
possible to compute interpoint distances between the points in the data so we have this uh you know technique which we     
45:01     
kind of use informally for you know analyzing Point clouds and finding structure in them but this is actually a     
45:07     
more formal method now you know TDA is not necessarily uh closely related to     
45:14     
network science so TDA is its own thing it's based in you know large all sorts     
45:19     
of different types of data sets not necessarily network data but we can use it in networks very well and it gives us     
45:27     
a lot of interesting information and so this kind of follows from hu's work last summer uh where we have this ring or we     
45:34     
have these structures where we try to decompose the ring and try to find the network in so this is where uh we're     
45:41     
sampling from a noisy Circle here and we're setting up our filter and we end     
45:47     
up with this basically the simplical complex the six-sided shape and so you can see that you have actually eight     
45:53     
nodes within that but it forms the shape of a hexagon so this just tells us how we do this at     
45:59     
the bottom is a simplical complex which we recover whose vertices are colored by the average filter value so some of     
46:05     
these are going to be you know have different filter values blue being low red being high so there's going to be a     
46:10     
different confidence across the structure but we can at least say that this is a Simplex that you know has some     
46:17     
statistical support and so although theistic has worked well for many data sets that     
46:24     
we've tried It suffers from the following limitations one if the cluster has very different densities it will     
46:29     
tend to pick out clusters of high density only so if you have more data sets or data points in an area it'll     
46:35     
pick from that disproportionately and give you probably what would be the wrong answer the second case is it is possible     
46:43     
to construct examples where the Clusters are distributed in such a way such that we recover the incorrect clustering so     
46:50     
you know this is where the Clusters are you know distributed in a way that's going to lead to false positives     
46:57     
and so we you know the procedure needs to be worked out I'm not really sure the stateof thee art already know I'm sure they've been working on these points     
47:04     
they're actually using a sort of a k means analog here so increasing K will increase the number of clusters we     
47:10     
observe and decreasing K will reduce it so this is theistic that they were referring to in in this part here um but     
47:18     
you know this is of course analogous to C so something that you know again we're     
47:23     
using this in the other technique so maybe there's some ction we have to find out more and so finally we have this     
47:30     
High dimensional parameter space that we try to reduce and so we're doing this     
47:35     
data reduction in general the mapper construction requires as input a a     
47:41     
parameter space defined by the functions and be a covering of the space we need to be able to cover the space but also     
47:47     
Define the space any covering of the parameter space may be used so this is a very uh General technique it can have a     
47:55     
lot of you know know we can apply a lot of methods to it and so I think combining um the neural developmental     
48:04     
programs with the mapper algorithm actually works out pretty well I know that hansu didn't get a lot of chance to     
48:10     
work out the details of the mapper algorithm with respect to data but you     
48:15     
know this is something that again we we do certain amount of work in the summer and then we approve upon it the next     
48:21     
year and the next year we try to you know by the end of the you know maybe four or five year period we have some     
48:27     
really good work and so finally I want to reiterate this aspect of the cell tracking challenge data sets so this is     
48:34     
something kamanu brought up in the conversation and this is the cell tracking challenge 10 years of objective     
48:41     
benchmarking so when they're looking for data sets we can go to the cell tracking Challenge and they have a lot of data     
48:46     
sets this is the paper from nature methods from last year where they described the project they describe what     
48:52     
was going on in this 10-year period and they've been generating a lot of training sets for different models and     
48:58     
the idea is to basically do the best job you can in terms of tracking cells in an     
49:03     
Embry so you have these florescent markers and you're tracking those florescent markers you're tracking cell     
49:09     
division you're tracking differentiation you're tracking migration we tracking their three-dimensional position     
49:15     
actually their four-dimensional position because those three-dimensional positions move over time and so we have     
49:22     
all this information that we can extract out of microscop it so speaking of which we     
49:28     
have some microscopy images here and so these are different data sets where you     
49:33     
know we have these three dimensional images of different systems and so     
49:39     
there's you know they usually use some sort of fluorescent protein where they stain different parts of the cell so in     
49:46     
this case this is gfp labeled act in these are lung cancer cells embedded in a matrial matrix so you can see the out     
49:54     
outer parts of the cell with the acting in it and you can see like the structure around the membrane and things like that     
50:01     
uh we also have lung cancer cells human breast carcinoma cells and then of     
50:07     
course here is the celegans developing emban and this is based on data from the water stem lab the Waterston lab has     
50:13     
produced really some of the foundational data available for seans Emos it's very     
50:18     
reliable they've basically uh you know labeled each cell and they have a Time     
50:24     
series so this data these two data sets the test and the training data you know the these are very standard uh data sets     
50:32     
very well characterized and they should be of use to like getting this this centroid of each cell and you know the     
50:39     
label is very precise and it usually has very little noise Al there is noise in     
50:44     
every data set uh you can see that it's producing you know from very few cells     
50:50     
to a lot of cells and they're very clear in terms of the background and in terms of the signal so that's good     
50:57     
but of course there are other data sets we have the drop melanogaster embryo which is the fruit fly this is a     
51:03     
different mode of development then celegans I mean you know they have a similar type of development but it's     
51:10     
it's a very different system so there are a lot of things going on the srio that are different but this is another     
51:15     
data set from the Color Lab where they have worked out some of the details of of the data it's very low noise it's     
51:23     
they're from janelia so they have like a lot of money to put into this they have the test and training data     
51:28     
sets for this and this is another data set there's also a data set for zebra fish from the color lab and I'm not sure     
51:34     
if it's on this list but this is something we worked with in the past um     
51:39     
so yeah I don't think it's here but we do have access to other data sets uh we     
51:44     
have the uh systems biology uh Institute at reiken where we also have a number of     
51:50     
data sets that we can get access to as well and that includes that zebra fish data set and color um so there are a lot     
51:57     
of data sets you can use now development you know in some in the model organisms has been pretty well characterized with     
52:04     
these tracking cell tracking data sets what our challenge is is to take those data sets take the information about spe     
52:12     
spatial position about the markers and things like that and use it on some of these techniques use it for theoretical     
52:18     
questions or for interesting analyses so that's where we're going with     
52:24     
this so that's your update for this week next week we actually have our 10th anniversary of Diva word we've been     
52:30     
around for 10 years and I have a presentation that's going to be on our YouTube channel hopefully by next week's     
52:37     
meeting but that technically the uh 10th anniversary I think is April 11th and if     
52:42     
you watch that presentation you'll find out why that is and so uh look look     
52:47     
forward to that uh and you know look forward to gach Google summer Cod will be working out this summer again our     
52:55     
chosen students will be uh revealed at the beginning of May so stay tuned for     
53:01     
that hopefully you know people who want to work on this project you don't have to be part of gso to work on it we can     
53:08     
develop a project around your own needs we've done this for a number of postback uh     
53:14     
postbacks uh you know graduate students people just interested in biology research and so forth and we have a lot     
53:20     
of people who have really uh cut their chops on machine learning in our group     
53:26     
so this is something that we also you know there are theoretical aspects to this as well so you don't just have to     
53:32     
focus on building software or you know being a real real expert at machine     
53:39     
learning or any Associated area you can do things like work on theoretical     
53:44     
questions using some of these tools but also you know even using the data sets we've got this as well so if you're     
53:50     
interested let me know thank you and see you next week
